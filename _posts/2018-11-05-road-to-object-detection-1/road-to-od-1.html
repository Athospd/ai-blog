<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Naming and locating objects in images</title>
  
  <meta property="description" itemprop="description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2018-11-05"/>
  <meta property="article:created" itemprop="dateCreated" content="2018-11-05"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Naming and locating objects in images"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Naming and locating objects in images"/>
  <meta property="twitter:description" content="Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We&#39;ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object."/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Naming and locating objects in images"]},{"type":"character","attributes":{},"value":["Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2018roadtood1"]},{"type":"character","attributes":{},"value":["11-05-2018"]},{"type":"character","attributes":{},"value":["Keras","Images","Introductions"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/preds_train.jpg"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/bicycle.jpeg","images/preds_train_2.jpg","images/preds_train.jpg","images/preds_valid_2.jpg","images/preds_valid.jpg","road-to-od-1_files/bowser-1.9.3/bowser.min.js","road-to-od-1_files/distill-2.2.21/template.v2.js","road-to-od-1_files/jquery-1.11.3/jquery.min.js","road-to-od-1_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="road-to-od-1_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="road-to-od-1_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="road-to-od-1_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="road-to-od-1_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Naming and locating objects in images","description":"Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2018-11-05T00:00:00.000-05:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Naming and locating objects in images</h1>
<p>Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We’ll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.</p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>11-05-2018
</div>

<div class="d-article">
<p>We’ve all become used to deep learning’s success in image classification. <em>Greater Swiss Mountain dog</em> or <em>Bernese mountain dog</em>? <em>Red panda</em> or <em>giant panda</em>? No problem. However, in real life it’s not enough to name the single most salient object on a picture. Like it or not, one of the most compelling examples is autonomous driving: We don’t want the algorithm to recognize just that car in front of us, but also the pedestrian about to cross the street. And, just detecting the pedestrian is not sufficient. The exact <em>location</em> of objects matters.</p>
<p>The term <em>object detection</em> is commonly used to refer to the task of naming and localizing multiple objects in an image frame. Object detection is difficult; we’ll build up to it in a loose series of posts, focusing on concepts instead of aiming for ultimate performance. Today, we’ll start with a few straightforward building blocks: Classification, both single and multiple; localization; and combining both classification and localization of a single object.</p>
<aside>
The structure and approaches of these posts will follow the excellent <a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb">fast.ai notebook on object detection</a>.
</aside>
<h2 id="dataset">Dataset</h2>
<p>We’ll be using images and annotations from the <em>Pascal VOC dataset</em> which can be downloaded from <a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">this mirror</a>. Specifically, we’ll use data from the 2007 challenge and the same JSON annotation file as used in the <em>fast.ai</em> course.</p>
<p>Quick download/organization instructions, shamelessly taken from a <a href="https://forums.fast.ai/t/quick-google-colab-setup-for-part-2-week-1-along-with-pascal-voc-dataset/13650">helpful post on the fast.ai wiki</a>, are as follows:</p>
<pre><code>
# mkdir data &amp;&amp; cd data
# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar
# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip
# tar -xf VOCtrainval_06-Nov-2007.tar
# unzip PASCAL_VOC.zip
# mv PASCAL_VOC/*.json .
# rmdir PASCAL_VOC
# tar -xvf VOCtrainval_06-Nov-2007.tar</code></pre>
<p>In words, we take the images and the annotation file from different places:</p>
<ul>
<li><a href="http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar">http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</a> provides us with the images, and after unzipping all we care about is the <code>JPEGImages</code> folder.</li>
<li>From <a href="https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip">https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip</a> all we will be needing is the annotation file, <code>pascal_train2007.json</code>.</li>
</ul>
<p>Whether you’re executing the listed commands or arranging files manually, you should eventually end up with directories/files analogous to these:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
img_dir &lt;- &quot;data/VOCdevkit/VOC2007/JPEGImages&quot;
annot_file &lt;- &quot;data/pascal_train2007.json&quot;</code></pre>
</div>
<p>Now we need to extract some information from that <em>json</em> file.</p>
<h2 id="preprocessing">Preprocessing</h2>
<p>Let’s quickly make sure we have all required libraries loaded.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
library(rjson)
library(magick)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)</code></pre>
</div>
<p>Annotations contain information about three types of things we’re interested in.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
annotations &lt;- fromJSON(file = annot_file)
str(annotations, max.level = 1)</code></pre>
</div>
<pre><code>
List of 4
 $ images     :List of 2501
 $ type       : chr &quot;instances&quot;
 $ annotations:List of 7844
 $ categories :List of 20</code></pre>
<p>First, characteristics of the image itself (height and width) and where it’s stored. Not surprisingly, here it’s one entry per image.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- annotations$images %&gt;% {
  tibble(
    id = map_dbl(., &quot;id&quot;),
    file_name = map_chr(., &quot;file_name&quot;),
    image_height = map_dbl(., &quot;height&quot;),
    image_width = map_dbl(., &quot;width&quot;)
  )
}</code></pre>
</div>
<p>Then, object class ids and bounding box coordinates. There may be multiple of these per image. In Pascal VOC, there are 20 object classes, from ubiquitous vehicles (<code>car</code>, <code>aeroplane</code>) over indispensable animals (<code>cat</code>, <code>sheep</code>) to more rare (in popular datasets) types like <code>potted plant</code> or <code>tv monitor</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
classes &lt;- c(
  &quot;aeroplane&quot;,
  &quot;bicycle&quot;,
  &quot;bird&quot;,
  &quot;boat&quot;,
  &quot;bottle&quot;,
  &quot;bus&quot;,
  &quot;car&quot;,
  &quot;cat&quot;,
  &quot;chair&quot;,
  &quot;cow&quot;,
  &quot;diningtable&quot;,
  &quot;dog&quot;,
  &quot;horse&quot;,
  &quot;motorbike&quot;,
  &quot;person&quot;,
  &quot;pottedplant&quot;,
  &quot;sheep&quot;,
  &quot;sofa&quot;,
  &quot;train&quot;,
  &quot;tvmonitor&quot;
)

boxinfo &lt;- annotations$annotations %&gt;% {
  tibble(
    image_id = map_dbl(., &quot;image_id&quot;),
    category_id = map_dbl(., &quot;category_id&quot;),
    bbox = map(., &quot;bbox&quot;)
  )
}</code></pre>
</div>
<p>The bounding boxes are now stored in a list column and need to be unpacked.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
boxinfo &lt;- boxinfo %&gt;% 
  mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = &quot; &quot;))))
boxinfo &lt;- boxinfo %&gt;% 
  separate(bbox, into = c(&quot;x_left&quot;, &quot;y_top&quot;, &quot;bbox_width&quot;, &quot;bbox_height&quot;))
boxinfo &lt;- boxinfo %&gt;% mutate_all(as.numeric)</code></pre>
</div>
<p>For the bounding boxes, the annotation file provides <code>x_left</code> and <code>y_top</code> coordinates, as well as width and height. We will mostly be working with corner coordinates, so we create the missing <code>x_right</code> and <code>y_top</code>.</p>
<p>As usual in image processing, the <code>y</code> axis starts from the top.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
boxinfo &lt;- boxinfo %&gt;% 
  mutate(y_bottom = y_top + bbox_height - 1, x_right = x_left + bbox_width - 1)</code></pre>
</div>
<p>Finally, we still need to match class ids to class names.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
catinfo &lt;- annotations$categories %&gt;%  {
  tibble(id = map_dbl(., &quot;id&quot;), name = map_chr(., &quot;name&quot;))
}</code></pre>
</div>
<p>So, putting it all together:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- imageinfo %&gt;%
  inner_join(boxinfo, by = c(&quot;id&quot; = &quot;image_id&quot;)) %&gt;%
  inner_join(catinfo, by = c(&quot;category_id&quot; = &quot;id&quot;))</code></pre>
</div>
<p>Note that here still, we have several entries per image, each annotated object occupying its own row.</p>
<p>There’s one step that will bitterly hurt our localization performance if we later forget it, so let’s do it now already: We need to scale all bounding box coordinates according to the actual image size we’ll use when we pass it to our network.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
target_height &lt;- 224
target_width &lt;- 224

imageinfo &lt;- imageinfo %&gt;% mutate(
  x_left_scaled = (x_left / image_width * target_width) %&gt;% round(),
  x_right_scaled = (x_right / image_width * target_width) %&gt;% round(),
  y_top_scaled = (y_top / image_height * target_height) %&gt;% round(),
  y_bottom_scaled = (y_bottom / image_height * target_height) %&gt;% round(),
  bbox_width_scaled =  (bbox_width / image_width * target_width) %&gt;% round(),
  bbox_height_scaled = (bbox_height / image_height * target_height) %&gt;% round()
)</code></pre>
</div>
<p>Let’s take a glance at our data. Picking one of the early entries and displaying the original image together with the object annotation yields</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
img_data &lt;- imageinfo[4,]
img &lt;- image_read(file.path(img_dir, img_data$file_name))
img &lt;- image_draw(img)
rect(
  img_data$x_left,
  img_data$y_bottom,
  img_data$x_right,
  img_data$y_top,
  border = &quot;white&quot;,
  lwd = 2
)
text(
  img_data$x_left,
  img_data$y_top,
  img_data$name,
  offset = 1,
  pos = 2,
  cex = 1.5,
  col = &quot;white&quot;
)
dev.off()</code></pre>
</div>
<p><img src="images/bicycle.jpeg" style="width:80.0%" /></p>
<p>Now as indicated above, in this post we’ll mostly address handling a single object in an image. This means we have to decide, per image, which object to single out.</p>
<p>A reasonable strategy seems to be choosing the object with the largest ground truth bounding box.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo &lt;- imageinfo %&gt;% mutate(area = bbox_width_scaled * bbox_height_scaled)

imageinfo_maxbb &lt;- imageinfo %&gt;%
  group_by(id) %&gt;%
  filter(which.max(area) == row_number())</code></pre>
</div>
<p>After this operation, we only have 2501 images to work with - not many at all! For classification, we could simply use data augmentation as provided by Keras, but to work with localization we’d have to spin our own augmentation algorithm. We’ll leave this to a later occasion and for now, focus on the basics.</p>
<p>Finally after train-test split</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_indices &lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &lt;- imageinfo_maxbb[train_indices,]
validation_data &lt;- imageinfo_maxbb[-train_indices,]</code></pre>
</div>
<p>our training set consists of 2000 images with one annotation each. We’re ready to start training, and we’ll start gently, with single-object classification.</p>
<h2 id="single-object-classification">Single-object classification</h2>
<p>In all cases, we will use XCeption as a basic feature extractor. Having been trained on ImageNet, we don’t expect much fine tuning to be necessary to adapt to Pascal VOC, so we leave XCeption’s weights untouched</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &quot;avg&quot;
)

feature_extractor %&gt;% freeze_weights()</code></pre>
</div>
<p>and put just a few custom layers on top.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 20, activation = &quot;softmax&quot;)

model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = &quot;sparse_categorical_crossentropy&quot;,
  metrics = list(&quot;accuracy&quot;)
)</code></pre>
</div>
<p>How should we pass our data to Keras? We could simple use Keras’ <code>image_data_generator</code>, but given we will need custom generators soon, we’ll build a simple one ourselves. This one delivers images as well as the corresponding targets in a stream. Note how the targets are not one-hot-encoded, but integers - using <code>sparse_categorical_crossentropy</code> as a loss function enables this convenience.</p>
<aside>
See the <a href="https://tensorflow.rstudio.com/learn/resources.html">Deep learning with R</a> book for an introduction to writing data generators like this one.
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
load_and_preprocess_image &lt;- function(image_name, target_height, target_width) {
  img_array &lt;- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %&gt;%
    image_to_array() %&gt;%
    xception_preprocess_input() 
  dim(img_array) &lt;- c(1, dim(img_array))
  img_array
}

classification_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]],
                                    target_height, target_width)
        y[j, ] &lt;-
          data[[indices[j], &quot;category_id&quot;]] - 1
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>Now how does training go?</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;class_only&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>For us, after 8 epochs, accuracies on the train resp. validation sets were at 0.68 and 0.74, respectively. Not too bad given given we’re trying to differentiate between 20 classes here.</p>
<p>Now let’s quickly think what we’d change if we were to classify multiple objects in one image. Changes mostly concern preprocessing steps.</p>
<h2 id="multiple-object-classification">Multiple object classification</h2>
<p>This time, we multi-hot-encode our data. For every image (as represented by its filename), here we have a vector of length 20 where 0 indicates absence, 1 means presence of the respective object class:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
image_cats &lt;- imageinfo %&gt;% 
  select(category_id) %&gt;%
  mutate(category_id = category_id - 1) %&gt;%
  pull() %&gt;%
  to_categorical(num_classes = 20)

image_cats &lt;- data.frame(image_cats) %&gt;%
  add_column(file_name = imageinfo$file_name, .before = TRUE)

image_cats &lt;- image_cats %&gt;% 
  group_by(file_name) %&gt;% 
  summarise_all(.funs = funs(max))

n_samples &lt;- nrow(image_cats)
train_indices &lt;- sample(1:n_samples, 0.8 * n_samples)
train_data &lt;- image_cats[train_indices,]
validation_data &lt;- image_cats[-train_indices,]</code></pre>
</div>
<p>Correspondingly, we modify the generator to return a target of dimensions <code>batch_size</code> * 20, instead of <code>batch_size</code> * 1.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
classification_generator &lt;- 
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 20))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y[j, ] &lt;-
          data[indices[j], 2:21] %&gt;% as.matrix()
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>Now, the most interesting change is to the model - even though it’s a change to two lines only. Were we to use <code>categorical_crossentropy</code> now (the non-sparse variant of the above), combined with a <code>softmax</code> activation, we would effectively tell the model to pick just one, namely, the most probable object.</p>
<aside>
See the <a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/">introduction to loss functions and activations</a> on this blog for a demonstration.
</aside>
<p>Instead, we want to decide: For each object class, is it present in the image or not? Thus, instead of <code>softmax</code> we use <code>sigmoid</code>, paired with <code>binary_crossentropy</code>, to obtain an independent verdict on every class.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;-
  application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = &quot;avg&quot;
  )

feature_extractor %&gt;% freeze_weights()

model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 20, activation = &quot;sigmoid&quot;)

model %&gt;% compile(optimizer = &quot;adam&quot;,
                  loss = &quot;binary_crossentropy&quot;,
                  metrics = list(&quot;accuracy&quot;))</code></pre>
</div>
<p>And finally, again, we fit the model:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;multiclass&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>This time, (binary) accuracy surpasses 0.95 after one epoch already, on both the train and validation sets. Not surprisingly, accuracy is significantly higher here than when we had to single out one of 20 classes (and that, with other confounding objects present in most cases!).</p>
<p>Now, chances are that if you’ve done any deep learning before, you’ve done image classification in some form, perhaps even in the multiple-object variant. To build up in the direction of object detection, it is time we add a new ingredient: localization.</p>
<h2 id="single-object-localization">Single-object localization</h2>
<p>From here on, we’re back to dealing with a single object per image. So the question now is, how do we learn bounding boxes? If you’ve never heard of this, the answer will sound unbelievably simple (naive even): We formulate this as a regression problem and aim to predict the actual coordinates. To set realistic expectations - we surely shouldn’t expect ultimate precision here. But in a way it’s amazing it does even work at all.</p>
<p>What does this mean, formulate as a regression problem? Concretely, it means we’ll have a <code>dense</code> output layer with 4 units, each corresponding to a corner coordinate.</p>
<p>So let’s start with the model this time. Again, we use Xception, but there’s an important difference here: Whereas before, we said <code>pooling = &quot;avg&quot;</code> to obtain an output tensor of dimensions <code>batch_size</code> * number of filters, here we don’t do any averaging or flattening out of the spatial grid. This is because it’s exactly the spatial information we’re interested in!</p>
<p>For Xception, the output resolution will be 7x7. So a priori, we shouldn’t expect high precision on objects much smaller than about 32x32 pixels (assuming the standard input size of 224x224).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

feature_extractor %&gt;% freeze_weights()</code></pre>
</div>
<p>Now we append our custom regression module.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  feature_extractor %&gt;%
  layer_flatten() %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5) %&gt;%
  layer_dense(units = 4)</code></pre>
</div>
<p>We will train with one of the loss functions common in regression tasks, mean absolute error. But in tasks like object detection or segmentation, we’re also interested in a more tangible quantity: How much do estimate and ground truth overlap?</p>
<p>Overlap is usually measured as <em>Intersection over Union</em>, or <em>Jaccard distance</em>. Intersection over Union is exactly what it says, a ratio between space shared by the objects and space occupied when we take them together.</p>
<p>To assess the model’s progress, we can easily code this as a custom metric:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
metric_iou &lt;- function(y_true, y_pred) {
  
  # order is [x_left, y_top, x_right, y_bottom]
  intersection_xmin &lt;- k_maximum(y_true[ ,1], y_pred[ ,1])
  intersection_ymin &lt;- k_maximum(y_true[ ,2], y_pred[ ,2])
  intersection_xmax &lt;- k_minimum(y_true[ ,3], y_pred[ ,3])
  intersection_ymax &lt;- k_minimum(y_true[ ,4], y_pred[ ,4])
  
  area_intersection &lt;- (intersection_xmax - intersection_xmin) * 
                       (intersection_ymax - intersection_ymin)
  area_y &lt;- (y_true[ ,3] - y_true[ ,1]) * (y_true[ ,4] - y_true[ ,2])
  area_yhat &lt;- (y_pred[ ,3] - y_pred[ ,1]) * (y_pred[ ,4] - y_pred[ ,2])
  area_union &lt;- area_y + area_yhat - area_intersection
  
  iou &lt;- area_intersection/area_union
  k_mean(iou)
  
}</code></pre>
</div>
<p>Model compilation then goes like</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = &quot;mae&quot;,
  metrics = list(custom_metric(&quot;iou&quot;, metric_iou))
)</code></pre>
</div>
<p>Now modify the generator to return bounding box coordinates as targets…</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
localization_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y &lt;- array(0, dim = c(length(indices), 4))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y[j, ] &lt;-
          data[indices[j], c(&quot;x_left_scaled&quot;,
                             &quot;y_top_scaled&quot;,
                             &quot;x_right_scaled&quot;,
                             &quot;y_bottom_scaled&quot;)] %&gt;% as.matrix()
      }
      x &lt;- x / 255
      list(x, y)
    }
  }

train_gen &lt;- localization_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- localization_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)</code></pre>
</div>
<p>… and we’re ready to go!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;loc_only&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>After 8 epochs, IOU on both training and test sets is around 0.35. This number doesn’t look too good. To learn more about how training went, we need to see some predictions. Here’s a convenience function that displays an image, the ground truth box of the most salient object (as defined above), and if given, class and bounding box predictions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
plot_image_with_boxes &lt;- function(file_name,
                                  object_class,
                                  box,
                                  scaled = FALSE,
                                  class_pred = NULL,
                                  box_pred = NULL) {
  img &lt;- image_read(file.path(img_dir, file_name))
  if(scaled) img &lt;- image_resize(img, geometry = &quot;224x224!&quot;)
  img &lt;- image_draw(img)
  x_left &lt;- box[1]
  y_bottom &lt;- box[2]
  x_right &lt;- box[3]
  y_top &lt;- box[4]
  rect(
    x_left,
    y_bottom,
    x_right,
    y_top,
    border = &quot;cyan&quot;,
    lwd = 2.5
  )
  text(
    x_left,
    y_top,
    object_class,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = &quot;cyan&quot;
  )
  if (!is.null(box_pred))
    rect(box_pred[1],
         box_pred[2],
         box_pred[3],
         box_pred[4],
         border = &quot;yellow&quot;,
         lwd = 2.5)
  if (!is.null(class_pred))
    text(
      box_pred[1],
      box_pred[2],
      class_pred,
      offset = 0,
      pos = 4,
      cex = 1.5,
      col = &quot;yellow&quot;)
  dev.off()
  img %&gt;% image_write(paste0(&quot;preds_&quot;, file_name))
  plot(img)
}</code></pre>
</div>
<p>First, let’s see predictions on sample images from the training set.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train_1_8 &lt;- train_data[1:8, c(&quot;file_name&quot;,
                               &quot;name&quot;,
                               &quot;x_left_scaled&quot;,
                               &quot;y_top_scaled&quot;,
                               &quot;x_right_scaled&quot;,
                               &quot;y_bottom_scaled&quot;)]

for (i in 1:8) {
  preds &lt;-
    model %&gt;% predict(
      load_and_preprocess_image(train_1_8[i, &quot;file_name&quot;], 
                                target_height, target_width),
      batch_size = 1
  )
  plot_image_with_boxes(train_1_8$file_name[i],
                        train_1_8$name[i],
                        train_1_8[i, 3:6] %&gt;% as.matrix(),
                        scaled = TRUE,
                        box_pred = preds)
}</code></pre>
</div>
<figure>
<img src="images/preds_train.jpg" alt="Sample bounding box predictions on the training set." style="width:100.0%" /><figcaption>Sample bounding box predictions on the training set.</figcaption>
</figure>
<p>As you’d guess from looking, the cyan-colored boxes are the ground truth ones. Now looking at the predictions explains a lot about the mediocre IOU values! Let’s take the very first sample image - we wanted the model to focus on the sofa, but it picked the table, which is also a category in the dataset (although in the form of <em>dining</em> <em>table</em>). Similar with the image on the right of the first row - we wanted to it to pick just the dog but it included the person, too (by far the most frequently seen category in the dataset). So we actually made the task a lot more difficult than had we stayed with e.g., ImageNet where normally a single object is salient.</p>
<p>Now check predictions on the validation set.</p>
<figure>
<img src="images/preds_valid.jpg" alt="Some bounding box predictions on the validation set." style="width:100.0%" /><figcaption>Some bounding box predictions on the validation set.</figcaption>
</figure>
<p>Again, we get a similar impression: The model <em>did</em> learn something, but the task is ill defined. Look at the third image in row 2: Isn’t it pretty consequent the model picks <em>all</em> people instead of singling out some special guy?</p>
<p>If single-object localization is that straightforward, how technically involved can it be to output a class label at the same time? As long as we stay with a single object, the answer indeed is: not much.</p>
<aside>
As a caveat, please note we’re talking about mapping concepts to technical approaches here. Obtaining ultimate performance is a different thing.
</aside>
<p>Let’s finish up today with a constrained combination of classification and localization: detection of a single object.</p>
<h2 id="single-object-detection">Single-object detection</h2>
<p>Combining regression and classification into one means we’ll want to have two outputs in our model. We’ll thus use the functional API this time. Otherwise, there isn’t much new here: We start with an XCeption output of spatial resolution 7x7, append some custom processing and return two outputs, one for bounding box regression and one for classification.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
feature_extractor &lt;- application_xception(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)

input &lt;- feature_extractor$input
common &lt;- feature_extractor$output %&gt;%
  layer_flatten(name = &quot;flatten&quot;) %&gt;%
  layer_activation_relu() %&gt;%
  layer_dropout(rate = 0.25) %&gt;%
  layer_dense(units = 512, activation = &quot;relu&quot;) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dropout(rate = 0.5)

regression_output &lt;-
  layer_dense(common, units = 4, name = &quot;regression_output&quot;)
class_output &lt;- layer_dense(
  common,
  units = 20,
  activation = &quot;softmax&quot;,
  name = &quot;class_output&quot;
)

model &lt;- keras_model(
  inputs = input,
  outputs = list(regression_output, class_output)
)</code></pre>
</div>
<p>When defining the losses (mean absolute error and categorical crossentropy, just as in the respective single tasks of regression and classification), we could weight them so they end up on approximately a common scale. In fact that didn’t make much of a difference so we show the respective code in commented form.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% freeze_weights(to = &quot;flatten&quot;)

model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  loss = list(&quot;mae&quot;, &quot;sparse_categorical_crossentropy&quot;),
  #loss_weights = list(
  #  regression_output = 0.05,
  #  class_output = 0.95),
  metrics = list(
    regression_output = custom_metric(&quot;iou&quot;, metric_iou),
    class_output = &quot;accuracy&quot;
  )
)</code></pre>
</div>
<p>Just like model outputs and losses are both lists, the data generator has to return the ground truth samples in a list. Fitting the model then goes as usual.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
loc_class_generator &lt;-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i &lt;- 1
    function() {
      if (shuffle) {
        indices &lt;- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size &gt;= nrow(data))
          i &lt;&lt;- 1
        indices &lt;- c(i:min(i + batch_size - 1, nrow(data)))
        i &lt;&lt;- i + length(indices)
      }
      x &lt;-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y1 &lt;- array(0, dim = c(length(indices), 4))
      y2 &lt;- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] &lt;-
          load_and_preprocess_image(data[[indices[j], &quot;file_name&quot;]], 
                                    target_height, target_width)
        y1[j, ] &lt;-
          data[indices[j], c(&quot;x_left&quot;, &quot;y_top&quot;, &quot;x_right&quot;, &quot;y_bottom&quot;)] 
            %&gt;% as.matrix()
        y2[j, ] &lt;-
          data[[indices[j], &quot;category_id&quot;]] - 1
      }
      x &lt;- x / 255
      list(x, list(y1, y2))
    }
  }

train_gen &lt;- loc_class_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen &lt;- loc_class_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)

model %&gt;% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path(&quot;loc_class&quot;, &quot;weights.{epoch:02d}-{val_loss:.2f}.hdf5&quot;)
    ),
    callback_early_stopping(patience = 2)
  )
)</code></pre>
</div>
<p>What about model predictions? A priori we might expect the bounding boxes to look better than in the regression-only model, as a significant part of the model is shared between classification and localization. Intuitively, I should be able to more precisely indicate the boundaries of <em>something</em> if I have an idea what that <em>something</em> is.</p>
<p>Unfortunately, that didn’t quite happen. The model has become <em>very</em> biased to detecting a <em>person</em> everywhere, which might be advantageous (thinking safety) in an autonomous driving application but isn’t quite what we’d hoped for here.</p>
<figure>
<img src="images/preds_train_2.jpg" alt="Example class and bounding box predictions on the training set." style="width:100.0%" /><figcaption>Example class and bounding box predictions on the training set.</figcaption>
</figure>
<figure>
<img src="images/preds_valid_2.jpg" alt="Example class and bounding box predictions on the validation set." style="width:100.0%" /><figcaption>Example class and bounding box predictions on the validation set.</figcaption>
</figure>
<p>Just to double-check this really has to do with class imbalance, here are the actual frequencies:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
imageinfo %&gt;% group_by(name)
  %&gt;% summarise(cnt = n()) 
  %&gt;% arrange(desc(cnt))</code></pre>
</div>
<pre><code>
# A tibble: 20 x 2
   name          cnt
   &lt;chr&gt;       &lt;int&gt;
 1 person       2705
 2 car           826
 3 chair         726
 4 bottle        338
 5 pottedplant   305
 6 bird          294
 7 dog           271
 8 sofa          218
 9 boat          208
10 horse         207
11 bicycle       202
12 motorbike     193
13 cat           191
14 sheep         191
15 tvmonitor     191
16 cow           185
17 train         158
18 aeroplane     156
19 diningtable   148
20 bus           131</code></pre>
<p>To get better performance, we’d need to find a successful way to deal with this. However, handling class imbalance in deep learning is a topic of its own, and here we want to build up in the direction of objection detection. So we’ll make a cut here and in an upcoming post, think about how we can classify and localize multiple objects in an image.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We have seen that single-object classification and localization are conceptually straightforward. The big question now is, are these approaches extensible to multiple objects? Or will new ideas have to come in? We’ll follow up on this giving a short overview of approaches and then, singling in on one of those and implementing it.</p>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
