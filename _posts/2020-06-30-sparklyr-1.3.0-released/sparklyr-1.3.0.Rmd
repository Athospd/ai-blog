---
title: "sparklyr 1.3: Higher-order Functions, Avro, and much more"
description: |
  Sparklyr 1.3 is now available, featuring exciting new functionalities such as integration of Spark SQL higher-order functions with dplyr and data import/export in Avro and in user-defined serialization formats!
author:
  - name: Yitao Li
    url: https://github.com/yl790
    affiliation: RStudio
    affiliation_url: https://www.rstudio.com
slug: sparklyr-1.3
date: 06-30-2020
categories:
  - R
  - Packages/Releases
  - Distributed Computing
output:
  distill::distill_article:
    self_contained: false
preview: images/sparklyr-1.3.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

What would be even better than [sparklyr](https://sparklyr.ai) 1.2? Many brilliant minds from RStudio, Databricks, and the broader open-source community have contemplated this question for months. Now they have an answer -- It's sparklyr 1.3!

In this post, we shall highlight some major new features introduced in sparklyr 1.3 and showcase scenarios where such features come in handy. While a number of enhancements and bug fixes (especially those related to `spark_apply`, Arrow, and secondary Spark connections) were also an important part of this release, they will not be the topic of this post, and it will be an easy exercise for the reader to find out more about them from the sparklyr [NEWS](https://github.com/sparklyr/sparklyr/blob/master/NEWS.md) file.


## Fun with higher-order functions

[Higher order functions](https://issues.apache.org/jira/browse/SPARK-19480) are built-in Spark SQL constructs that allow user-defined lambda expressions to be applied efficiently to complex data types such as arrays and structs. As a quick demo to see why higher-order functions are useful, let's say one day Scrooge Mcduck dived into his huge vault of money and found large quantities of pennies, nickels, dimes, and quarters. Having an impeccable taste in data structure, he decided to store the quantities and face values of everything into 2 Spark SQL array columns:

```{bash echo=TRUE, eval=FALSE}
$SPARK_HOME/bin/spark-sql
spark-sql> CREATE TABLE `coins` (`quantities` ARRAY<INT>, `values` ARRAY<INT>) USING ORC;
spark-sql> INSERT INTO `coins` VALUES (ARRAY(4000, 3000, 2000, 1000), ARRAY(1, 5, 10, 25));
```

thus declaring his net worth of 4k pennies, 3k nickels, 2k dimes, and 1k quarters. In Spark 2.4 or above, we can use a higher-order function named `ZIP_WITH` to help Scrooge Mcduck calculate the total value of each type of coin, and store the results in an array of length 4:

```{bash echo=TRUE, eval=FALSE}
spark-sql> SELECT ZIP_WITH(`quantities`, `values`, (quantity, value) -> quantity * value)
         > AS `total_values` FROM `coins`;
```
```
[4000,15000,20000,25000]
```

So the query result `[4000,15000,20000,25000]` indicates there are in total 40 dollars worth of pennies, 150 dollars worth of nickels, 200 dollars worth of dimes, and 250 dollars worth of quarters, as expected.

With sparklyr 1.3, the same computation can be performed on a Spark data frame using `hof_zip_with()` with lambda expression above replaced by a [one-sided formula](https://stat.ethz.ch/R-manual/R-devel/library/base/html/tilde.html) of `.x` and `.y` in R, as shown below:

```{r echo=TRUE, eval=FALSE}
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.4")
coins_tbl <- copy_to(
  sc,
  tibble::tibble(quantities = list(c(4000, 3000, 2000, 1000)), values = list(c(1, 5, 10, 25)))
)

result <- coins %>%
  hof_zip_with(~ .x * .y, dest_col = total_values) %>%
  dplyr::select(total_values) %>%
  collect()

result$total_values

```
```
[[1]]
[1]  4000 15000 20000 25000
```

Other higher-order functions supported by Spark SQL so far include `transform`, `filter`, `aggregate`, and `exists`, as documented in [here](https://spark.apache.org/docs/latest/api/sql/index.html), and similar to the example above, their counterparts (namely, `hof_transform`, `hof_filter`, `hof_aggregate`, and `hof_exists`) all exist in sparklyr 1.3, so that they can be integrated with other `dplyr` verbs in an idiomatic manner in R.


## spark_{read,write}_avro

Another highlight of the sparklyr 1.3 release is its built-in support for Avro data sources. Apache Avro is a widely used data serialization protocol that combines efficiency of binary data format with flexibility of JSON schema definitions. To make working with Avro data sources simpler, in sparklyr 1.3, as soon as a Spark connection is instantiated with `spark_connect(..., package = "avro")`, sparklyr will automatically figure out which version of `spark-avro` package to use with that connection, saving a lot of potential headache from sparklyr users trying to determine the correct version of `spark-avro` by themseleves. Similar to how `spark_read_csv` and `spark_write_csv` are in place to work with CSV data, `spark_read_avro` and `spark_write_avro` methods were implemented in sparklyr 1.3 to facilitate reading and writing Avro files through an Avro-capable Spark connection, as illustrated in the example below:

```{r echo=TRUE, eval=FALSE}
library(sparklyr)

# The `package = "avro"` option is only supported in Spark 2.4 or higher
sc <- spark_connect(master = "local", version = "2.4", package = "avro")

sdf <- sdf_copy_to(
  sc,
  tibble::tibble(
    a = c(1, NaN, 3, 4, NaN),
    b = c(-2L, 0L, 1L, 3L, 2L),
    c = c("a", "b", "c", "", "d")
  )
)

# This example Avro schema is a JSON string that essentially says all columns
# ("a", "b", "c") of `sdf` are nullable.
avro_schema <- rjson::toJSON(list(
  type = "record",
  name = "topLevelRecord",
  fields = list(
    list(name = "a", type = list("double", "null")),
    list(name = "b", type = list("int", "null")),
    list(name = "c", type = list("string", "null"))
  )
))

# persist the Spark data frame from above in Avro format
spark_write_avro(sdf, "/tmp/data.avro", avro_schema)

# and then read the same data frame back
sdf <- spark_read_avro(sc, "/tmp/data.avro")

```

## spark_{read,write}

In addition to commonly used data serialization formats such as CSV, JSON, Parquet, and Avro, starting from sparklyr 1.3, customized data frame serialization and deserialization procedures implemented in R can also be run on Spark workers via the newly implemented `spark_read` and `spark_write` methods. We can see both of them in action through a quick example below, where `saveRDS` is called from a user-defined writer function to save all rows within a Spark data frame into 2 RDS files on disk, and `readRDS` is called from a user-defined reader function to read the data from the RDS files back to Spark:

```{r echo=TRUE, eval=FALSE}
library(sparklyr)

sc <- spark_connect(master = "local")
sdf <- sdf_len(sc, 7)
paths <- c("/tmp/file1.RDS", "/tmp/file2.RDS")

spark_write(sdf, writer = function(df, path) saveRDS(df, path), paths = paths)
spark_read(sc, paths, reader = function(path) readRDS(path), columns = c(id = "integer"))
```

```
# Source: spark<?> [?? x 1]
     id
  <int>
1     1
2     2
3     3
4     4
5     5
6     6
7     7
```


## Acknowledgement

In chronological order, we want to thank the following individuals for submitting pull requests towards sparklyr 1.3:

* Jozef Hajnala <jozef.hajnala@gmail.com>
* Hossein Falaki <falaki@gmail.com>
* Samuel MacÃªdo <samuelmacedo@recife.ifpe.edu.br>
* Yitao Li <yitao@rstudio.com>
* Andy Zhang <yue.zhang@databricks.com>
* Javier Luraschi <javierluraschi@hotmail.com>
* Neal Richardson <neal.p.richardson@gmail.com>

We are also grateful for valuable input on sparklyr 1.3 roadmap and on [#2434](https://github.com/sparklyr/sparklyr/pull/2434), [#2551](https://github.com/sparklyr/sparklyr/pull/2551), etc from Javier Luraschi, and great spiritual advice on [#1773](https://github.com/sparklyr/sparklyr/issues/1773) and [#2514](https://github.com/sparklyr/sparklyr/issues/2514) from [\@mattpollock](https://github.com/mattpollock) and [\@benmwhite](https://github.com/benmwhite).

If you wish to learn more about `sparklyr`, we recommend visiting [sparklyr.ai](https://sparklyr.ai), [spark.rstudio.com](https://spark.rstudio.com), and some of the previous release posts such as [sparklyr 1.2](https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/) and [sparklyr 1.1](https://blog.rstudio.com/2020/01/29/sparklyr-1-1/).

Thanks for reading!
