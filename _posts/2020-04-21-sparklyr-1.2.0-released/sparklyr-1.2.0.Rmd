---
title: "Sparklyr v1.2.0 released"
description: |
  Sparklyr v1.2.0 is now available. This release features new functionalities such as support for Databricks Connect and registerDoSpark, inter-op improvements for working with Spark-3.0.0-preview, as well as a number of bug fixes and improvements addressing user-visible pain points.
author:
  - name: Yitao Li
    url: https://github.com/yl790
    affiliation: RStudio
    affiliation_url: https://www.rstudio.com
date: 04-21-2020
categories:
  - Packages/Releases
  - Distributed Computing
output:
  distill::distill_article:
    self_contained: false
preview: images/sparklyr.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Behold the glory that is `sparklyr` v1.2.0!

In this release, the following new hotnesses have emerged into spotlight:

- Support for [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), allowing `sparklyr` to connect to a Databricks cluster. This is a result of close collaboration between Javier from RStudio and Hossein, Andy, and others from DataBricks.
- A `registerDoSpark` method to create `foreach` parallel backend powered by Spark

A number of inter-op issues observed with `sparklyr` and Spark-3.0.0-preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, `sparklyr` will be fully ready to work with it. Most notably, key features such as `sparklyr::spark_submit`, `sparklyr::sdf_bind_rows`, and `sparklyr` gateway connection are now finally working with Spark-3.0.0-preview. For more gory details, see [2285](https://github.com/sparklyr/sparklyr/pull/2285), [2310](https://github.com/sparklyr/sparklyr/pull/2310), [2321](https://github.com/sparklyr/sparklyr/issues/2321), etc.

Last but not least, we heard about a number of pain points `sparklyr` users have run into, and have addressed many of them in this release as well. For example:

 -  date type in R is now correctly serialized into Spark SQL date type by `sdf_copy_to`
 -  `<spark dataframe> %>% print(n = 20)` now actually prints 20 rows as expected instead of 10
 - `spark_connect(master = "local")` will emit a more informative error message if it's failing because the loopback interface is not up

  ...  to just name a few. We wish to thank the open source community for their continuous feedback on `sparklyr`, and are looking forward to incorporating more of that feedback to make `sparklyr` even better in the future.

Finally, in chronological order, we wish to thank the following individuals for contributing to `sparklyr` v1.2.0:

zero323 <mszymkiewicz@gmail.com>,
Andy Zhang <yue.zhang@databricks.com>,
Yitao Li <yitao@rstudio.com>,
Javier Luraschi <javierluraschi@hotmail.com>,
Hossein Falaki <falaki@gmail.com>,
Lu Wang <lu.wang@databricks.com>, and
Samuel Macedo <samuelmacedo@recife.ifpe.edu.br>


Great job everyone!
