<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Innocent unicorns considered harmful? How to experiment with GPT-2 from R</title>
  
  <meta property="description" itemprop="description" content="Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won&#39;t answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI&#39;s public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-10-23"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-10-23"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  <meta name="article:author" content="Javier Luraschi"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Innocent unicorns considered harmful? How to experiment with GPT-2 from R"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won&#39;t answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI&#39;s public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Innocent unicorns considered harmful? How to experiment with GPT-2 from R"/>
  <meta property="twitter:description" content="Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won&#39;t answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI&#39;s public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Mitigating gender bias in natural language processing: Literature review;citation_publication_date=2019;citation_volume=abs/1906.08976;citation_author=Tony Sun;citation_author=Andrew Gaut;citation_author=Shirlyn Tang;citation_author=Yuxin Huang;citation_author=Mai ElSherief;citation_author=Jieyu Zhao;citation_author=Diba Mirza;citation_author=Elizabeth M. Belding;citation_author=Kai-Wei Chang;citation_author=William Yang Wang"/>
  <meta name="citation_reference" content="citation_title=Improving language understanding by generative pre-training;citation_publication_date=2018;citation_author=Alec Radford"/>
  <meta name="citation_reference" content="citation_title=Language models are unsupervised multitask learners;citation_publication_date=2019;citation_author=Alec Radford;citation_author=Jeff Wu;citation_author=Rewon Child;citation_author=David Luan;citation_author=Dario Amodei;citation_author=Ilya Sutskever"/>
  <meta name="citation_reference" content="citation_title=The curious case of neural text degeneration;citation_publication_date=2019;citation_author=Ari Holtzman;citation_author=Jan Buys;citation_author=Maxwell Forbes;citation_author=Yejin Choi"/>
  <meta name="citation_reference" content="citation_title=Logic and conversation;citation_publication_date=1975;citation_publisher=Academic Press;citation_author=H. P. Grice"/>
  <meta name="citation_reference" content="citation_title=Attention is all you need;citation_publication_date=2017;citation_publisher=Curran Associates, Inc.;citation_author=Ashish Vaswani;citation_author=Noam Shazeer;citation_author=Niki Parmar;citation_author=Jakob Uszkoreit;citation_author=Llion Jones;citation_author=Aidan N Gomez;citation_author=Lukasz Kaiser;citation_author=Illia Polosukhin"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Innocent unicorns considered harmful? How to experiment with GPT-2 from R"]},{"type":"character","attributes":{},"value":["Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations. \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Javier Luraschi"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydanaluraschi2019gpt2"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["10-23-2019"]},{"type":"character","attributes":{},"value":["Natural Language Processing","Packages/Releases"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb.jpg"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","gpt2_files/bowser-1.9.3/bowser.min.js","gpt2_files/distill-2.2.21/template.v2.js","gpt2_files/jquery-1.11.3/jquery.min.js","gpt2_files/webcomponents-2.0.0/webcomponents.js","images/thumb.jpg"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="gpt2_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="gpt2_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="gpt2_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="gpt2_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Innocent unicorns considered harmful? How to experiment with GPT-2 from R","description":"Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"},{"author":"Javier Luraschi","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2019-10-23T00:00:00.000+02:00","citationText":"Keydana & Luraschi, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Innocent unicorns considered harmful? How to experiment with GPT-2 from R</h1>
<p><p>Is society ready to deal with challenges brought about by artificially-generated information - fake images, fake videos, fake text? While this post won’t answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019. We introduce gpt2, an R package that wraps OpenAI’s public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
,   Javier Luraschi  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>10-23-2019
</div>

<div class="d-article">
<p>When this year in February, OpenAI presented <a href="https://openai.com/blog/better-language-models/">GPT-2</a><span class="citation" data-cites="radford2019language">(Radford et al. <a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span>, a large <em>Transformer</em>-based language model trained on an enormous amount of web-scraped text, their announcement caught great attention, not just in the NLP community. This was primarily due to two facts. First, the samples of generated text were stunning.</p>
<p>Presented with the following input</p>
<blockquote>
<p>In a shocking finding, scientist [sic] discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</p>
</blockquote>
<p>this was how the model continued:</p>
<blockquote>
<p>The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. […]</p>
</blockquote>
<p>Second, “due to our concerns about malicious applications” (quote) they didn’t release the full model, but a smaller one that has less than one tenth the number of parameters. Neither did they make public the dataset, nor the training code.</p>
<p>While at first glance, this may look like a marketing move (<em>we created something so powerful that it’s too dangerous to be released to the public!</em>), let’s not make things that easy on ourselves.</p>
<h2 id="with-great-power">With great power …</h2>
<p>Whatever your take on the “innate priors in deep learning” discussion – how much knowledge needs to be hardwired into neural networks for them to solve tasks that involve more than pattern matching? – there is no doubt that in many areas, systems driven by “AI”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> will impact our lives in an essential, and ever more powerful, way. Although there may be some awareness of the ethical, legal, and political problems this poses, it is probably fair to say that by and large, society is closing its eyes and holding its hands over its ears.</p>
<p>If you were a deep learning researcher working in an area susceptible to abuse, generative ML say, what options would you have? As always in the history of science, what can be done will be done; all that remains is the search for antidotes. You may doubt that on a political level, constructive responses could evolve. But you can encourage other researchers to scrutinize the artifacts your algorithm created and develop other algorithms designed to spot the fakes – essentially like in malware detection. Of course this is a feedback system: Like with GANs, impostor algorithms will happily take the feedback and go on working on their shortcomings. But still, deliberately entering this circle <em>might</em> be the only viable action to take.</p>
<p>Although it may be the first thing that comes to mind, the question of veracity here isn’t the only one. With ML systems, it’s always: garbage in - garbage out. What is fed as training data determines the quality of the output, and any biases in its upbringing will carry through to an algorithm’s grown-up behavior. Without interventions, software designed to do translation, autocompletion and the like will be biased.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>In this light, all we can sensibly do is – constantly – point out the biases, analyze the artifacts, and conduct adversarial attacks. These are the kinds of responses OpenAI was asking for. In appropriate modesty, they called their approach an <em>experiment</em>. Put plainly, no-one today knows how to deal with the threats emerging from powerful AI appearing in our lives. But there is no way around exploring our options.</p>
<h2 id="the-story-unwinding">The story unwinding</h2>
<p>Three months later, OpenAI published an update to the initial post, stating that they had decided on a staged-release strategy. In addition to making public the next-in-size, 355M-parameters version of the model, they also released a dataset of <a href="https://github.com/openai/gpt-2-output-dataset">generated outputs from all model sizes</a>, to facilitate research. Last not least, they announced partnerships with academic and non-academic institutions, to increase “societal preparedness” (quote).</p>
<p>Again after three months, in a <a href="https://openai.com/blog/gpt-2-6-month-follow-up/">new post</a> OpenAI announced the release of a yet larger – 774M-parameter – version of the model. At the same time, they reported evidence demonstrating insufficiencies in current statistical fake detection, as well as study results suggesting that indeed, text generators exist that can trick humans.</p>
<p>Due to those results, they said, no decision had yet been taken as to the release of the biggest, the “real” model, of size 1.5 billion parameters.</p>
<h2 id="gpt-2">GPT-2</h2>
<p>So what is GPT-2? Among state-of-the-art NLP models, GPT-2 stands out due to the gigantic (40G) dataset it was trained on, as well as its enormous number of weights. The architecture, in contrast, wasn’t new when it appeared. GPT-2, as well as its predecessor GPT <span class="citation" data-cites="Radford2018ImprovingLU">(Radford <a href="#ref-Radford2018ImprovingLU" role="doc-biblioref">2018</a>)</span>, is based on a transformer architecture.</p>
<p>The original Transformer <span class="citation" data-cites="NIPS2017_7181">(Vaswani et al. <a href="#ref-NIPS2017_7181" role="doc-biblioref">2017</a>)</span> is an encoder-decoder architecture designed for sequence-to-sequence tasks, like machine translation. The paper introducing it was called “Attention is all you need”, emphasizing – by absence – what you don’t need: RNNs.</p>
<p>Before its publication, the prototypical model for e.g. machine translation would use some form of RNN as an encoder, some form of RNN as a decoder, and an attention mechanism that at each time step of output generation, told the decoder where in the encoded input to look. Now the transformer was disposing with RNNs, essentially replacing them by a mechanism called <em>self-attention</em> where already during <em>encoding</em>, the encoder stack would encode each token not independently, but as a weighted sum of tokens encountered before (including itself).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Many subsequent NLP models built on the Transformer, but – depending on purpose – either picked up the encoder stack only, or just the decoder stack. GPT-2 was trained to predict consecutive words in a sequence. It is thus a <em>language model</em>, a term resounding the conception that an algorithm which can predict future words and sentences somehow has to <em>understand</em> language (and a lot more, we might add). As there is no input to be encoded (apart from an optional one-time prompt), all that is needed is the stack of decoders.</p>
<p>In our experiments, we’ll be using the biggest as-yet released pretrained model, but this being a pretrained model our degrees of freedom are limited. We can, of course, condition on different input prompts. In addition, we can influence the sampling algorithm used.</p>
<h2 id="sampling-options-with-gpt-2">Sampling options with GPT-2</h2>
<p>Whenever a new token is to be predicted, a <em>softmax</em> is taken over the vocabulary.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Directly taking the softmax output amounts to maximum likelihood estimation. In reality, however, always choosing the maximum likelihood estimate results in highly repetitive output.</p>
<p>A natural option seems to be using the softmax outputs as probabilities: Instead of just taking the <em>argmax</em>, we sample from the output distribution. Unfortunately, this procedure has negative ramifications of its own. In a big vocabulary, very improbable words together make up a substantial part of the probability mass; at every step of generation, there is thus a non-negligible probability that an improbable word may be chosen. This word will now exert great influence on what is chosen next. In that manner, highly improbable sequences can build up.</p>
<p>The task thus is to navigate between the Scylla of determinism and the Charybdis of weirdness. With the GPT-2 model presented below, we have three options:</p>
<ul>
<li>vary the <em>temperature</em> (parameter <code>temperature</code>);</li>
<li>vary <code>top_k</code>, the number of tokens considered; or</li>
<li>vary <code>top_p</code>, the probability mass considered.</li>
</ul>
<p>The <em>temperature</em> concept is rooted in statistical mechanics. Looking at the Boltzmann distribution used to model state probabilities <span class="math inline">\(p_i\)</span>dependent on energy <span class="math inline">\(\epsilon_i\)</span>:</p>
<p><span class="math display">\[p_i \sim e^{-\frac{\epsilon_i}{kT}}\]</span></p>
<p>we see there is a moderating variable <em>temperature</em> <span class="math inline">\(T\)</span><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> that dependent on whether it’s below or above 1, will exert an either amplifying or attenuating influence on differences between probabilities.</p>
<p>Analogously, in the context of predicting the next token, the individual logits are scaled by the temperature, and only then is the softmax taken. Temperatures below zero would make the model even more rigorous in choosing the maximum likelihood candidate; instead, we’d be interested in experimenting with temperatures above 1 to give higher chances to less likely candidates – hopefully, resulting in more human-like text.</p>
<p>In top-<span class="math inline">\(k\)</span> sampling, the softmax outputs are sorted, and only the top-<span class="math inline">\(k\)</span> tokens are considered for sampling. The difficulty here is how to choose <span class="math inline">\(k\)</span>. Sometimes a few words make up for almost all probability mass, in which case we’d like to choose a low number; in other cases the distribution is flat, and a higher number would be adequate.</p>
<p>This sounds like rather than the number of candidates, a target probability mass should be specified. This is the approach suggested by <span class="citation" data-cites="2019arXiv190409751H">(Holtzman et al. <a href="#ref-2019arXiv190409751H" role="doc-biblioref">2019</a>)</span>. Their method, called top-<span class="math inline">\(p\)</span>, or Nucleus sampling, computes the cumulative distribution of softmax outputs and picks a cut-off point <span class="math inline">\(p\)</span>. Only the tokens constituting the top-<span class="math inline">\(p\)</span> portion of probability mass is retained for sampling.</p>
<p>Now all you need to experiment with GPT-2 is the model.</p>
<h2 id="setup">Setup</h2>
<p>Install <code>gpt2</code> from <a href="https://github.com/r-tensorflow/gpt2">github</a>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
remotes::install_github(&quot;r-tensorflow/gpt2&quot;)</code></pre>
</div>
<p>The R package being a wrapper to the implementation <a href="https://github.com/openai/gpt-2">provided by OpenAI</a>, we then need to install the Python runtime.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
gpt2::install_gpt2(envname = &quot;r-gpt2&quot;)</code></pre>
</div>
<p>This command will also install TensorFlow into the designated environment. All TensorFlow-related installation options (resp. recommendations) apply. Python 3 is required.</p>
<p>While OpenAI indicates a dependency on TensorFlow 1.12, the R package was adapted to work with more current versions. The following versions have been found to be working fine:</p>
<ul>
<li>if running on GPU: TF 1.15</li>
<li>CPU-only: TF 2.0</li>
</ul>
<p>Unsurprisingly, with GPT-2, running on GPU vs. CPU makes a huge difference.</p>
<p>As a quick test if installation was successful, just run <code>gpt2()</code> with the default parameters:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# equivalent to:
# gpt2(prompt = &quot;Hello my name is&quot;, model = &quot;124M&quot;, seed = NULL, batch_size = 1, total_tokens = NULL,
#      temperature = 1, top_k = 0, top_p = 1)
# see ?gpt2 for an explanation of the parameters
#
# available models as of this writing: 124M, 355M, 774M
#
# on first run of a given model, allow time for download
gpt2()</code></pre>
</div>
<h2 id="things-to-try-out">Things to try out</h2>
<p>So <em>how dangerous exactly</em> is GPT-2? We can’t say, as we don’t have access to the “real” model. But we can compare outputs, given the same prompt, obtained from all available models. The number of parameters has approximately doubled at every release – 124M, 355M, 774M. The biggest, yet unreleased, model, again has twice the number of weights: about 1.5B. In light of the evolution we observe, what do we expect to get from the 1.5B version?</p>
<p>In performing these kinds of experiments, don’t forget about the different sampling strategies explained above. Non-default parameters might yield more real-looking results.</p>
<p>Needless to say, the prompt we specify will make a difference. The models have been trained on a web-scraped dataset, <a href="https://openai.com/blog/better-language-models/">subject to the quality criterion “3 stars on reddit”</a>. We expect more fluency in certain areas than in others, to put it in a cautious way.</p>
<p>Most definitely, we expect various biases in the outputs.</p>
<p>Undoubtedly, by now the reader will have her own ideas about what to test. But there is more.</p>
<h2 id="language-models-are-unsupervised-multitask-learners">“Language Models are Unsupervised Multitask Learners”</h2>
<p>Here we are citing the title of the official GPT-2 paper <span class="citation" data-cites="radford2019language">(Radford et al. <a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span>. What is that supposed to mean? It means that a model like GPT-2, trained to predict the next token in naturally occurring text, can be used to “solve” standard NLP tasks that, in the majority of cases, are approached via supervised training (translation, for example).</p>
<p>The clever idea is to present the model with cues about the task at hand. Some information on how to do this is given in the paper; more (unofficial; conflicting or confirming) hints can be found on the net. From what we found, here are some things you could try.</p>
<h3 id="summarization">Summarization</h3>
<p>The clue to induce summarization is “TL;DR:”, written on a line by itself. The authors report that this worked best setting <code>top_k = 2</code> and asking for 100 tokens. Of the generated output, they took the first three sentences as a summary.</p>
<p>To try this out, we chose a sequence of content-wise standalone paragraphs from <a href="https://climate.nasa.gov/evidence/">a NASA website dedicated to climate change</a>, the idea being that with a clearly structured text like this, it should be easier to establish relationships between input and output.</p>
<pre><code>
# put this in a variable called text

The planet&#39;s average surface temperature has risen about 1.62 degrees Fahrenheit
(0.9 degrees Celsius) since the late 19th century, a change driven largely by
increased carbon dioxide and other human-made emissions into the atmosphere.4 Most
of the warming occurred in the past 35 years, with the five warmest years on record
taking place since 2010. Not only was 2016 the warmest year on record, but eight of
the 12 months that make up the year — from January through September, with the
exception of June — were the warmest on record for those respective months.

The oceans have absorbed much of this increased heat, with the top 700 meters
(about 2,300 feet) of ocean showing warming of more than 0.4 degrees Fahrenheit
since 1969.

The Greenland and Antarctic ice sheets have decreased in mass. Data from NASA&#39;s
Gravity Recovery and Climate Experiment show Greenland lost an average of 286
billion tons of ice per year between 1993 and 2016, while Antarctica lost about 127
billion tons of ice per year during the same time period. The rate of Antarctica
ice mass loss has tripled in the last decade.

Glaciers are retreating almost everywhere around the world — including in the Alps,
Himalayas, Andes, Rockies, Alaska and Africa.

Satellite observations reveal that the amount of spring snow cover in the Northern
Hemisphere has decreased over the past five decades and that the snow is melting
earlier.

Global sea level rose about 8 inches in the last century. The rate in the last two
decades, however, is nearly double that of the last century and is accelerating
slightly every year.

Both the extent and thickness of Arctic sea ice has declined rapidly over the last
several decades.

The number of record high temperature events in the United States has been
increasing, while the number of record low temperature events has been decreasing,
since 1950. The U.S. has also witnessed increasing numbers of intense rainfall events.

Since the beginning of the Industrial Revolution, the acidity of surface ocean
waters has increased by about 30 percent.13,14 This increase is the result of humans
emitting more carbon dioxide into the atmosphere and hence more being absorbed into
the oceans. The amount of carbon dioxide absorbed by the upper layer of the oceans
is increasing by about 2 billion tons per year.

TL;DR:</code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
gpt2(prompt = text,
     model = &quot;774M&quot;,
     total_tokens = 100,
     top_k = 2)</code></pre>
</div>
<p>Here is the generated result, whose quality on purpose we don’t comment on. (Of course one can’t help having “gut reactions”; but to actually present an evaluation we’d want to conduct a systematic experiment, varying not only input prompts but also function parameters. All we want to show in this post is how you can set up such experiments yourself.)</p>
<pre><code>
&quot;\nGlobal temperatures are rising, but the rate of warming has been accelerating.
\n\nThe oceans have absorbed much of the increased heat, with the top 700 meters of
ocean showing warming of more than 0.4 degrees Fahrenheit since 1969.
\n\nGlaciers are retreating almost everywhere around the world, including in the
Alps, Himalayas, Andes, Rockies, Alaska and Africa.
\n\nSatellite observations reveal that the amount of spring snow cover in the
Northern Hemisphere has decreased over the past&quot;</code></pre>
<p>Speaking of parameters to vary, – they fall into two classes, in a way. It is unproblematic to vary the sampling strategy, let alone the prompt. But for tasks like summarization, or the ones we’ll see below, it doesn’t feel right to have to tell the model how many tokens to generate. Finding the right length of the answer seems to be part of the task.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Breaking our “we don’t judge” rule just a single time, we can’t help but remark that even in less clear-cut tasks, language generation models that are meant to approach human-level competence would have to fulfill a criterion of <em>relevance</em> <span class="citation" data-cites="grice1975logic">(Grice <a href="#ref-grice1975logic" role="doc-biblioref">1975</a>)</span>.</p>
<h3 id="question-answering">Question answering</h3>
<p>To trick GPT-2 into question answering, the common approach seems to be presenting it with a number of <em>Q:</em> / <em>A:</em> pairs, followed by a final question and a final <em>A:</em> on its own line.</p>
<p>We tried like this, asking questions on the above climate change - related text:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
q &lt;- str_c(str_replace(text, &quot;\nTL;DR:\n&quot;, &quot;&quot;), &quot; \n&quot;, &quot;
Q: What time period has seen the greatest increase in global temperature? 
A: The last 35 years. 
Q: What is happening to the Greenland and Antarctic ice sheets? 
A: They are rapidly decreasing in mass. 
Q: What is happening to glaciers? 
A: &quot;)

gpt2(prompt = q,
     model = &quot;774M&quot;,
     total_tokens = 10,
     top_p = 0.9)</code></pre>
</div>
<p>This did not turn out so well.</p>
<pre><code>
&quot;\nQ: What is happening to the Arctic sea&quot;</code></pre>
<p>But maybe, more successful tricks exist.</p>
<h3 id="translation">Translation</h3>
<p>For translation, the strategy presented in the paper is juxtaposing sentences in two languages, joined by " = “, followed by a single sentence on its own and a” =". Thinking that English &lt;-&gt; French might be the combination best represented in the training corpus, we tried the following:</p>
<pre><code>
# save this as eng_fr

The issue of climate change concerns all of us. = La question du changement
climatique nous affecte tous. \n
The problems of climate change and global warming affect all of humanity, as well as
the entire ecosystem. = Les problèmes créés par les changements climatiques et le
réchauffement de la planète touchent toute l&#39;humanité, de même que l&#39;écosystème tout
entier.\n
Climate Change Central is a not-for-profit corporation in Alberta, and its mandate
is to reduce Alberta&#39;s greenhouse gas emissions. = Climate Change Central est une
société sans but lucratif de l&#39;Alberta ayant pour mission de réduire les émissions
de gaz. \n
Climate change will affect all four dimensions of food security: food availability,
food accessibility, food utilization and food systems stability. = &quot;

gpt2(prompt = eng_fr,
     model = &quot;774M&quot;,
     total_tokens = 25,
     top_p = 0.9)</code></pre>
<p>Results varied a lot between different runs. Here are three examples:</p>
<pre><code>
&quot;ét durant les pages relevantes du Centre d&#39;Action des Sciences Humaines et dans sa
species situé,&quot;

&quot;études des loi d&#39;affaires, des reasons de demande, des loi d&#39;abord and de&quot;

&quot;étiquettes par les changements changements changements et les bois d&#39;escalier,
ainsi que des&quot;</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>With that, we conclude our tour of “what to explore with GPT-2”. Keep in mind that the yet-unreleased model has double the number of parameters; essentially, <em>what we see is not what we get</em>.</p>
<p>This post’s goal was to show how you can experiment with GPT-2 from R. But it also reflects the decision to, from time to time, widen the narrow focus on technology and allow ourselves to think about ethical and societal implications of ML/DL.</p>
<p>Thanks for reading!</p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-grice1975logic">
<p>Grice, H. P. 1975. “Logic and Conversation.” In <em>Syntax and Semantics: Vol. 3: Speech Acts</em>, 41–58. Academic Press. <a href="http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf">http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf</a>.</p>
</div>
<div id="ref-2019arXiv190409751H">
<p>Holtzman, Ari, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. “The Curious Case of Neural Text Degeneration.” <em>arXiv E-Prints</em>, April, arXiv:1904.09751. <a href="http://arxiv.org/abs/1904.09751">http://arxiv.org/abs/1904.09751</a>.</p>
</div>
<div id="ref-Radford2018ImprovingLU">
<p>Radford, Alec. 2018. “Improving Language Understanding by Generative Pre-Training.” In.</p>
</div>
<div id="ref-radford2019language">
<p>Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.”</p>
</div>
<div id="ref-abs-1906-08976">
<p>Sun, Tony, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2019. “Mitigating Gender Bias in Natural Language Processing: Literature Review.” <em>CoRR</em> abs/1906.08976. <a href="http://arxiv.org/abs/1906.08976">http://arxiv.org/abs/1906.08976</a>.</p>
</div>
<div id="ref-NIPS2017_7181">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The acronym here is used for convenience only, not to imply any specific view on what is, or is not, “artificial intelligence”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>For an overview of bias detection and mitigation specific to gender bias, see e.g. <span class="citation" data-cites="abs-1906-08976">(Sun et al. <a href="#ref-abs-1906-08976" role="doc-biblioref">2019</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>For a detailed, and exceptionally visual, explanation of the Transformer, <em>the</em> place to go is <a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s post</a>. Also check out <a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning</a>, the article that might be held mainly responsible for the pervasive sesame-streetification of NLP.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>For an introduction to how softmax activation behaves, see <a href="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/">Winner takes all: A look at activations and cost functions</a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><span class="math inline">\(k\)</span> is the Boltzmann constant<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Formally, <code>total_tokens</code> isn’t a required parameter. If not passed, a default based on model size will be applied, resulting in lengthy output that definitely will have to be processed by some human-made rule.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{abs-1906-08976,
  author    = {Tony Sun and
    Andrew Gaut and
    Shirlyn Tang and
    Yuxin Huang and
    Mai ElSherief and
    Jieyu Zhao and
    Diba Mirza and
    Elizabeth M. Belding and
    Kai-Wei Chang and
    William Yang Wang},
  title     = {Mitigating Gender Bias in Natural Language Processing: Literature
    Review},
  journal   = {CoRR},
  volume    = {abs/1906.08976},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.08976},
  archivePrefix = {arXiv},
  eprint    = {1906.08976},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-08976},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford},
  year={2018}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@ARTICLE{2019arXiv190409751H,
       author = {Holtzman, Ari and Buys, Jan and Forbes, Maxwell and Choi, Yejin},
        title = "The Curious Case of Neural Text Degeneration",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.09751},
        pages = {arXiv:1904.09751},
archivePrefix = {arXiv},
       eprint = {1904.09751},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190409751H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@incollection{grice1975logic,
  author = {Grice, H. P.},
  booktitle = {Syntax and Semantics: Vol. 3: Speech Acts},
  pages = {41-58},
  publisher = {Academic Press},
  title = {Logic and Conversation},
  url = {http://www.ucl.ac.uk/ls/studypacks/Grice-Logic.pdf},
  year = {1975}
}

@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
