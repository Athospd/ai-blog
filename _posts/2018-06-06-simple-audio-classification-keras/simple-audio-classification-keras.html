<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Simple Audio Classification with Keras</title>
  
  <meta property="description" itemprop="description" content="In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2018-06-06"/>
  <meta property="article:created" itemprop="dateCreated" content="2018-06-06"/>
  <meta name="article:author" content="Daniel Falbel"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Simple Audio Classification with Keras"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Simple Audio Classification with Keras"/>
  <meta property="twitter:description" content="In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words."/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","preview","categories","output"]}},"value":[{"type":"character","attributes":{},"value":["Simple Audio Classification with Keras"]},{"type":"character","attributes":{},"value":["In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Daniel Falbel"]},{"type":"character","attributes":{},"value":["https://github.com/dfalbel"]},{"type":"character","attributes":{},"value":["Curso-R"]},{"type":"character","attributes":{},"value":["http://curso-r.com/"]}]}]},{"type":"character","attributes":{},"value":["06-06-2018"]},{"type":"character","attributes":{},"value":["https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png"]},{"type":"character","attributes":{},"value":["Keras","Examples","Audio","Datasets"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/alluvial.png","simple-audio-classification-keras_files/bowser-1.9.3/bowser.min.js","simple-audio-classification-keras_files/distill-2.2.21/template.v2.js","simple-audio-classification-keras_files/jquery-1.11.3/jquery.min.js","simple-audio-classification-keras_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="simple-audio-classification-keras_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="simple-audio-classification-keras_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="simple-audio-classification-keras_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="simple-audio-classification-keras_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Simple Audio Classification with Keras","description":"In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.","authors":[{"author":"Daniel Falbel","authorURL":"https://github.com/dfalbel","affiliation":"Curso-R","affiliationURL":"http://curso-r.com/"}],"publishedDate":"2018-06-06T00:00:00.000+02:00","citationText":"Falbel, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Simple Audio Classification with Keras</h1>
<p>In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.</p>
</div>

<div class="d-byline">
  Daniel Falbel <a href="https://github.com/dfalbel" class="uri">https://github.com/dfalbel</a> (Curso-R)<a href="http://curso-r.com/" class="uri">http://curso-r.com/</a>
  
<br/>06-06-2018
</div>

<div class="d-article">
<h2 id="introduction">Introduction</h2>
<p>In this tutorial we will build a deep learning model to classify words. We will use <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a> to handle data IO and pre-processing, and <a href="https://keras.rstudio.com">Keras</a> to build and train the model.</p>
<p>We will use the <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">Speech Commands dataset</a> which consists of 65,000 one-second audio files of people saying 30 different words. Each file contains a single spoken English word. The dataset was released by Google under CC License.</p>
<p>Our model is a Keras port of the <a href="https://www.tensorflow.org/tutorials/audio_recognition#top_of_page">TensorFlow tutorial on <em>Simple Audio Recognition</em></a> which in turn was inspired by <a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf"><em>Convolutional Neural Networks for Small-footprint Keyword Spotting</em></a>. There are other approaches to the speech recognition task, like <a href="https://svds.com/tensorflow-rnn-tutorial/">recurrent neural networks</a>, <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">dilated (atrous) convolutions</a> or <a href="https://arxiv.org/abs/1711.10282">Learning from Between-class Examples for Deep Sound Recognition</a>.</p>
<p>The model we will implement here is not the state of the art for audio recognition systems, which are way more complex, but is relatively simple and fast to train. Plus, we show how to efficiently use <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a> to preprocess and serve data.</p>
<h2 id="audio-representation">Audio representation</h2>
<p>Many deep learning models are end-to-end, i.e. we let the model learn useful representations directly from the raw data. However, audio data grows very fast - 16,000 samples per second with a very rich structure at many time-scales. In order to avoid having to deal with raw wave sound data, researchers usually use some kind of feature engineering.</p>
<p>Every sound wave can be represented by its spectrum, and digitally it can be computed using the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">Fast Fourier Transform (FFT)</a>.</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png" alt="By Phonical - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64473578" /><figcaption>By Phonical - Own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=64473578" class="uri">https://commons.wikimedia.org/w/index.php?curid=64473578</a></figcaption>
</figure>
<p>A common way to represent audio data is to break it into small chunks, which usually overlap. For each chunk we use the FFT to calculate the magnitude of the frequency spectrum. The spectra are then combined, side by side, to form what we call a <a href="https://en.wikipedia.org/wiki/Spectrogram"><strong>spectrogram</strong></a>.</p>
<p>It’s also common for speech recognition systems to further transform the spectrum and compute the <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-Frequency Cepstral Coefficients</a>. This transformation takes into account that the human ear can’t discern the difference between two closely spaced frequencies and smartly creates bins on the frequency axis. A great tutorial on MFCCs can be found <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">here</a>.</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png" alt="By Aquegg - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=5544473" /><figcaption>By Aquegg - Own work, Public Domain, <a href="https://commons.wikimedia.org/w/index.php?curid=5544473" class="uri">https://commons.wikimedia.org/w/index.php?curid=5544473</a></figcaption>
</figure>
<p>After this procedure, we have an image for each audio sample and we can use convolutional neural networks, the standard architecture type in image recognition models.</p>
<h2 id="downloading">Downloading</h2>
<p>First, let’s download data to a directory in our project. You can either download from <a href="http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz">this link</a> (~1GB) or from R with:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dir.create(&quot;data&quot;)

download.file(
  url = &quot;http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz&quot;, 
  destfile = &quot;data/speech_commands_v0.01.tar.gz&quot;
)

untar(&quot;data/speech_commands_v0.01.tar.gz&quot;, exdir = &quot;data/speech_commands_v0.01&quot;)</code></pre>
</div>
<p>Inside the <code>data</code> directory we will have a folder called <code>speech_commands_v0.01</code>. The WAV audio files inside this directory are organised in sub-folders with the label names. For example, all one-second audio files of people speaking the word “bed” are inside the <code>bed</code> directory. There are 30 of them and a special one called <code>_background_noise_</code> which contains various patterns that could be mixed in to simulate background noise.</p>
<h2 id="importing">Importing</h2>
<p>In this step we will list all audio .wav files into a <code>tibble</code> with 3 columns:</p>
<ul>
<li><code>fname</code>: the file name;</li>
<li><code>class</code>: the label for each audio file;</li>
<li><code>class_id</code>: a unique integer number starting from zero for each class - used to one-hot encode the classes.</li>
</ul>
<p>This will be useful to the next step when we will create a generator using the <code>tfdatasets</code> package.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(stringr)
library(dplyr)

files &lt;- fs::dir_ls(
  path = &quot;data/speech_commands_v0.01/&quot;, 
  recursive = TRUE, 
  glob = &quot;*.wav&quot;
)

files &lt;- files[!str_detect(files, &quot;background_noise&quot;)]

df &lt;- data_frame(
  fname = files, 
  class = fname %&gt;% str_extract(&quot;1/.*/&quot;) %&gt;% 
    str_replace_all(&quot;1/&quot;, &quot;&quot;) %&gt;%
    str_replace_all(&quot;/&quot;, &quot;&quot;),
  class_id = class %&gt;% as.factor() %&gt;% as.integer() - 1L
)</code></pre>
</div>
<h2 id="generator">Generator</h2>
<p>We will now create our <code>Dataset</code>, which in the context of <code>tfdatasets</code>, adds operations to the TensorFlow graph in order to read and pre-process data. Since they are TensorFlow ops, they are executed in C++ and in parallel with model training.</p>
<p>The generator we will create will be responsible for reading the audio files from disk, creating the spectrogram for each one and batching the outputs.</p>
<p>Let’s start by creating the dataset from slices of the <code>data.frame</code> with audio file names and classes we just created.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tfdatasets)
ds &lt;- tensor_slices_dataset(df) </code></pre>
</div>
<p>Now, let’s define the parameters for spectrogram creation. We need to define <code>window_size_ms</code> which is the size in milliseconds of each chunk we will break the audio wave into, and <code>window_stride_ms</code>, the distance between the centers of adjacent chunks:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
window_size_ms &lt;- 30
window_stride_ms &lt;- 10</code></pre>
</div>
<p>Now we will convert the window size and stride from milliseconds to samples. We are considering that our audio files have 16,000 samples per second (1000 ms).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
window_size &lt;- as.integer(16000*window_size_ms/1000)
stride &lt;- as.integer(16000*window_stride_ms/1000)</code></pre>
</div>
<p>We will obtain other quantities that will be useful for spectrogram creation, like the number of chunks and the FFT size, i.e., the number of bins on the frequency axis. The function we are going to use to compute the spectrogram doesn’t allow us to change the FFT size and instead by default uses the first power of 2 greater than the window size.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
fft_size &lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
n_chunks &lt;- length(seq(window_size/2, 16000 - window_size/2, stride))</code></pre>
</div>
<p>We will now use <code>dataset_map</code> which allows us to specify a pre-processing function for each observation (line) of our dataset. It’s in this step that we read the raw audio file from disk and create its spectrogram and the one-hot encoded response vector.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# shortcuts to used TensorFlow modules.
audio_ops &lt;- tf$contrib$framework$python$ops$audio_ops

ds &lt;- ds %&gt;%
  dataset_map(function(obs) {
    
    # a good way to debug when building tfdatsets pipelines is to use a print
    # statement like this:
    # print(str(obs))
    
    # decoding wav files
    audio_binary &lt;- tf$read_file(tf$reshape(obs$fname, shape = list()))
    wav &lt;- audio_ops$decode_wav(audio_binary, desired_channels = 1)
    
    # create the spectrogram
    spectrogram &lt;- audio_ops$audio_spectrogram(
      wav$audio, 
      window_size = window_size, 
      stride = stride,
      magnitude_squared = TRUE
    )
    
    # normalization
    spectrogram &lt;- tf$log(tf$abs(spectrogram) + 0.01)
    
    # moving channels to last dim
    spectrogram &lt;- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))
    
    # transform the class_id into a one-hot encoded vector
    response &lt;- tf$one_hot(obs$class_id, 30L)
    
    list(spectrogram, response)
  }) </code></pre>
</div>
<p>Now, we will specify how we want batch observations from the dataset. We’re using <code>dataset_shuffle</code> since we want to shuffle observations from the dataset, otherwise it would follow the order of the <code>df</code> object. Then we use <code>dataset_repeat</code> in order to tell TensorFlow that we want to keep taking observations from the dataset even if all observations have already been used. And most importantly here, we use <code>dataset_padded_batch</code> to specify that we want batches of size 32, but they should be padded, ie. if some observation has a different size we pad it with zeroes. The padded shape is passed to <code>dataset_padded_batch</code> via the <code>padded_shapes</code> argument and we use <code>NULL</code> to state that this dimension doesn’t need to be padded.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ds &lt;- ds %&gt;% 
  dataset_shuffle(buffer_size = 100) %&gt;%
  dataset_repeat() %&gt;%
  dataset_padded_batch(
    batch_size = 32, 
    padded_shapes = list(
      shape(n_chunks, fft_size, NULL), 
      shape(NULL)
    )
  )</code></pre>
</div>
<p>This is our dataset specification, but we would need to rewrite all the code for the validation data, so it’s good practice to wrap this into a function of the data and other important parameters like <code>window_size_ms</code> and <code>window_stride_ms</code>. Below, we will define a function called <code>data_generator</code> that will create the generator depending on those inputs.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
data_generator &lt;- function(df, batch_size, shuffle = TRUE, 
                           window_size_ms = 30, window_stride_ms = 10) {
  
  window_size &lt;- as.integer(16000*window_size_ms/1000)
  stride &lt;- as.integer(16000*window_stride_ms/1000)
  fft_size &lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
  n_chunks &lt;- length(seq(window_size/2, 16000 - window_size/2, stride))
  
  ds &lt;- tensor_slices_dataset(df)
  
  if (shuffle) 
    ds &lt;- ds %&gt;% dataset_shuffle(buffer_size = 100)  
  
  ds &lt;- ds %&gt;%
    dataset_map(function(obs) {
      
      # decoding wav files
      audio_binary &lt;- tf$read_file(tf$reshape(obs$fname, shape = list()))
      wav &lt;- audio_ops$decode_wav(audio_binary, desired_channels = 1)
      
      # create the spectrogram
      spectrogram &lt;- audio_ops$audio_spectrogram(
        wav$audio, 
        window_size = window_size, 
        stride = stride,
        magnitude_squared = TRUE
      )
      
      spectrogram &lt;- tf$log(tf$abs(spectrogram) + 0.01)
      spectrogram &lt;- tf$transpose(spectrogram, perm = c(1L, 2L, 0L))
      
      # transform the class_id into a one-hot encoded vector
      response &lt;- tf$one_hot(obs$class_id, 30L)
      
      list(spectrogram, response)
    }) %&gt;%
    dataset_repeat()
  
  ds &lt;- ds %&gt;% 
    dataset_padded_batch(batch_size, list(shape(n_chunks, fft_size, NULL), shape(NULL)))
  
  ds
}</code></pre>
</div>
<p>Now, we can define training and validation data generators. It’s worth noting that executing this won’t actually compute any spectrogram or read any file. It will only define in the TensorFlow graph how it should read and pre-process data.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
set.seed(6)
id_train &lt;- sample(nrow(df), size = 0.7*nrow(df))

ds_train &lt;- data_generator(
  df[id_train,], 
  batch_size = 32, 
  window_size_ms = 30, 
  window_stride_ms = 10
)
ds_validation &lt;- data_generator(
  df[-id_train,], 
  batch_size = 32, 
  shuffle = FALSE, 
  window_size_ms = 30, 
  window_stride_ms = 10
)</code></pre>
</div>
<p>To actually get a batch from the generator we could create a TensorFlow session and ask it to run the generator. For example:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
sess &lt;- tf$Session()
batch &lt;- next_batch(ds_train)
str(sess$run(batch))</code></pre>
</div>
<pre><code>
List of 2
 $ : num [1:32, 1:98, 1:257, 1] -4.6 -4.6 -4.61 -4.6 -4.6 ...
 $ : num [1:32, 1:30] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<p>Each time you run <code>sess$run(batch)</code> you should see a different batch of observations.</p>
<h2 id="model-definition">Model definition</h2>
<p>Now that we know how we will feed our data we can focus on the model definition. The spectrogram can be treated like an image, so architectures that are commonly used in image recognition tasks should work well with the spectrograms too.</p>
<p>We will build a convolutional neural network similar to what we have built <a href="https://keras.rstudio.com/articles/examples/mnist_cnn.html">here</a> for the MNIST dataset.</p>
<p>The input size is defined by the number of chunks and the FFT size. Like we explained earlier, they can be obtained from the <code>window_size_ms</code> and <code>window_stride_ms</code> used to generate the spectrogram.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
window_size &lt;- as.integer(16000*window_size_ms/1000)
stride &lt;- as.integer(16000*window_stride_ms/1000)
fft_size &lt;- as.integer(2^trunc(log(window_size, 2)) + 1)
n_chunks &lt;- length(seq(window_size/2, 16000 - window_size/2, stride))</code></pre>
</div>
<p>We will now define our model using the Keras sequential API:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential()
model %&gt;%  
  layer_conv_2d(input_shape = c(n_chunks, fft_size, 1), 
                filters = 32, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% 
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% 
  layer_conv_2d(filters = 256, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% 
  layer_dropout(rate = 0.25) %&gt;% 
  layer_flatten() %&gt;% 
  layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% 
  layer_dropout(rate = 0.5) %&gt;% 
  layer_dense(units = 30, activation = &#39;softmax&#39;)</code></pre>
</div>
<p>We used 4 layers of convolutions combined with max pooling layers to extract features from the spectrogram images and 2 dense layers at the top. Our network is comparatively simple when compared to more advanced architectures like ResNet or DenseNet that perform very well on image recognition tasks.</p>
<p>Now let’s compile our model. We will use categorical cross entropy as the loss function and use the Adadelta optimizer. It’s also here that we define that we will look at the accuracy metric during training.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
</div>
<h2 id="model-fitting">Model fitting</h2>
<p>Now, we will fit our model. In Keras we can use TensorFlow Datasets as inputs to the <code>fit_generator</code> function and we will do it here.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;% fit_generator(
  generator = ds_train,
  steps_per_epoch = 0.7*nrow(df)/32,
  epochs = 10, 
  validation_data = ds_validation, 
  validation_steps = 0.3*nrow(df)/32
)</code></pre>
</div>
<pre><code>
Epoch 1/10
1415/1415 [==============================] - 87s 62ms/step - loss: 2.0225 - acc: 0.4184 - val_loss: 0.7855 - val_acc: 0.7907
Epoch 2/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.8781 - acc: 0.7432 - val_loss: 0.4522 - val_acc: 0.8704
Epoch 3/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.6196 - acc: 0.8190 - val_loss: 0.3513 - val_acc: 0.9006
Epoch 4/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.4958 - acc: 0.8543 - val_loss: 0.3130 - val_acc: 0.9117
Epoch 5/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.4282 - acc: 0.8754 - val_loss: 0.2866 - val_acc: 0.9213
Epoch 6/10
1415/1415 [==============================] - 76s 53ms/step - loss: 0.3852 - acc: 0.8885 - val_loss: 0.2732 - val_acc: 0.9252
Epoch 7/10
1415/1415 [==============================] - 75s 53ms/step - loss: 0.3566 - acc: 0.8991 - val_loss: 0.2700 - val_acc: 0.9269
Epoch 8/10
1415/1415 [==============================] - 76s 54ms/step - loss: 0.3364 - acc: 0.9045 - val_loss: 0.2573 - val_acc: 0.9284
Epoch 9/10
1415/1415 [==============================] - 76s 53ms/step - loss: 0.3220 - acc: 0.9087 - val_loss: 0.2537 - val_acc: 0.9323
Epoch 10/10
1415/1415 [==============================] - 76s 54ms/step - loss: 0.2997 - acc: 0.9150 - val_loss: 0.2582 - val_acc: 0.9323</code></pre>
<p>The model’s accuracy is 93.23%. Let’s learn how to make predictions and take a look at the confusion matrix.</p>
<h2 id="making-predictions">Making predictions</h2>
<p>We can use the<code>predict_generator</code> function to make predictions on a new dataset. Let’s make predictions for our validation dataset. The <code>predict_generator</code> function needs a step argument which is the number of times the generator will be called.</p>
<p>We can calculate the number of steps by knowing the batch size, and the size of the validation dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
df_validation &lt;- df[-id_train,]
n_steps &lt;- nrow(df_validation)/32 + 1</code></pre>
</div>
<p>We can then use the <code>predict_generator</code> function:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
predictions &lt;- predict_generator(
  model, 
  ds_validation, 
  steps = n_steps
  )
str(predictions)</code></pre>
</div>
<pre><code>
num [1:19424, 1:30] 1.22e-13 7.30e-19 5.29e-10 6.66e-22 1.12e-17 ...</code></pre>
<p>This will output a matrix with 30 columns - one for each word and n_steps*batch_size number of rows. Note that it starts repeating the dataset at the end to create a full batch.</p>
<p>We can compute the predicted class by taking the column with the highest probability, for example.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
classes &lt;- apply(predictions, 1, which.max) - 1</code></pre>
</div>
<p>A nice visualization of the confusion matrix is to create an alluvial diagram:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(dplyr)
library(alluvial)
x &lt;- df_validation %&gt;%
  mutate(pred_class_id = head(classes, nrow(df_validation))) %&gt;%
  left_join(
    df_validation %&gt;% distinct(class_id, class) %&gt;% rename(pred_class = class),
    by = c(&quot;pred_class_id&quot; = &quot;class_id&quot;)
  ) %&gt;%
  mutate(correct = pred_class == class) %&gt;%
  count(pred_class, class, correct)

alluvial(
  x %&gt;% select(class, pred_class),
  freq = x$n,
  col = ifelse(x$correct, &quot;lightblue&quot;, &quot;red&quot;),
  border = ifelse(x$correct, &quot;lightblue&quot;, &quot;red&quot;),
  alpha = 0.6,
  hide = x$n &lt; 20
)</code></pre>
</div>
<figure>
<img src="images/alluvial.png" alt="Alluvial Plot" /><figcaption>Alluvial Plot</figcaption>
</figure>
<p>We can see from the diagram that the most relevant mistake our model makes is to classify “tree” as “three”. There are other common errors like classifying “go” as “no”, “up” as “off”. At 93% accuracy for 30 classes, and considering the errors we can say that this model is pretty reasonable.</p>
<p>The saved model occupies 25Mb of disk space, which is reasonable for a desktop but may not be on small devices. We could train a smaller model, with fewer layers, and see how much the performance decreases.</p>
<p>In speech recognition tasks its also common to do some kind of data augmentation by mixing a background noise to the spoken audio, making it more useful for real applications where it’s common to have other irrelevant sounds happening in the environment.</p>
<p>The full code to reproduce this tutorial is available <a href="https://github.com/dfalbel/speech-keras">here</a>.</p>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
