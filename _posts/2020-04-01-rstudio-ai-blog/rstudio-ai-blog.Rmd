---
title: "AI without the hype"
description: > 
  This blog just got a new title: RStudio AI Blog. While there are practical reasons for it, we take this as an  occasion to say a few words: What we don't mean to imply; a conception of AI we like; and how we think transparency and control can help preserve the human sense of vigilance and responsibility required in world of ever-more-present AI applications.
author:
  - name: Sigrid Keydana
    affiliation: RStudio
    affiliation_url: https://www.rstudio.com/
slug: keydana2020rstudioaiblog
date: 04-01-2020
categories:
  - AI in Society
bibliography: bibliography.bib
output:
  distill::distill_article:
    self_contained: false
preview: images/thumb.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

Why the new name, *RStudio AI blog*? There is a straightforward reason. The
previous title, "TensorFlow for R Blog", used to be a good match for the content
we've been covering so far: Mainly, technical or applied aspects of doing deep
learning with TensorFlow and Keras. But our team is not working exclusively in
those areas; instead, enabling distributed computing from R
([sparklyr](https://github.com/sparklyr/sparklyr)), integrating automated
machine learning workflows ([mlflow](https://github.com/mlflow/mlflow)), and
optimizing data ingestion ([pins](https://github.com/rstudio/pins)) are
substantial aspects of what we do. So, we'd like to have a platform we can use
to tell you about our work in these areas, as well. Secondly, regarding the
hitherto dominant topic on this blog, *deep learning*, we might want to more
frequently reflect about it in a less technical way, focussing on impacts on
society, ethics, or even "just" epistemic questions. (Yes, full discIosure, I'm
taking this as an occasion to write such a "reflective" kind of post right now.)

So we needed a new name, but why "AI"? To subsume all of the above-mentioned
areas, *data science* would have made for a good term. [Citing Michael
Jordan](https://hdsr.mitpress.mit.edu/pub/wot7mkc1), whose ideas I'll refer to
more frequently in this post,

> Data science is an umbrella that brings statisticians together with database
> and distributed systems researchers. It brings together inference, data,
> modeling, and scalability. [@Jordan2019Artificial]

However, the "science" in data science brings up connotations of formality and
theoretic ambitions we'd rather avoid. If we go hype, why not go the full way?
So: artificial intelligence, or *AI*. Everything is AI now, and everybody is an
AI engineer, so no immoderate aspirations there. **But**, let's get into
discussing how we mean it.

I'll start from what we do *not* mean. By no means do we see artificial
intelligence as an end in itself, as some people do. "Building an intelligent
machine", as an end in itself, just how can that even be a dream? Well, perhaps
I should not pose that question; as someone who only ever read a single book of
science fiction (Stanislaw Lem's *Solaris*, and even that just because of the
film) I'm bound to lack understanding. As a dream, why not. But from a thing
billions of dollars are spent on each year, we may want to demand more, namely,
that these intelligent machines be good for humanity: That they serve the health
sciences, for example; mitigate or even counteract (if possible) climate change;
decrease inequality and injustice in the world; or just somehow improve life on
earth.

#### AI as a new engineering discipline (M. Jordan)

Instead, we favor the views proferred by the aforementioned [Michael
Jordan](https://en.wikipedia.org/wiki/Michael_I._Jordan), who is not just a
prominent figure in computer science and statistics, but also one of the
soundest voices in the field, often stating things that common sense suggests
and that should be obvious, but unfortunately, are not, in our society. Jordan,
who prefers the term *machine learning* over *artificial intelligence*, but
would obviously rather discuss content than arbitrary terminology
[@Jordan2019Artificial], envisions a new engineering discipline that builds on
existing knowledge about inference, optimization, computation, and uncertainty
the way that chemical engineering and civil engineering built upon chemistry and
physics, respectively. Supplementing those building blocks (from mathematics,
statistics, computer science), this new discipline includes guidance from the
social sciences and the humanities. (How exactly that's going to look remains to
be discussed in greater detail; and there are additional disciplines to be added
-- last not least, law.)

As Jordan puts it,

> While the building blocks are in place, the principles for putting these
> blocks together are not, and so the blocks are currently being put together in
> ad-hoc ways. [@Jordan2019Artificial]

This *ad-hocism* makes it hard to know what, and when, to trust. Sometimes,
neural networks output probabilities (classifiers, for example, do); sometimes
they even indicate uncertainty (for example, when the output is a distribution).
But often, this is not enough information to actually *make a decision*: and
make decisions we often have to, in real life. So, a lot of creative work is to
be done in this new discipline.

In the meantime, while effort is directed towards creating safe, useful, and
usable AI, there are things to be avoided. The following are all simple,
common-sense things that nonetheless are heard far too seldom.

#### Avoid anthropomorphism and stay vigilant

For AI to be safe to use, it is paramount that people stay aware of what can,
and what cannot, at a given point in time, be accomplished by it -- in other
words, to stay vigilant and keep critical judgment. As Daniel Dennett [@Dennett
pp.Â 402-405] points out, this is made easier if AI devices are *not* designed to
"act" in a way that makes them look more authoritative, competent, and
all-knowing than they are. People are already more than happy to attribute
intentions and feelings to non-living entities[^1]; endowing artificial agents
(e.g., expert systems in the form of chatbots) with anthropomorphic features is
likely to make humans cede judgement to them far too easily.

#### Let users stay in control

Closely related to this awareness -- being an "empowered citizen" in the state
of AI -- is the topic of *transparency* and *control*. Dennett cites Douglas
Hofstadter, expressing, in an open letter to a former student then working at
Google, his dismay at the then-new "search auto-suggest" functionality:

> I want machines to be reliably mechanical, not to be constantly slipping away
> from what I ask them to do. [...]
>
> To me, this kind of attempt to read my mind is fantastically annoying if not
> dangerous, because it almost never is correct or even in the right ballpark. I
> want machines to remain reliably mechanical, so that I know for sure what I am
> dealing with. I don't want them to try to "outsmart" me, [...]

That letter was written in 2010, and ten years later, hardly anyone still
notices functionality like that. In the meantime, we have become used to
substantial loss of control. But the point still applies: For AI to be *safe to
use*, it should be clear where it starts and where it ends; it should be a
reliable tool -- but still, a *tool*. Because if it isn't , why should we, as
humans, still be responsible? *The machine said so*; now what did *I* do?

Put differently, for the sake of society, it is essential that the basic human
feeling of autonomy, of responsibility and control, be not violated -- or better
yet, be *enhanced* in proportion to the use of AI. I'm not talking about "real
autonomy", whatever that is, or actual control; often in life, there may not be
much of it, dependent on where you live, your position in the social hierarchy,
luck -- and more. All I'm talking about is that little bit of autonomy required
to feel responsible, to act responsibly. It's analogous to free will: Most
probably, there is none [^2]; however, we act as though there were, and the
assumption, to some degree, (probably)[^3] is integral to functioning societies.

#### Don't instrumentalize - create a market (M. Jordan)

Finally, to awareness and transparency let's add another connected idea, coming
back to M. Jordan a last time. Coming along with loss of transparency and
control is the classic Silicon Valley advertising-based business model. If
you're like me just months ago, on reading "advertising-based business model"
you might just shrug and think, yeah sure, "[The best minds of my generation are
thinking about how to make people click
ads](https://quoteinvestigator.com/2017/06/12/click/)", yadda yadda yadda ...
but we knew all that. What that really means is less known (at least I didn't
know). It means that for optimal profit, every agent in the network of players
involved in ad serving is interested in matching users and products in the best
possible way. Not just globally, or based on time-invariant features. Instead,
the moment counts: What is the most promising product to suggest to someone
who's on their way back from their usual lunchtime run (known through
geolocation), probably hungry (see calories burnt indicated by sports watch),
and still upset from conflict-heavy discussions before midday (known from audio
recordings, or perhaps pressure applied on keyboard, or heart rate from fitness
tracker). The more data, the better; and what do I know about what *they* know
about me.

The problem is: While this business model is often criticized, you don't often
hear viable alternatives. I'm not an economist, but I find it encouraging that
people at least think about alternative models. One such alternative, again, was
formulated by Jordan[@Jordan2019Dr]: Create a market joining producers and
consumers (e.g., restaurants and people who want to have dinner; musicians and
people who listen to music; all kinds of services and service seekers), where
matches are made based on individual expectations and the givens of concrete
situations. This would involve recommender systems on both sides, and have to
take into account scarcity and real-time, real-space constraints; so not be a
task "too easy" for ML. In contrast to advertising-based models though, while
the broker -- whoever creates the platform and runs the algorithm -- shall get
their share, both parties in that market should be happy as well. Certainly, the
profit for the broker wouldn't be that enormous, but it sounds like a fair model
for society.

In sum: It's not obvious what a single person can do (it for sure depends on the
person), but let's all try our best to make AI the safest we can, staying
vigilant and reflective, and responsible. Thanks for reading!

[^1]: See e.g.[@Blackmore pp.Â 336-338]

[^2]: In my view, there is none; your take on the issue may differ.

[^3]: In traditional, Western societies, probably it is. For a discussion see
    [@Blackmore pp.Â 241-245]
