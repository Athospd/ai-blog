<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Parallelized sampling using exponential variates</title>

  <meta property="description" itemprop="description" content="How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2020-07-29"/>
  <meta property="article:created" itemprop="dateCreated" content="2020-07-29"/>
  <meta name="article:author" content="Yitao Li"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Parallelized sampling using exponential variates"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Parallelized sampling using exponential variates"/>
  <meta property="twitter:description" content="How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that."/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Weighted Random Sampling;citation_publication_date=2016;citation_publisher=Springer New York;citation_doi=10.1007/978-1-4939-2864-4_478;citation_author=Pavlos Efraimidis;citation_author=Paul (Pavlos) Spirakis"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","bibliography","date","categories","output","preview","header-includes"]}},"value":[{"type":"character","attributes":{},"value":["Parallelized sampling using exponential variates"]},{"type":"character","attributes":{},"value":["How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Yitao Li"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["07-29-2020"]},{"type":"character","attributes":{},"value":["Concepts","Distributed Computing"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/dice.jpg"]},{"type":"character","attributes":{},"value":["\\usepackage{algorithm2e}"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/d1.jpg","images/d2.jpg","images/d3.jpg","images/dice.jpg","images/tree.jpg","parallelized-sampling_files/bowser-1.9.3/bowser.min.js","parallelized-sampling_files/distill-2.2.21/template.v2.js","parallelized-sampling_files/header-attrs-2.3/header-attrs.js","parallelized-sampling_files/jquery-1.11.3/jquery.min.js","parallelized-sampling_files/webcomponents-2.0.0/webcomponents.js","proof.pdf","proof.tex","samplingutils.scala","test_plan"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for table of contents */

  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }

  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }

  .d-toc a {
    border-bottom: none;
  }

  .d-toc ul {
    padding-left: 0;
  }

  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }

  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }

  .d-toc li {
    margin-bottom: 0.9em;
  }

  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }

  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }



  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */

  d-code {
    overflow-x: auto !important;
  }

  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  pre.text-output {

    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  @media(min-width: 768px) {

  d-code {
    overflow-x: visible !important;
  }

  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }

  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }



  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }


  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="parallelized-sampling_files/header-attrs-2.3/header-attrs.js"></script>
  <script src="parallelized-sampling_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="parallelized-sampling_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="parallelized-sampling_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="parallelized-sampling_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Parallelized sampling using exponential variates","description":"How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.","authors":[{"author":"Yitao Li","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2020-07-29T00:00:00.000+00:00","citationText":"Li, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Parallelized sampling using exponential variates</h1>
<p><p>How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.</p></p>
</div>

<div class="d-byline">
  Yitao Li  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>07-29-2020
</div>

<div class="d-article">
<p>As part of our recent work to support weighted sampling of Spark data frames in <code>sparklyr</code>, we embarked on a journey searching for algorithms that can perform weighted sampling, especially sampling without replacement, in efficient and scalable ways within a distributed cluster-computing framework, such as Apache Spark.</p>
<p>In the interest of brevity, “weighted sampling without replacement” shall be shortened into <strong>SWoR</strong> for the remainder of this blog post.</p>
<p>In the following sections, we will explain and illustrate what <strong>SWoR</strong> means probability-wise, briefly outline some alternative solutions we have considered but were not completely satisfied with, and then deep-dive into exponential variates, a simple mathematical construct that made the ideal solution for this problem possible.</p>
<p>If you cannot wait to jump into action, there is also a <a href="#examples">section</a> in which we showcase example usages of <code>sdf_weighted_sample()</code> in <code>sparklyr</code>. In addition, you can examine the implementation detail of <code>sparklyr::sdf_weighted_sample()</code> in this <a href="https://github.com/sparklyr/sparklyr/pull/2606">pull request</a>.</p>
<h2 id="how-it-all-started">How it all started</h2>
<p>Our journey started from a <a href="https://github.com/sparklyr/sparklyr/issues/2592">Github issue</a> inquiring about the possibility of supporting the equivalent of <code>dplyr::sample_frac(..., weight = &lt;weight_column&gt;)</code> for Spark data frames in <code>sparklyr</code>. For example,</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dplyr::sample_frac(mtcars, 0.25, weight = gear, replace = FALSE)</code></pre>
</div>
<pre><code>
##                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Merc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Chrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat X1-9         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Maserati Bora     15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Ferrari Dino      19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6</code></pre>
<p>will randomly select one-fourth of all rows from a R data frame named “mtcars” without replacement, using <code>mtcars$gear</code> as weights. We were unable to find any function implementing the weighted versions of <code>dplyr::sample_frac</code> among <a href="https://spark.apache.org/docs/3.0.0/api/sql/index.html">Spark SQL built-in functions</a> in Spark 3.0 or in earlier versions, which means a future version of <code>sparklyr</code> will need to run its own weighted sampling algorithm to support such use cases.</p>
<h2 id="swor">What exactly is <strong>SWoR</strong></h2>
<p>The purpose of this section is to mathematically describe the probability distribution generated by <strong>SWoR</strong> in terms of <span class="math inline">\(w_1, \dotsc, w_N\)</span>, so that readers can clearly see that the exponential-variate based algorithm presented in a subsequent section in fact samples from precisely the same probability distribution. Readers already having a crystal-clear mental picture of what <strong>SWoR</strong> entails should probably skip most of this section. The key take-away here is given <span class="math inline">\(N\)</span> rows <span class="math inline">\(r_1, \dotsc, r_N\)</span> and their weights <span class="math inline">\(w_1, \dotsc, w_N\)</span> and a desired sample size <span class="math inline">\(n\)</span>, the probability of <strong>SWoR</strong> selecting <span class="math inline">\((r_1, \dotsc, r_n)\)</span> is <span class="math inline">\(\prod\limits_{j = 1}^{n} \left( {w_j} \middle/ {\sum\limits_{k = j}^{N}{w_k}} \right)\)</span>.</p>
<p><code>SWOR</code> is conceptually equivalent to a <span class="math inline">\(n\)</span>-step process of selecting 1 out of <span class="math inline">\((n - j + 1)\)</span> remaining rows in the <span class="math inline">\(j\)</span>-th step for <span class="math inline">\(j \in \{1, \dotsc, n\}\)</span>, with each remaining row’s likelihood of getting selected being linearly proportional to its weight in any of the steps, i.e.,</p>
<pre><code>
samples := {}
population := {r[1], ..., r[N]}

for j = 1 to n
  select r[x] from population with probability
    (w[x] / TotalWeight(population))
  samples := samples + {r[x]}
  population := population - {r[x]}</code></pre>
<p>Notice the outcome of a <strong>SWoR</strong> process is in fact order-significant, which is why in this post it will always be represented as an ordered tuple of elements.</p>
<p>Intuitively, <strong>SWoR</strong> is analogous to throwing darts at a bunch of tiles. For example, let’s say the size of our sample space is 5:</p>
<ul>
<li><p>Imagine <span class="math inline">\(r_1, r_2, \dotsc, r_5\)</span> as 5 rectangular tiles laid out contiguously on a wall with widths <span class="math inline">\(w_1, w_2, \dotsc, w_5\)</span>, with <span class="math inline">\(r_1\)</span> covering <span class="math inline">\([0, w_1)\)</span>, <span class="math inline">\(r_2\)</span> covering <span class="math inline">\([w_1, w_1 + w_2)\)</span>, …, and <span class="math inline">\(r_5\)</span> covering <span class="math inline">\(\left[\sum\limits_{j = 1}^{4} w_j, \sum\limits_{j = 1}^{5} w_j\right)\)</span></p></li>
<li><p>Equate drawing a random sample in each step to throwing a dart uniformly randomly within the interval covered by all tiles that are not hit yet</p></li>
<li><p>After a tile is hit, it gets taken out and remaining tiles are re-arranged so that they continue to cover a contiguous interval without overlapping</p></li>
</ul>
<p>If our sample size is 3, then we shall ask ourselves what is the probability of the dart hitting <span class="math inline">\((r_1, r_2, r_3)\)</span> in that order?</p>
<p>In step <span class="math inline">\(j = 1\)</span>, the dart will hit <span class="math inline">\(r_1\)</span> with probability <span class="math inline">\(\left. w_1 \middle/ \left(\sum\limits_{k = 1}^{N}w_k\right) \right.\)</span></p>
<p><img src="images/d1.jpg" style="width:50.0%;height:50.0%" alt="step 1" /> .</p>
<p>After deleting <span class="math inline">\(r_1\)</span> from the sample space after it’s hit, step <span class="math inline">\(j = 2\)</span> will look like this:</p>
<p><img src="images/d2.jpg" style="width:48.0%;height:45.0%" alt="step 2" /> ,</p>
<p>and the probability of the dart hitting <span class="math inline">\(r_2\)</span> in step 2 is <span class="math inline">\(\left. w_2 \middle/ \left(\sum\limits_{k = 2}^{N}w_k\right) \right.\)</span> .</p>
<p>Finally, moving on to step <span class="math inline">\(j = 3\)</span>, we have:</p>
<p><img src="images/d3.jpg" style="width:40.0%;height:30.0%" alt="step 3" /> ,</p>
<p>with the probability of the dart hitting <span class="math inline">\(r_3\)</span> being <span class="math inline">\(\left. w_3 \middle/ \left(\sum\limits_{k = 3}^{N}w_k\right) \right.\)</span>.</p>
<p>So, combining all of the above, the overall probability of selecting <span class="math inline">\((r_1, r_2, r_3)\)</span> is <span class="math inline">\(\prod\limits_{j = 1}^{3} \left( {w_j} \middle/ {\sum\limits_{k = j}^{N}{w_k}} \right)\)</span>.</p>
<h2 id="naive-approaches-for-implementing-swor">Naive approaches for implementing <strong>SWoR</strong></h2>
<p>This section outlines some possible approaches that were briefly under consideration. Because none of these approaches scales well to a large number of rows or a non-trivial number of partitions in a Spark data frame, we decided to avoid all of them in <code>sparklyr</code>.</p>
<h3 id="a-tree-base-approach">A tree-base approach</h3>
<p>One possible way to accomplish <strong>SWoR</strong> is to have a mutable data structure keeping track of the sample space at each step.</p>
<p>Continuing with the dart-throwing analogy from the previous section, let us say initially, none of the tiles has been taken out yet, and a dart has landed at some point <span class="math inline">\(x \in \left[0, \sum\limits_{k = 1}^{N} w_k\right)\)</span>. Which tile did it hit? This can be answered efficiently if we have a binary tree, pictured as the following (or in general, some <span class="math inline">\(b\)</span>-ary tree for integer <span class="math inline">\(b \ge 2\)</span>)</p>
<figure>
<img src="images/tree.jpg" style="width:60.0%;height:40.0%" alt="" /><figcaption>.</figcaption>
</figure>
<p>To find the tile that was hit given the dart’s position <span class="math inline">\(x\)</span>, we simply need to traverse down the tree, going through the box containing <span class="math inline">\(x\)</span> in each level, incurring a <span class="math inline">\(O(\log(N))\)</span> cost in time complexity for each sample. To take a tile out of the picture, we update the width of the tile to <span class="math inline">\(0\)</span> and propagate this change upwards from leaf level to root of the tree, again incurring a <span class="math inline">\(O(\log(N))\)</span> cost in time complexity, making the overall time complexity of selecting <span class="math inline">\(n\)</span> samples <span class="math inline">\(O(n \cdot \log(N))\)</span>, which is not so great for large data sets, and also, not parallelizable across multiple partitions of a Spark data frame.</p>
<h3 id="rejection-sampling">Rejection sampling</h3>
<p>Another possible approach is to use rejection sampling. In term of the previously mentioned dart-throwing analogy, that means not removing any tile that is hit, hence avoiding the performance cost of keeping the sample space up-to-date, but then having to re-throw the dart in each of the subsequent rounds until the dart lands on a tile that was not hit previously. This approach, just like the previous one, would not be performant, and would not be parallelizable across multiple partitions of a Spark data frame either.</p>
<h1 id="exponential-variates-to-the-rescue">Exponential variates to the rescue</h1>
<p>A solution that has proven to be much better than any of the naive approaches turns out to be a numerical stable variant of the algorithm described in “Weighted Random Sampling” <span class="citation" data-cites="Efraimidis2016">(Efraimidis and Spirakis <a href="#ref-Efraimidis2016" role="doc-biblioref">2016</a>)</span> by Pavlos S. Efraimidis and Paul G. Spirakis.</p>
<p>A version of this sampling algorithm implemented by <code>sparklyr</code> does the following to sample <span class="math inline">\(n\)</span> out of <span class="math inline">\(N\)</span> rows from a Spark data frame <span class="math inline">\(X\)</span>:</p>
<ul>
<li>For each row <span class="math inline">\(r_j \in X\)</span>, draw a random number <span class="math inline">\(u_j\)</span> independently and uniformly randomly from <span class="math inline">\((0, 1)\)</span> and compute the key of <span class="math inline">\(r_j\)</span> as <span class="math inline">\(k_j = \ln(u_j) / w_j\)</span>, where <span class="math inline">\(w_j\)</span> is the weight of <span class="math inline">\(r_j\)</span>. Perform this calulation in parallel across all partitions of <span class="math inline">\(X\)</span>.</li>
<li>Select <span class="math inline">\(n\)</span> rows with largest keys and return them as the result. This step is also mostly parallelizable: for each partition of <span class="math inline">\(X\)</span>, one can select up to <span class="math inline">\(n\)</span> rows having largest keys within that partition as candidates, and after selecting candidates from all partitions in parallel, simply extract the top <span class="math inline">\(n\)</span> rows among all candidates, and return them as the <span class="math inline">\(n\)</span> chosen samples.</li>
</ul>
<p>There are at least 4 reasons why this solution is highly appealing and was chosen to be implemented in <code>sparklyr</code>:</p>
<ul>
<li>It is a one-pass algorithm (i.e., only need to iterate through all rows of a data frame exactly once).</li>
<li>Its computational overhead is quite low (as selecting top <span class="math inline">\(n\)</span> rows at any stage only requires a bounded priority queue of max size <span class="math inline">\(n\)</span>, which costs <span class="math inline">\(O(\log(n))\)</span> per update in time complexity).</li>
<li>More importantly, most of its required computations can be performed in parallel. In fact, the only non-parallelizable step is the very last stage of combining top candidates from all partitions and choosing the top <span class="math inline">\(n\)</span> rows among those candidates. So, it fits very well into the world of Spark / MapReduce, and has drastically better horizontal scalability compared to the naive approaches.</li>
<li>Bonus: It is also suitable for weighted reservoir sampling (i.e., can sample <span class="math inline">\(n\)</span> out of a possibly infinite stream of rows according to their weights such that at any moment the <span class="math inline">\(n\)</span> samples will be a weighted representation of all rows that have been processed so far).</li>
</ul>
<h2 id="why-does-this-algorithm-work">Why does this algorithm work</h2>
<p>As an interesting aside, some readers have probably seen this technique presented in a slightly different form under another name. It is in fact equivalent to a generalized version of the <a href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions">Gumbel-max trick</a> which is commonly referred to as the Gumbel-top-k trick. Readers familiar with properties of the Gumbel distribution will no doubt have an easy time convincing themselves the algorithm above works as expected.</p>
<p>In this section, we will also present a proof of correctness for this algorithm based on elementary properties of <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (shortened as PDF from now on), <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function</a> (shortened as CDF from now on), and basic calculus.</p>
<p>First of all, to make sense of all the <span class="math inline">\(\ln(u_j) / w_j\)</span> calculations in this algorithm, one has to understand <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a>. For each <span class="math inline">\(j \in \{1, \dotsc, N\}\)</span>, consider the probability distribution defined on <span class="math inline">\((-\infty, 0)\)</span> with CDF <span class="math inline">\(F_j(x) = e^{w_j \cdot x}\)</span>. In order to pluck out a value <span class="math inline">\(y\)</span> from this distribution, we first sample a value <span class="math inline">\(u_j\)</span> uniformly randomly out of <span class="math inline">\((0, 1)\)</span> that determines the percentile of <span class="math inline">\(y\)</span> (i.e., how our <span class="math inline">\(y\)</span> value ranks relative to all possible <span class="math inline">\(y\)</span> values, a.k.a, the “overall population”, from this distribution), and then apply <span class="math inline">\(F_j^{-1}\)</span> to <span class="math inline">\(u_j\)</span> to find <span class="math inline">\(y\)</span>, so, <span class="math inline">\(y = F_j^{-1}(u_j) = \ln(u_j) / w_j\)</span>.</p>
<p>Secondly, after defining all the required CDF functions <span class="math inline">\(F_j(x) = e^{w_j \cdot x}\)</span> for <span class="math inline">\(j \in \{1, \dotsc, N\}\)</span>, we can also easily derive their corresponding PDF functions <span class="math inline">\(f_j\)</span>: <span class="math display">\[f_j(x) = \frac{d F_j(x)}{dx} = w_j e^{w_j \cdot x}\]</span>.</p>
<p>Finally, with a clear understanding of the family of probability distributions involved, one can prove the probability of this algorithm selecting a given sequence of rows <span class="math inline">\((r_1, \dotsc, r_n)\)</span> is equal to <span class="math inline">\(\prod\limits_{j = 1}^{n} \left( {w_j} \middle/ {\sum\limits_{k = j}^{N}{w_k}} \right)\)</span>, identical to the probability previously mentioned in the <a href="#swor">“What exactly is <strong>SWoR</strong>”</a> section, which implies the possible outcomes of this algorithm will follow exactly the same probability distribution as that of a <span class="math inline">\(n\)</span>-step <strong>SWoR</strong>.</p>
<p>In order to not deprive our dear readers the pleasure of completing this proof by themselves, we have decided to not inline the rest of the proof (which boils down to a calculus exercise) within this blog post, but it is available in <a href="proof.pdf">this file</a>.</p>
<h1 id="weighted-sampling-with-replacement">Weighted sampling with replacement</h1>
<p>While all previous sections focused entirely on weighted sampling without replacement, this section will briefly discuss how the exponential-variate approach can also benefit the weighted-sampling-with-replacement use case (which will be shortened as <code>SWR</code> from now on).</p>
<p>Although <code>SWR</code> with sample size <span class="math inline">\(n\)</span> can be carried out by <span class="math inline">\(n\)</span> independent processes each selecting <span class="math inline">\(1\)</span> sample, parallelizing a <code>SWR</code> workload across all partitions of a Spark data frame (let’s call it <span class="math inline">\(X\)</span>) will still be more performant if the number of partitions is much larger than <span class="math inline">\(n\)</span> and more than <span class="math inline">\(n\)</span> executors are available in a Spark cluster.</p>
<p>An initial solution we had in mind was to run <code>SWR</code> with sample size <span class="math inline">\(n\)</span> in parallel on each partition of <span class="math inline">\(X\)</span>, and then re-sample the results based on relative total weights of each partition. Despite sounding deceptively simple when summarized in words, implementing such a solution in practice would be a moderately complicated task. First, one has to apply the <a href="https://en.wikipedia.org/wiki/Alias_method">alias method</a> or similar in order to perform weighted sampling efficiently on each partition of <span class="math inline">\(X\)</span>, and on top of that, implementing the re-sampling logic across all partitions correctly and verifying the correctness of such procedure will also require considerable effort.</p>
<p>In comparison, with the help of exponential variates, a <code>SWR</code> carried out as <span class="math inline">\(n\)</span> independent <strong>SWoR</strong> processes each selecting <span class="math inline">\(1\)</span> sample is much simpler to implement, while still being comparable to our initial solution in terms of efficiency and scalability. An example implementation of it (which takes fewer than 60 lines of Scala) is presented in <a href="samplingutils.scala">samplingutils.scala</a>.</p>
<h1 id="testing">Testing</h1>
<p>While parallelized sampling based on exponential variates looks fantastic on paper, there are still plenty of potential pitfalls when it comes to translating such idea into code, and as usual, a good testing plan is necessary to ensure implementation correctness.</p>
<p>For instance, numerical instability issues from floating point numbers arise if <span class="math inline">\(\ln(u_j) / w_j\)</span> were replaced by <span class="math inline">\(u_j ^ {1 / w_j}\)</span> in the aforementioned computations.</p>
<p>Another more subtle source of error is the usage of PRNG seeds. For example, consider the following:</p>
<pre><code>
  def sampleWithoutReplacement(
    rdd: RDD[Row],
    weightColumn: String,
    sampleSize: Int,
    seed: Long
  ): RDD[Row] = {
    val sc = rdd.context
    if (0 == sampleSize) {
      sc.emptyRDD
    } else {
      val random = new Random(seed)
      val mapRDDs = rdd.mapPartitions { iter =&gt;
        for (row &lt;- iter) {
          val weight = row.getAs[Double](weightColumn)
          val key = scala.math.log(random.nextDouble) / weight
          &lt;and then make sampling decision for `row` based on its `key`,
           as described in the previous section&gt;
        }
        ...
      }
      ...
    }
  }</code></pre>
<p>Even though it might look OK upon first glance, <code>rdd.mapPartitions(...)</code> from the above will cause the same sequence of pseudorandom numbers to be applied to multiple partitions of the input Spark data frame, which will cause undesired bias (i.e., sampling outcomes from one partition will have non-trivial correlation with those from another partition when such correlation should be negligible in a correct implementation).</p>
<p>The code snippet below is an example implementation in which each partition of the input Spark data frame is sampled using a different sequence of pseudorandom numbers:</p>
<pre><code>
  def sampleWithoutReplacement(
    rdd: RDD[Row],
    weightColumn: String,
    sampleSize: Int,
    seed: Long
  ): RDD[Row] = {
    val sc = rdd.context
    if (0 == sampleSize) {
      sc.emptyRDD
    } else {
      val mapRDDs = rdd.mapPartitionsWithIndex { (index, iter) =&gt;
        val random = new Random(seed + index)

        for (row &lt;- iter) {
          val weight = row.getAs[Double](weightColumn)
          val key = scala.math.log(random.nextDouble) / weight
          &lt;and then make sampling decision for `row` based on its `key`,
           as described in the previous section&gt;
        }

        ...
      }
    ...
  }
}</code></pre>
<p>An example test case in which a two-sided Kolmogorov-Smirnov test is used to compare distribution of sampling outcomes from <code>dplyr::slice_sample()</code> with that from <code>sparklyr::sdf_weighted_sample()</code> is shown in <a href="test_plan">this file</a>. Such tests have proven to be effective in surfacing non-obvious implementation errors such as the ones mentioned above.</p>
<h1 id="examples">Example Usages</h1>
<p>Please note the <code>sparklyr::sdf_weighted_sample()</code> functionality is not included in any official release of <code>sparklyr</code> yet. We are aiming to ship it as part of <code>sparklyr</code> 1.4 in about 2 to 3 months from now.</p>
<p>In the meanwhile, you can try it out with the following steps:</p>
<p>First, make sure <code>devtools</code> is installed, and then run</p>
<pre><code>
devtools::install_github(&quot;sparklyr/sparklyr&quot;, ref = &quot;master&quot;)</code></pre>
<p>to install <code>sparklyr</code> from source.</p>
<p>Next, create a test data frame with numeric weight column consisting of non-negative weight for each row, and then copy it to Spark (see code snippet below as an example):</p>
<pre><code>
library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;)

example_df &lt;- data.frame(
  x = seq(100),
  weight = c(
    rep(1, 50),
    rep(2, 25),
    rep(4, 10),
    rep(8, 10),
    rep(16, 5)
  )
)
example_sdf &lt;- copy_to(sc, example_df, repartition = 5, overwrite = TRUE)</code></pre>
<p>Finally, run <code>sparklyr::sdf_weighted_sample()</code> on <code>example_sdf</code>:</p>
<pre><code>
sample_size &lt;- 20

samples_without_replacement &lt;- example_sdf %&gt;%
  sdf_weighted_sample(
    weight_col = &quot;weight&quot;,
    k = sample_size,
    replacement = FALSE
  )

samples_with_replacement &lt;- example_sdf %&gt;%
  sdf_weighted_sample(
    weight_col = &quot;weight&quot;,
    k = sample_size,
    replacement = TRUE
  )</code></pre>
<h1 id="acknowledgement">Acknowledgement</h1>
<p>First and foremost, the author wishes to thank <a href="https://github.com/ajing">@ajing</a> for reporting the weighted sampling use cases were not properly supported yet in <code>sparklyr</code> 1.3 and suggesting it should be part of some future version of <code>sparklyr</code> in this <a href="https://github.com/sparklyr/sparklyr/issues/2592">Github issue</a>.</p>
<p>Special thanks also goes to Javier (<a href="https://github.com/javierluraschi">@javierluraschi</a>) for reviewing the <a href="https://github.com/sparklyr/sparklyr/pull/2606">implementation</a> of all exponential-variate based sampling algorithms in <code>sparklyr</code>, and to Mara (<a href="https://github.com/batpigandme">@batpigandme</a>), Sigrid (<a href="https://github.com/skeydan">@Sigrid</a>), and Javier (<a href="https://github.com/javierluraschi">@javierluraschi</a>) for their valuable editorial suggestions.</p>
<p>We hope you have enjoyed reading this blog post! If you wish to learn more about <code>sparklyr</code>, we recommend visiting <a href="https://sparklyr.ai">sparklyr.ai</a>, <a href="https://spark.rstudio.com">spark.rstudio.com</a>, and some of the previous release posts such as <a href="https://blog.rstudio.com/2020/07/16/sparklyr-1-3/">sparklyr 1.3</a> and <a href="https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/">sparklyr 1.2</a>. Also, your contributions to <code>sparklyr</code> are more than welcome. Please send your pull requests through <a href="https://github.com/sparklyr/sparklyr/pulls">here</a> and file any bug report or feature request in <a href="https://github.com/sparklyr/sparklyr">here</a>.</p>
<p>Thanks for reading!</p>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Efraimidis2016">
<p>Efraimidis, Pavlos, and Paul (Pavlos) Spirakis. 2016. “Weighted Random Sampling.” In <em>Encyclopedia of Algorithms</em>, edited by Ming-Yang Kao, 2365–7. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-1-4939-2864-4_478">https://doi.org/10.1007/978-1-4939-2864-4_478</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@Inbook{Efraimidis2016,
author="Efraimidis, Pavlos
and Spirakis, Paul (Pavlos)",
editor="Kao, Ming-Yang",
title="Weighted Random Sampling",
bookTitle="Encyclopedia of Algorithms",
year="2016",
publisher="Springer New York",
address="New York, NY",
pages="2365--2367",
isbn="978-1-4939-2864-4",
doi="10.1007/978-1-4939-2864-4_478",
url="https://doi.org/10.1007/978-1-4939-2864-4_478"
}
@inproceedings{DBLP:conf/icml/KoolHW19,
  author    = {Wouter Kool and
               Herke van Hoof and
               Max Welling},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for
               Sampling Sequences Without Replacement},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {3499--3508},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/kool19a.html},
  timestamp = {Fri, 17 Apr 2020 14:12:04 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/KoolHW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
