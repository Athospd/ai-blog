<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Word Embeddings with Keras</title>
  
  <meta property="description" itemprop="description" content="Word embedding is a method used to map words of a vocabulary to dense vectors of real&#10;numbers where semantically similar words are mapped to nearby points. In this example&#10;we&#39;ll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.&#10;"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2017-12-22"/>
  <meta property="article:created" itemprop="dateCreated" content="2017-12-22"/>
  <meta name="article:author" content="Daniel Falbel"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Word Embeddings with Keras"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Word embedding is a method used to map words of a vocabulary to dense vectors of real&#10;numbers where semantically similar words are mapped to nearby points. In this example&#10;we&#39;ll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.&#10;"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Word Embeddings with Keras"/>
  <meta property="twitter:description" content="Word embedding is a method used to map words of a vocabulary to dense vectors of real&#10;numbers where semantically similar words are mapped to nearby points. In this example&#10;we&#39;ll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.&#10;"/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","categories","preview","output"]}},"value":[{"type":"character","attributes":{},"value":["Word Embeddings with Keras"]},{"type":"character","attributes":{},"value":["Word embedding is a method used to map words of a vocabulary to dense vectors of real\nnumbers where semantically similar words are mapped to nearby points. In this example\nwe'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Daniel Falbel"]},{"type":"character","attributes":{},"value":["https://github.com/dfalbel"]},{"type":"character","attributes":{},"value":["Curso-R"]},{"type":"character","attributes":{},"value":["http://curso-r.com/"]}]}]},{"type":"character","attributes":{},"value":["12-22-2017"]},{"type":"character","attributes":{},"value":["Keras","Examples","Text","Embeddings"]},{"type":"character","attributes":{},"value":["word-embeddings-with-keras.png"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["word-embeddings-with-keras_files/bowser-1.9.3/bowser.min.js","word-embeddings-with-keras_files/distill-2.2.21/template.v2.js","word-embeddings-with-keras_files/jquery-1.11.3/jquery.min.js","word-embeddings-with-keras_files/webcomponents-2.0.0/webcomponents.js","word-embeddings-with-keras.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#fn1>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.text();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.text(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        if ($.inArray(language, ["r", "cpp", "c", "java"]) != -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      $(this).children().not('d-code, pre.text-output')
        .wrap($('<div class="' + layout + '"></div>'));
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      $(this).find('img, .html-widget').css('width', '100%');
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="word-embeddings-with-keras_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="word-embeddings-with-keras_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="word-embeddings-with-keras_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="word-embeddings-with-keras_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Word Embeddings with Keras","description":"Word embedding is a method used to map words of a vocabulary to dense vectors of real\nnumbers where semantically similar words are mapped to nearby points. In this example\nwe'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.\n","authors":[{"author":"Daniel Falbel","authorURL":"https://github.com/dfalbel","affiliation":"Curso-R","affiliationURL":"http://curso-r.com/"}],"publishedDate":"2017-12-22T00:00:00.000-05:00","citationText":"Falbel, 2017"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Word Embeddings with Keras</h1>
<p>Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we’ll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.</p>
</div>

<div class="d-byline">
  Daniel Falbel <a href="https://github.com/dfalbel" class="uri">https://github.com/dfalbel</a> (Curso-R)<a href="http://curso-r.com/" class="uri">http://curso-r.com/</a>
  
<br/>12-22-2017
</div>

<div class="d-article">
<h2 id="introduction">Introduction</h2>
<p>Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. Representing words in this vector space help algorithms achieve better performance in natural language processing tasks like syntactic parsing and sentiment analysis by grouping similar words. For example, we expect that in the embedding space “cats” and “dogs” are mapped to nearby points since they are both animals, mammals, pets, etc.</p>
<p>In this tutorial we will implement the skip-gram model created by <a href="https://arxiv.org/abs/1301.3781">Mikolov et al</a> in R using the <a href="https://keras.rstudio.com/">keras</a> package. The skip-gram model is a flavor of word2vec, a class of computationally-efficient predictive models for learning word embeddings from raw text. We won’t address theoretical details about embeddings and the skip-gram model. If you want to get more details you can read the paper linked above. The TensorFlow <a href="https://www.tensorflow.org/tutorials/word2vec">Vector Representation of Words</a> tutorial includes additional details as does the <em>Deep Learning With R</em> <a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html">notebook about embeddings</a>.</p>
<p>There are other ways to create vector representations of words. For example, GloVe Embeddings are implemented in the <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html">text2vec</a> package by Dmitriy Selivanov. There’s also a tidy approach described in Julia Silge’s blog post <a href="https://juliasilge.com/blog/tidy-word-vectors/">Word Vectors with Tidy Data Principles</a>.</p>
<h2 id="getting-the-data">Getting the Data</h2>
<p>We will use the <a href="https://snap.stanford.edu/data/web-FineFoods.html">Amazon Fine Foods Reviews dataset</a>. This dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and narrative text.</p>
<p>Data can be downloaded (~116MB) by running:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
download.file(&quot;https://snap.stanford.edu/data/finefoods.txt.gz&quot;, &quot;finefoods.txt.gz&quot;)</code></pre>
</div>
<p>We will now load the plain text reviews into R.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(readr)
library(stringr)
reviews &lt;- read_lines(&quot;finefoods.txt.gz&quot;) 
reviews &lt;- reviews[str_sub(reviews, 1, 12) == &quot;review/text:&quot;]
reviews &lt;- str_sub(reviews, start = 14)
reviews &lt;- iconv(reviews, to = &quot;UTF-8&quot;)</code></pre>
</div>
<p>Let’s take a look at some reviews we have in the dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
head(reviews, 2)</code></pre>
</div>
<pre><code>
[1] &quot;I have bought several of the Vitality canned dog food products ...
[2] &quot;Product arrived labeled as Jumbo Salted Peanuts...the peanuts ... </code></pre>
<h2 id="preprocessing">Preprocessing</h2>
<p>We’ll begin with some text pre-processing using a keras <code>text_tokenizer()</code>. The tokenizer will be responsible for transforming each review into a sequence of integer tokens (which will subsequently be used as input into the skip-gram model).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
tokenizer &lt;- text_tokenizer(num_words = 20000)
tokenizer %&gt;% fit_text_tokenizer(reviews)</code></pre>
</div>
<p>Note that the <code>tokenizer</code> object is modified in place by the call to <code>fit_text_tokenizer()</code>. An integer token will be assigned for each of the 20,000 most common words (the other words will be assigned to token 0).</p>
<h2 id="skip-gram-model">Skip-Gram Model</h2>
<p>In the skip-gram model we will use each word as input to a log-linear classifier with a projection layer, then predict words within a certain range before and after this word. It would be very computationally expensive to output a probability distribution over all the vocabulary for each target word we input into the model. Instead, we are going to use negative sampling, meaning we will sample some words that don’t appear in the context and train a binary classifier to predict if the context word we passed is truly from the context or not.</p>
<p>In more practical terms, for the skip-gram model we will input a 1d integer vector of the target word tokens and a 1d integer vector of sampled context word tokens. We will generate a prediction of 1 if the sampled word really appeared in the context and 0 if it didn’t.</p>
<p>We will now define a generator function to yield batches for model training.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(reticulate)
library(purrr)
skipgrams_generator &lt;- function(text, tokenizer, window_size, negative_samples) {
  gen &lt;- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip &lt;- generator_next(gen) %&gt;%
      skipgrams(
        vocabulary_size = tokenizer$num_words, 
        window_size = window_size, 
        negative_samples = 1
      )
    x &lt;- transpose(skip$couples) %&gt;% map(. %&gt;% unlist %&gt;% as.matrix(ncol = 1))
    y &lt;- skip$labels %&gt;% as.matrix(ncol = 1)
    list(x, y)
  }
}</code></pre>
</div>
<p>A <a href="https://keras.rstudio.com/articles/faq.html#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory">generator function</a> is a function that returns a different value each time it is called (generator functions are often used to provide streaming or dynamic data for training models). Our generator function will receive a vector of texts, a tokenizer and the arguments for the skip-gram (the size of the window around each target word we examine and how many negative samples we want to sample for each target word).</p>
<p>Now let’s start defining the keras model. We will use the Keras <a href="https://keras.rstudio.com/articles/functional_api.html">functional API</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
embedding_size &lt;- 128  # Dimension of the embedding vector.
skip_window &lt;- 5       # How many words to consider left and right.
num_sampled &lt;- 1       # Number of negative examples to sample for each word.</code></pre>
</div>
<p>We will first write placeholders for the inputs using the <code>layer_input</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
input_target &lt;- layer_input(shape = 1)
input_context &lt;- layer_input(shape = 1)</code></pre>
</div>
<p>Now let’s define the embedding matrix. The embedding is a matrix with dimensions (vocabulary, embedding_size) that acts as lookup table for the word vectors.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
embedding &lt;- layer_embedding(
  input_dim = tokenizer$num_words + 1, 
  output_dim = embedding_size, 
  input_length = 1, 
  name = &quot;embedding&quot;
)

target_vector &lt;- input_target %&gt;% 
  embedding() %&gt;% 
  layer_flatten()

context_vector &lt;- input_context %&gt;%
  embedding() %&gt;%
  layer_flatten()</code></pre>
</div>
<p>The next step is to define how the <code>target_vector</code> will be related to the <code>context_vector</code> in order to make our network output 1 when the context word really appeared in the context and 0 otherwise. We want <code>target_vector</code> to be <em>similar</em> to the <code>context_vector</code> if they appeared in the same context. A typical measure of similarity is the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>. Give two vectors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> the cosine similarity is defined by the Euclidean Dot product of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> normalized by their magnitude. As we don’t need the similarity to be normalized inside the network, we will only calculate the dot product and then output a dense layer with sigmoid activation.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
dot_product &lt;- layer_dot(list(target_vector, context_vector), axes = 1)
output &lt;- layer_dense(dot_product, units = 1, activation = &quot;sigmoid&quot;)</code></pre>
</div>
<p>Now we will create the model and compile it.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model(list(input_target, input_context), output)
model %&gt;% compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;)</code></pre>
</div>
<p>We can see the full definition of the model by calling <code>summary</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
summary(model)</code></pre>
</div>
<pre><code>
_________________________________________________________________________________________
Layer (type)                 Output Shape       Param #    Connected to                  
=========================================================================================
input_1 (InputLayer)         (None, 1)          0                                        
_________________________________________________________________________________________
input_2 (InputLayer)         (None, 1)          0                                        
_________________________________________________________________________________________
embedding (Embedding)        (None, 1, 128)     2560128    input_1[0][0]                 
                                                           input_2[0][0]                 
_________________________________________________________________________________________
flatten_1 (Flatten)          (None, 128)        0          embedding[0][0]               
_________________________________________________________________________________________
flatten_2 (Flatten)          (None, 128)        0          embedding[1][0]               
_________________________________________________________________________________________
dot_1 (Dot)                  (None, 1)          0          flatten_1[0][0]               
                                                           flatten_2[0][0]               
_________________________________________________________________________________________
dense_1 (Dense)              (None, 1)          2          dot_1[0][0]                   
=========================================================================================
Total params: 2,560,130
Trainable params: 2,560,130
Non-trainable params: 0
_________________________________________________________________________________________</code></pre>
<h2 id="model-training">Model Training</h2>
<p>We will fit the model using the <code>fit_generator()</code> function We need to specify the number of training steps as well as number of epochs we want to train. We will train for 100,000 steps for 5 epochs. This is quite slow (~1000 seconds per epoch on a modern GPU). Note that you may also get reasonable results with just one epoch of training.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model %&gt;%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples), 
    steps_per_epoch = 100000, epochs = 5
    )</code></pre>
</div>
<pre><code>
Epoch 1/1
100000/100000 [==============================] - 1092s - loss: 0.3749      
Epoch 2/5
100000/100000 [==============================] - 1094s - loss: 0.3548     
Epoch 3/5
100000/100000 [==============================] - 1053s - loss: 0.3630     
Epoch 4/5
100000/100000 [==============================] - 1020s - loss: 0.3737     
Epoch 5/5
100000/100000 [==============================] - 1017s - loss: 0.3823 </code></pre>
<p>We can now extract the embeddings matrix from the model by using the <code>get_weights()</code> function. We also added <code>row.names</code> to our embedding matrix so we can easily find where each word is.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(dplyr)

embedding_matrix &lt;- get_weights(model)[[1]]

words &lt;- data_frame(
  word = names(tokenizer$word_index), 
  id = as.integer(unlist(tokenizer$word_index))
)

words &lt;- words %&gt;%
  filter(id &lt;= tokenizer$num_words) %&gt;%
  arrange(id)

row.names(embedding_matrix) &lt;- c(&quot;UNK&quot;, words$word)</code></pre>
</div>
<h2 id="understanding-the-embeddings">Understanding the Embeddings</h2>
<p>We can now find words that are close to each other in the embedding. We will use the cosine similarity, since this is what we trained the model to minimize.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(text2vec)

find_similar_words &lt;- function(word, embedding_matrix, n = 5) {
  similarities &lt;- embedding_matrix[word, , drop = FALSE] %&gt;%
    sim2(embedding_matrix, y = ., method = &quot;cosine&quot;)
  
  similarities[,1] %&gt;% sort(decreasing = TRUE) %&gt;% head(n)
}</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
find_similar_words(&quot;2&quot;, embedding_matrix)</code></pre>
</div>
<pre><code>
        2         4         3       two         6 
1.0000000 0.9830254 0.9777042 0.9765668 0.9722549 </code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
find_similar_words(&quot;little&quot;, embedding_matrix)</code></pre>
</div>
<pre><code>
   little       bit       few     small     treat 
1.0000000 0.9501037 0.9478287 0.9309829 0.9286966 </code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
find_similar_words(&quot;delicious&quot;, embedding_matrix)</code></pre>
</div>
<pre><code>
delicious     tasty wonderful   amazing     yummy 
1.0000000 0.9632145 0.9619508 0.9617954 0.9529505 </code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
find_similar_words(&quot;cats&quot;, embedding_matrix)</code></pre>
</div>
<pre><code>
     cats      dogs      kids       cat       dog 
1.0000000 0.9844937 0.9743756 0.9676026 0.9624494 </code></pre>
<p>The <strong>t-SNE</strong> algorithm can be used to visualize the embeddings. Because of time constraints we will only use it with the first 500 words. To understand more about the <em>t-SNE</em> method see the article <a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively</a>.</p>
<p>This plot may look like a mess, but if you zoom into the small groups you end up seeing some nice patterns. Try, for example, to find a group of web related words like <code>http</code>, <code>href</code>, etc. Another group that may be easy to pick out is the pronouns group: <code>she</code>, <code>he</code>, <code>her</code>, etc.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(Rtsne)
library(ggplot2)
library(plotly)

tsne &lt;- Rtsne(embedding_matrix[2:500,], perplexity = 50, pca = FALSE)

tsne_plot &lt;- tsne$Y %&gt;%
  as.data.frame() %&gt;%
  mutate(word = row.names(embedding_matrix)[2:500]) %&gt;%
  ggplot(aes(x = V1, y = V2, label = word)) + 
  geom_text(size = 3)
tsne_plot</code></pre>
</div>
<iframe src="https://rstudio-pubs-static.s3.amazonaws.com/343548_04348b5de4124a3eb05f62506c6c5827.html" width="100%" height="750" style="border: none;">
</iframe>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
