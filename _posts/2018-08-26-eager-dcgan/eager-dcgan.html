<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Generating digits with Keras and TensorFlow eager execution</title>
  
  <meta property="description" itemprop="description" content="Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2018-08-26"/>
  <meta property="article:created" itemprop="dateCreated" content="2018-08-26"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Generating digits with Keras and TensorFlow eager execution"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Generating digits with Keras and TensorFlow eager execution"/>
  <meta property="twitter:description" content="Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Unsupervised representation learning with deep convolutional generative adversarial networks;citation_publication_date=2015;citation_volume=abs/1511.06434;citation_author=Alec Radford;citation_author=Luke Metz;citation_author=Soumith Chintala"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Generating digits with Keras and TensorFlow eager execution"]},{"type":"character","attributes":{},"value":["Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["http://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["08-26-2018"]},{"type":"character","attributes":{},"value":["Eager","Keras","TensorFlow","GAN"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","eager-dcgan_files/bowser-1.9.3/bowser.min.js","eager-dcgan_files/distill-2.2.21/template.v2.js","eager-dcgan_files/jquery-1.11.3/jquery.min.js","eager-dcgan_files/webcomponents-2.0.0/webcomponents.js","images/images_epoch_150.png","images/thumb.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.text();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.text(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        if ($.inArray(language, ["r", "cpp", "c", "java"]) != -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      $(this).find('img, .html-widget').css('width', '100%');
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.radix-site-header a').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="eager-dcgan_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="eager-dcgan_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="eager-dcgan_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="eager-dcgan_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Generating digits with Keras and TensorFlow eager execution","description":"Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"http://www.rstudio.com/"}],"publishedDate":"2018-08-26T00:00:00.000+02:00","citationText":"Keydana, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Generating digits with Keras and TensorFlow eager execution</h1>
<p>Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities - images, often. We show how to code them using Keras and TensorFlow eager execution.</p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="http://www.rstudio.com/" class="uri">http://www.rstudio.com/</a>
  
<br/>08-26-2018
</div>

<div class="d-article">
<p>The recent announcement of TensorFlow 2.0, much disseminated on social media, names <em>eager execution</em> as the number one central feature of the new major version. What does this mean for R users? As demonstrated in our recent post on neural machine translation, you can use eager execution from R now already, in combination with Keras custom models and the datasets API. It’s good to know you <em>can</em> use it - but why should you? And in which cases?</p>
<p>In this and a few upcoming posts, we want to show how eager execution can make developing models a lot easier. The degree of simplication will depend on the task - and just <em>how much</em> easier you’ll find the new way might also depend on your experience using the functional API to model more complex relationships. Even if you think that GANs, encoder-decoder architectures, or neural style transfer didn’t pose any problems before the advent of eager execution, you might find that the alternative is a better fit to how we humans mentally picture a situation.</p>
<p>For this post, we are porting code from a recent <a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb">Google Colaboratory notebook</a> implementing the DCGAN architecture.<span class="citation" data-cites="RadfordMC15">(Radford, Metz, and Chintala <a href="#ref-RadfordMC15">2015</a>)</span> No prior knowledge of GANs is required - we’ll keep this post practical (no maths) and focus on how to achieve your goal, mapping a simple and vivid concept into an astonishingly small number of lines of code.</p>
<p>As in the post on machine translation with attention, we first have to cover some prerequisites. By the way, no need to copy out the code snippets - you’ll find the complete code <a href="https://github.com/rstudio/keras/tree/master/vignettes/examples/eager_dcgan.R">here</a>).</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>The code in this post depends on the development versions of several of the TensorFlow R packages. You can install these packages as follows:</p>
<pre class="r"><code>
devtools::install_github(c(
  &quot;rstudio/reticulate&quot;,
  &quot;rstudio/tensorflow&quot;,
  &quot;rstudio/keras&quot;,
  &quot;rstudio/tfdatasets&quot;
))</code></pre>
<p>You should also be sure that you are running the very latest version of TensorFlow (v1.10), which you can install like so:</p>
<pre class="r"><code>
library(tensorflow)
install_tensorflow()</code></pre>
<p>There are additional requirements for using TensorFlow eager execution. First, we need to call <code>tfe_enable_eager_execution()</code> right at the beginning of the program. Second, we need to use the implementation of Keras included in TensorFlow, rather than the base Keras implementation.</p>
<p>We’ll also use the <a href="https://tensorflow.rstudio.com/tools/tfdatasets/articles/introduction.html">tfdatasets</a> package for our input pipeline. So we end up with the following libraries needed for this example:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(keras)
use_implementation(&quot;tensorflow&quot;)

library(tensorflow)
tfe_enable_eager_execution(device_policy = &quot;silent&quot;)

library(tfdatasets)</code></pre>
</div>
<p>That’s it. Let’s get started.</p>
<h2 id="so-whats-a-gan">So what’s a GAN?</h2>
<p>GAN stands for <em>Generative Adversarial Network</em>. It is a setup of two agents, the <em>generator</em> and the <em>discriminator</em>, that act against each other (thus, <em>adversarial</em>). It is <em>generative</em> because the goal is to generate output (as opposed to, say, classification or regression).</p>
<p>In human learning, feedback - direct or indirect - plays a central role. Say we wanted to forge a banknote (as long as those still exist). Assuming we can get away with unsuccessful trials, we would learn from every rejection and distrustful inspection. Optimizing our technique, we would end up rich. This concept of optimizing from feedback is embodied in the first of the two agents, the <em>generator</em>. It gets its feedback from the <em>discriminator</em>, in an upside-down way: If it can fool the discriminator, making it believe that the banknote was real, all is fine; if the discriminator notices the fake, it has to do things differently. For a neural network, that means it has to update its weights.</p>
<p>How does the discriminator know what is real and what is fake? It too has to be trained, on real banknotes (or whatever the kind of objects involved) and the fake ones produced by the generator. So the complete setup is two agents competing, one striving to generate realistic-looking fake objects, and the other, to disavow the deception. The purpose of training is to have both evolve and get better, in turn causing the other to get better, too.</p>
<p>In this system, there is no objective minimum to the loss function: We want both components to learn and getter better “in lockstep”, instead of one winning out over the other. This makes optimization difficult. In practice therefore, tuning a GAN can seem more like alchemy than like science, and it often makes sense to lean on practices and “tricks” reported by others.</p>
<p>In this example, just like in the Google notebook we’re porting, the goal is to generate MNIST digits. While that may not sound like the most exciting task one could imagine, it lets us focus on the mechanics, and allows us to keep computation and memory requirements (comparatively) low.</p>
<p>Let’s load the data (training set needed only) and then, look at the first actor in our drama, the generator.</p>
<h2 id="training-data">Training data</h2>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mnist &lt;- dataset_mnist()
c(train_images, train_labels) %&lt;-% mnist$train

train_images &lt;- train_images %&gt;% 
  k_expand_dims() %&gt;%
  k_cast(dtype = &quot;float32&quot;)

train_images &lt;- (train_images - 127.5) / 127.5</code></pre>
</div>
<p>Our complete training set will be streamed once per epoch:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
buffer_size &lt;- 60000
batch_size &lt;- 256L
batches_per_epoch &lt;- (buffer_size / batch_size) %&gt;% round()

train_dataset &lt;- tensor_slices_dataset(train_images) %&gt;%
  dataset_shuffle(buffer_size) %&gt;%
  dataset_batch(batch_size)</code></pre>
</div>
<p>This input will be fed to the discriminator only.</p>
<h2 id="generator">Generator</h2>
<p>Both generator and discriminator are <a href="https://keras.rstudio.com/articles/custom_models.html">custom models</a>. As we will soon see, the generator gets passed vectors of random noise for input. This vector is transformed to 3d (height, width, channels) and then, successively upsampled to the required output size of (28,28,3).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
generator &lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$fc1 &lt;- layer_dense(units = 7 * 7 * 64, use_bias = FALSE)
      self$batchnorm1 &lt;- layer_batch_normalization()
      self$leaky_relu1 &lt;- layer_activation_leaky_relu()
      self$conv1 &lt;-
        layer_conv_2d_transpose(
          filters = 64,
          kernel_size = c(5, 5),
          strides = c(1, 1),
          padding = &quot;same&quot;,
          use_bias = FALSE
        )
      self$batchnorm2 &lt;- layer_batch_normalization()
      self$leaky_relu2 &lt;- layer_activation_leaky_relu()
      self$conv2 &lt;-
        layer_conv_2d_transpose(
          filters = 32,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &quot;same&quot;,
          use_bias = FALSE
        )
      self$batchnorm3 &lt;- layer_batch_normalization()
      self$leaky_relu3 &lt;- layer_activation_leaky_relu()
      self$conv3 &lt;-
        layer_conv_2d_transpose(
          filters = 1,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &quot;same&quot;,
          use_bias = FALSE,
          activation = &quot;tanh&quot;
        )
      
      function(inputs,
               mask = NULL,
               training = TRUE) {
        self$fc1(inputs) %&gt;%
          self$batchnorm1(training = training) %&gt;%
          self$leaky_relu1() %&gt;%
          k_reshape(shape = c(-1, 7, 7, 64)) %&gt;%
          self$conv1() %&gt;%
          self$batchnorm2(training = training) %&gt;%
          self$leaky_relu2() %&gt;%
          self$conv2() %&gt;%
          self$batchnorm3(training = training) %&gt;%
          self$leaky_relu3() %&gt;%
          self$conv3()
      }
    })
  }</code></pre>
</div>
<h2 id="discriminator">Discriminator</h2>
<p>The discriminator is just a pretty normal convolutional network outputting a score. Here, usage of “score” instead of “probability” is on purpose: If you look at the last layer, it is fully connected, of size 1 but lacking the usual sigmoid activation. This is because unlike Keras’ <code>loss_binary_crossentropy</code>, the loss function we’ll be using here - <code>tf$losses$sigmoid_cross_entropy</code> - works with the raw logits, not the outputs of the sigmoid.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
discriminator &lt;-
  function(name = NULL) {
    keras_model_custom(name = name, function(self) {
      
      self$conv1 &lt;- layer_conv_2d(
        filters = 64,
        kernel_size = c(5, 5),
        strides = c(2, 2),
        padding = &quot;same&quot;
      )
      self$leaky_relu1 &lt;- layer_activation_leaky_relu()
      self$dropout &lt;- layer_dropout(rate = 0.3)
      self$conv2 &lt;-
        layer_conv_2d(
          filters = 128,
          kernel_size = c(5, 5),
          strides = c(2, 2),
          padding = &quot;same&quot;
        )
      self$leaky_relu2 &lt;- layer_activation_leaky_relu()
      self$flatten &lt;- layer_flatten()
      self$fc1 &lt;- layer_dense(units = 1)
      
      function(inputs,
               mask = NULL,
               training = TRUE) {
        inputs %&gt;% self$conv1() %&gt;%
          self$leaky_relu1() %&gt;%
          self$dropout(training = training) %&gt;%
          self$conv2() %&gt;%
          self$leaky_relu2() %&gt;%
          self$flatten() %&gt;%
          self$fc1()
      }
    })
  }</code></pre>
</div>
<h2 id="setting-the-scene">Setting the scene</h2>
<p>Before we can start training, we need to create the usual components of a deep learning setup: the model (or models, in this case), the loss function(s), and the optimizer(s).</p>
<p>Model creation is just a function call, with a little extra on top:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
generator &lt;- generator()
discriminator &lt;- discriminator()

# https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun
generator$call = tf$contrib$eager$defun(generator$call)
discriminator$call = tf$contrib$eager$defun(discriminator$call)</code></pre>
</div>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun"><em>defun</em></a> compiles a Python function (once per different combination of argument shapes and non-tensor objects values)) into a TensorFlow graph, and is used to speed up computations. This comes with side effects and possibly unexpected behavior - please consult the documentation for the details. Here, we were mainly curious in how much of a speedup we might notice when using this from R - in our example, it resulted in a speedup of 130%.</p>
<p>On to the losses. Discriminator loss consists of two parts: Does it correctly identify real images as real, and does it correctly spot fake images as fake. Here <code>real_output</code> and <code>generated_output</code> contain the logits returned from the discriminator - that is, its judgment of whether the respective images are fake or real.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
discriminator_loss &lt;- function(real_output, generated_output) {
  real_loss &lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = k_ones_like(real_output),
    logits = real_output)
  generated_loss &lt;- tf$losses$sigmoid_cross_entropy(
    multi_class_labels = k_zeros_like(generated_output),
    logits = generated_output)
  real_loss + generated_loss
}</code></pre>
</div>
<p>Generator loss depends on how the discriminator judged its creations: It would hope for them all to be seen as real.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
generator_loss &lt;- function(generated_output) {
  tf$losses$sigmoid_cross_entropy(
    tf$ones_like(generated_output),
    generated_output)
}</code></pre>
</div>
<p>Now we still need to define optimizers, one for each model.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
discriminator_optimizer &lt;- tf$train$AdamOptimizer(1e-4)
generator_optimizer &lt;- tf$train$AdamOptimizer(1e-4)</code></pre>
</div>
<h2 id="training-loop">Training loop</h2>
<p>There are two models, two loss functions and two optimizers, but there is just one training loop, as both models depend on each other. The training loop will be over MNIST images streamed in batches, but we still need input to the generator - a random vector of size 100, in this case.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
noise_dim &lt;- 100L</code></pre>
</div>
<p>Let’s take the training loop step by step. There will be an outer and an inner loop, one over epochs and one over batches. At the start of each epoch, we create a fresh iterator over the dataset:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
 for (epoch in seq_len(num_epochs)) {
    start &lt;- Sys.time()
    total_loss_gen &lt;- 0
    total_loss_disc &lt;- 0
    iter &lt;- make_iterator_one_shot(train_dataset)</code></pre>
</div>
<p>Now for every batch we obtain from the iterator, we are calling the generator and having it generate images from random noise. Then, we’re calling the dicriminator on real images as well as the fake images just generated. For the discriminator, its relative outputs are directly fed into the loss function. For the generator, its loss will depend on how the discriminator judged its creations:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
 until_out_of_range({
      batch &lt;- iterator_get_next(iter)
      noise &lt;- k_random_normal(c(batch_size, noise_dim))
      with(tf$GradientTape() %as% gen_tape, {
      with(tf$GradientTape() %as% disc_tape, {
        generated_images &lt;- generator(noise)
        disc_real_output &lt;- discriminator(batch, training = TRUE)
        disc_generated_output &lt;-
          discriminator(generated_images, training = TRUE)
        gen_loss &lt;- generator_loss(disc_generated_output)
        disc_loss &lt;-
          discriminator_loss(disc_real_output, disc_generated_output)
      })
      })</code></pre>
</div>
<p>Note that all model calls happen inside <code>tf$GradientTape</code> contexts. This is so the forward passes can be recorded and “played back” to back propagate the losses through the network.</p>
<p>Obtain the gradients of the losses to the respective models’ variables (<code>tape$gradient</code>) and have the optimizers apply them to the models’ weights (<code>optimizer$apply_gradients</code>):</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
      gradients_of_generator &lt;-
        gen_tape$gradient(gen_loss, generator$variables)
      gradients_of_discriminator &lt;-
        disc_tape$gradient(disc_loss, discriminator$variables)
      
      generator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_generator, generator$variables)
      ))
      discriminator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_discriminator, discriminator$variables)
      ))
      
      total_loss_gen &lt;- total_loss_gen + gen_loss
      total_loss_disc &lt;- total_loss_disc + disc_loss
})</code></pre>
</div>
<p>This ends the loop over batches. Finish off the loop over epochs displaying current losses and saving a few of the generator’s artwork:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
    cat(&quot;Time for epoch &quot;, epoch, &quot;: &quot;, Sys.time() - start, &quot;\n&quot;)
    cat(&quot;Generator loss: &quot;, total_loss_gen$numpy() / batches_per_epoch, &quot;\n&quot;)
    cat(&quot;Discriminator loss: &quot;, total_loss_disc$numpy() / batches_per_epoch, &quot;\n\n&quot;)
    if (epoch %% 10 == 0)
      generate_and_save_images(generator,
                               epoch,
                               random_vector_for_generation)
    
}</code></pre>
</div>
<p>Here’s the training loop again, shown as a whole - even including the lines for reporting on progress, it is remarkably concise, and allows for a quick grasp of what is going on:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
train &lt;- function(dataset, epochs, noise_dim) {
  for (epoch in seq_len(num_epochs)) {
    start &lt;- Sys.time()
    total_loss_gen &lt;- 0
    total_loss_disc &lt;- 0
    iter &lt;- make_iterator_one_shot(train_dataset)
    
    until_out_of_range({
      batch &lt;- iterator_get_next(iter)
      noise &lt;- k_random_normal(c(batch_size, noise_dim))
      with(tf$GradientTape() %as% gen_tape, {
      with(tf$GradientTape() %as% disc_tape, {
        generated_images &lt;- generator(noise)
        disc_real_output &lt;- discriminator(batch, training = TRUE)
        disc_generated_output &lt;-
          discriminator(generated_images, training = TRUE)
        gen_loss &lt;- generator_loss(disc_generated_output)
        disc_loss &lt;-
          discriminator_loss(disc_real_output, disc_generated_output)
      })
      })
      
      gradients_of_generator &lt;-
        gen_tape$gradient(gen_loss, generator$variables)
      gradients_of_discriminator &lt;-
        disc_tape$gradient(disc_loss, discriminator$variables)
      
      generator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_generator, generator$variables)
      ))
      discriminator_optimizer$apply_gradients(purrr::transpose(
        list(gradients_of_discriminator, discriminator$variables)
      ))
      
      total_loss_gen &lt;- total_loss_gen + gen_loss
      total_loss_disc &lt;- total_loss_disc + disc_loss
      
    })
    
    cat(&quot;Time for epoch &quot;, epoch, &quot;: &quot;, Sys.time() - start, &quot;\n&quot;)
    cat(&quot;Generator loss: &quot;, total_loss_gen$numpy() / batches_per_epoch, &quot;\n&quot;)
    cat(&quot;Discriminator loss: &quot;, total_loss_disc$numpy() / batches_per_epoch, &quot;\n\n&quot;)
    if (epoch %% 10 == 0)
      generate_and_save_images(generator,
                               epoch,
                               random_vector_for_generation)
    
  }
}</code></pre>
</div>
<p>Here’s the function for saving generated images…</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
generate_and_save_images &lt;- function(model, epoch, test_input) {
  predictions &lt;- model(test_input, training = FALSE)
  png(paste0(&quot;images_epoch_&quot;, epoch, &quot;.png&quot;))
  par(mfcol = c(5, 5))
  par(mar = c(0.5, 0.5, 0.5, 0.5),
      xaxs = &#39;i&#39;,
      yaxs = &#39;i&#39;)
  for (i in 1:25) {
    img &lt;- predictions[i, , , 1]
    img &lt;- t(apply(img, 2, rev))
    image(
      1:28,
      1:28,
      img * 127.5 + 127.5,
      col = gray((0:255) / 255),
      xaxt = &#39;n&#39;,
      yaxt = &#39;n&#39;
    )
  }
  dev.off()
}</code></pre>
</div>
<p>… and we’re ready to go!</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
num_epochs &lt;- 150
train(train_dataset, num_epochs, noise_dim)</code></pre>
</div>
<h2 id="results">Results</h2>
<p>Here are some generated images after training for 150 epochs:</p>
<p><img src="images/images_epoch_150.png" /></p>
<p>As they say, your results will most certainly vary!</p>
<h2 id="conclusion">Conclusion</h2>
<p>While certainly tuning GANs will remain a challenge, we hope we were able to show that mapping concepts to code is not difficult when using eager execution. In case you’ve played around with GANs before, you may have found you needed to pay careful attention to set up the losses the right way, freeze the discriminator’s weights when needed, etc. This need goes away with eager execution. In upcoming posts, we will show further examples where using it makes model development easier.</p>
<div id="refs" class="references">
<div id="ref-RadfordMC15">
<p>Radford, Alec, Luke Metz, and Soumith Chintala. 2015. “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.” <em>CoRR</em> abs/1511.06434. <a href="http://arxiv.org/abs/1511.06434" class="uri">http://arxiv.org/abs/1511.06434</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{RadfordMC15,
  author    = {Alec Radford and
               Luke Metz and
               Soumith Chintala},
  title     = {Unsupervised Representation Learning with Deep Convolutional Generative
               Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1511.06434},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06434},
  archivePrefix = {arXiv},
  eprint    = {1511.06434},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RadfordMC15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
