<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>tbd</title>
  
  <meta property="description" itemprop="description" content="tbd"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-05-23"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-05-23"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="tbd"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="tbd"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="tbd"/>
  <meta property="twitter:description" content="tbd"/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["tbd"]},{"type":"character","attributes":{},"value":["tbd  \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2019cafes"]},{"type":"character","attributes":{},"value":["05-23-2019"]},{"type":"character","attributes":{},"value":["Probability and statistics","Introductions"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/thumb.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["images/lkj.png","images/shrinkage1.png","images/shrinkage2.png","images/trace.png","varying_slopes_files/bowser-1.9.3/bowser.min.js","varying_slopes_files/distill-2.2.21/template.v2.js","varying_slopes_files/jquery-1.11.3/jquery.min.js","varying_slopes_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="varying_slopes_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="varying_slopes_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="varying_slopes_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="varying_slopes_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"tbd","description":"tbd","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2019-05-23T00:00:00.000+02:00","citationText":"Keydana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>tbd</h1>
<p>tbd<br /></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>05-23-2019
</div>

<div class="d-article">
<p>In a previous post, we showed how to use <a href="https://rstudio.github.io/tfprobability/">tfprobability</a> – the R interface to TensorFlow Probability – to build a <em>multilevel</em>, or <em>partial pooling</em>, model of tadpole survival in differently sized (and thus, differing in inhabitant number) tanks.</p>
<p>A completely <em>pooled</em> model would have resulted in a global estimate of survival count, irrespective of tank; an <em>unpooled</em> model would have learned to predict survival count for each tank separately. The former approach does not take into account different circumstances; the latter does not make use of common information. (Also, it clearly has no predictive use unless we want to make predictions for the very same entities we used to train the model.)</p>
<p>In contrast, a <em>partially pooled</em> model lets you make predictions for the familiar, as well as new entities: Just use the appropriate prior.</p>
<p>Assuming we <em>are</em> in fact interested in the same entities – why would we want to apply partial pooling? For the same reasons so much effort in machine learning goes into devising regularization mechanisms. We don’t want to overfit too much to actual measurements, be they related to the same entity or a class of entities. If I want to predict my heart rate as I wake up next morning, based on a single measurement I’m taking now (let’s say it’s evening and I’m frantically typing a blog post), I better take into account some facts about heart rate behavior in general (instead of just projecting into the future the exact value measured right now).</p>
<p>In the tadpole example, this means we expect generalization to work better for tanks with many inhabitants, compared to more solitary environments. For the latter ones, we better take a peek at survival rates from other tanks, to supplement the sparse idiosyncratic information available. Or using the technical term, in the latter case we hope for the model to <em>shrink</em> its estimates toward the overall mean more noticeably than in the former.</p>
<p>This type of information sharing is already very useful, but it gets better. The tadpole model is a <em>varying intercepts</em> model, as McElreath calls it (or <em>random intercepts</em>, as it is sometimes – confusingly – called <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>) – <em>intercepts</em> referring to the way we make predictions for entities (here: tanks), with no predictor variables present. So if we can pool information about intercepts, why not pool information about <em>slopes</em> as well? This will allow us to, in addition, make use of <em>relationships</em> between variables learnt on different entities in the training set.</p>
<p>So as you might have guessed by now, <em>varying slopes</em> (or <em>random slopes</em>, if you will) is the topiof today’s post. Again, we take up an example from McElreath’s book, and show how to accomplish the same thing with <code>tfprobability</code>.</p>
<h2 id="coffee-please">Coffee, please</h2>
<p>Unlike the tadpole case, this time we work with simulated data. This is the data McElreath uses to introduce the <em>varying slopes</em> modeling technique; he then goes on and applies it to one of the book’s most featured datasets, the <em>pro-social</em> (or indifferent, rather!) chimpanzees. For today, we stay with the simulated data for two reasons: First, the subject matter per se is non-trivial enough; and second, we want to keep careful track of what our model does, and whether its output is sufficiently close to the results McElreath obtained from <em>Stan</em> <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>So, the scenario is this. <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Cafés vary in how popular they are. In a popular café, when you order coffee, you’re likely to <em>wait</em>. In a less popular café, you’ll likely be served much faster. That’s one thing. Second, all cafés tend to be more crowded in the mornings than in the afternoons. Thus in the morning, you’ll wait longer than in the afternoon – this goes for the popular as well as the less popular cafés.</p>
<p>In terms of intercepts and slopes, we can picture the morning waits as intercepts, and the resultant afternoon waits as arising due to the slopes of the lines joining each morning and afternoon wait, respectively.</p>
<p>So when we partially-pool <em>intercepts</em>, we have one “intercept prior” (itself constrained by a prior, of course), and a set of café-specific intercepts that will vary around it. When we partially-pool <em>slopes</em>, we have a “slope prior” reflecting the overall relationship between morning and afternoon waits, and a set of café-specific slopes reflecting the individual relationships. Cognitively, that means that if you have never been to the <em>Café Gerbeaud</em> in Budapest but have been to cafés before, you might have a less-than-uninformed idea about how long you are going to wait; it also means that if you normally get your coffee in your favorite corner café in the mornings, and now you pass by there in the afternoon, you have an approximate idea how long it’s going to take (namely, fewer minutes than in the mornings).</p>
<p>So is that all? Actually, no. In our scenario, intercepts and slopes are related. If, at a less popular café, I always get my coffee before two minutes have passed, there is little room for improvement. At a highly popular café though, if it could easily take ten minutes in the mornings, then there is quite some potential for decrease in waiting time in the afternoon. So in my prediction for this afternoon’s waiting time, I should factor in this interaction effect.</p>
<p>So, now that we have an idea of what this is all about, let’s see how we can model these effects with <code>tfprobability</code>. But first, we actually have to generate the data.</p>
<h2 id="simulate-the-data">Simulate the data</h2>
<p>We directly follow McElreath in the way the data are generated.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
##### Inputs needed to generate the covariance matrix between intercepts and slopes #####

# average morning wait time
a &lt;- 3.5
# average difference afternoon wait time
# we wait less in the afternoons
b &lt;- -1
# standard deviation in the (café-specific) intercepts
sigma_a &lt;- 1
# standard deviation in the (café-specific) slopes
sigma_b &lt;- 0.5
# correlation between intercepts and slopes
# the higher the intercept, the more the wait goes down
rho &lt;- -0.7


##### Generate the covariance matrix #####

# means of intercepts and slopes
mu &lt;- c(a, b)
# standard deviations of means and slopes
sigmas &lt;- c(sigma_a, sigma_b) 
# correlation matrix
# a correlation matrix has ones on the diagonal and the correlation in the off-diagonals
rho &lt;- matrix(c(1, rho, rho, 1), nrow = 2) 
# now matrix multiply to get covariance matrix
cov_matrix &lt;- diag(sigmas) %*% rho %*% diag(sigmas)


##### Generate the café-specific intercepts and slopes #####

# 20 cafés overall
n_cafes &lt;- 20

library(MASS)
set.seed(5) # used to replicate example
# multivariate distribution of intercepts and slopes
vary_effects &lt;- mvrnorm(n_cafes , mu ,cov_matrix)
# intercepts are in the first column
a_cafe &lt;- vary_effects[ ,1]
# slopes are in the second
b_cafe &lt;- vary_effects[ ,2]


##### Generate the actual wait times #####

set.seed(22)
# 10 visits per café
n_visits &lt;- 10

# alternate values for mornings and afternoons in the data frame
afternoon &lt;- rep(0:1, n_visits * n_cafes/2)
# data for each café are consecutive rows in the data frame
cafe_id &lt;- rep(1:n_cafes, each = n_visits)

# the regression equation for the mean waiting time
mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon
# standard deviation of waiting time within cafés
sigma &lt;- 0.5 # std dev within cafes
# generate instances of waiting times
wait &lt;- rnorm(n_visits * n_cafes, mu, sigma)

d &lt;- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)</code></pre>
</div>
<p>Take a glimpse at the data:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
d %&gt;% glimpse()</code></pre>
</div>
<pre><code>
Observations: 200
Variables: 3
$ cafe      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,...
$ afternoon &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,...
$ wait      &lt;dbl&gt; 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.54365,...</code></pre>
<p>On to building the model.</p>
<h2 id="the-model">The model</h2>
<p>As in the <a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/">previous post on multi-level modeling</a>, we use <a href="https://rstudio.github.io/tfprobability/reference/tfd_joint_distribution_sequential.html">tfd_joint_distribution_sequential</a> to define the model and <a href="https://rstudio.github.io/tfprobability/reference/mcmc_hamiltonian_monte_carlo.html">Hamiltonian Monte Carlo</a> for sampling. Consider taking a look at the first section of that post for a quick reminder of the overall procedure.</p>
<p>Before we code the model, let’s quickly get library loading out of the way. Importantly, again just like in the previous post, we need to install a <code>master</code> build of TensorFlow Probability, as we’re making use of very new features not yet available in the current release version. The same goes for the R packages <code>tensorflow</code> and <code>tfprobability</code>: Please install the respective development versions from github.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
devtools::install_github(&quot;rstudio/tensorflow&quot;)
devtools::install_github(&quot;rstudio/tfprobability&quot;)

# this will install the latest nightlies of TensorFlow as well as TensorFlow Probability
tensorflow::install_tensorflow(version = &quot;nightly&quot;)

library(tensorflow)
tf$compat$v1$enable_v2_behavior()

library(tfprobability)

library(tidyverse)
library(zeallot)
library(abind)
library(gridExtra)
library(HDInterval)
library(ellipse)</code></pre>
</div>
<p>Now here is the model definition. We’ll go through it step by step in an instant.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- function(cafe_id) {
  tfd_joint_distribution_sequential(
      list(
        # rho, the prior for the correlation matrix between intercepts and slopes
        tfd_lkj(2, 2, input_output_cholesky = TRUE), 
        # sigma, prior variance for the waiting time
        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1),
        # sigma_cafe, prior of variances for intercepts and slopes (vector of 2)
        tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2), 
        # b, the prior mean for the slopes
        tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1),
        # a, the prior mean for the intercepts
        tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1), 
        # mvn, multivariate distribution of intercepts and slopes
        # shape: batch size, 20, 2
        function(a,b,sigma_cafe,sigma,chol_rho) 
          tfd_sample_distribution(
            tfd_multivariate_normal_tri_l(
              loc = tf$concat(list(a,b), axis = -1L),
              scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),
            sample_shape = n_cafes),
        # waiting time
        # shape should be batch size, 200
        function(mvn, a, b, sigma_cafe, sigma)
          tfd_independent(
            # need to pull out the correct cafe_id in the middle column
            tfd_normal(
              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +
                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), 
              scale=sigma),  # Shape [batch,  1]
        reinterpreted_batch_ndims=1
        )
    )
  )
}</code></pre>
</div>
<p>The first five distributions are priors. First,</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# rho, the prior correlation matrix between intercepts and slopes
tfd_lkj(2, 2, input_output_cholesky = TRUE)</code></pre>
</div>
<p>we have the prior for the correlation matrix: an <a href="">LKJ distribution</a> of shape <code>2x2</code> and with <em>concentration</em> parameter equal to 2. What kind of prior is this? As McElreath keeps reminding us, nothing more instructive than sampling from the prior:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
corr_prior &lt;- tfd_lkj(2, 2)
correlation &lt;- (corr_prior %&gt;% tfd_sample(100))[ , 1, 2] %&gt;% as.numeric()
library(ggplot2)
data.frame(correlation) %&gt;% ggplot(aes(x = correlation)) + geom_density()</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/lkj.png" width="297" /></p>
</div>
<p>So this prior is moderately skeptical about strong correlations, but pretty open to learning from data.</p>
<p>One short note about the <code>input_output_cholesky</code> parameter in the definition of the LKJ. This is a pure performance optimization, allowing for more efficient construction of the multivariate normal that makes use of the LKJ’s output a few lines later.</p>
<p>The next distribution in line</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# sigma, prior variance for the waiting time
tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 1)</code></pre>
</div>
<p>is the prior for the variance of the waiting time, the very last distribution in the list.</p>
<p>Next is the prior distribution of variances for the intercepts and slopes. This prior is the same for both cases, but we specify a <code>sample_shape</code> of 2 to get two individual samples.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# sigma_cafe, prior of variances for intercepts and slopes (vector of 2)
tfd_sample_distribution(tfd_exponential(rate = 1), sample_shape = 2)</code></pre>
</div>
<p>Now that we have the respective prior variances, we move on to the prior means. Both are normal distributions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# b, the prior mean for the slopes
tfd_sample_distribution(tfd_normal(loc = -1, scale = 0.5), sample_shape = 1)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# a, the prior mean for the intercepts
tfd_sample_distribution(tfd_normal(loc = 5, scale = 2), sample_shape = 1)</code></pre>
</div>
<p>On to the heart of the model, where the partial pooling happens. We are going to construct partially pooled intercepts and slopes for all of the cafés. Like we said above, intercepts and slopes are not independent; they interact. Thus, we need to use a multivariate normal distribution. The means are given by the prior means defined right above, while the covariance matrix is built from the above prior variances and the prior correlation matrix. The output shape here is determined by the number of cafés: We want an intercept and a slope for every café.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# mvn, multivariate distribution of intercepts and slopes
# shape: batch size, 20, 2
function(a,b,sigma_cafe,sigma,chol_rho) 
  tfd_sample_distribution(
    tfd_multivariate_normal_tri_l(
      loc = tf$concat(list(a,b), axis = -1L),
      scale_tril = tf$linalg$LinearOperatorDiag(sigma_cafe)$matmul(chol_rho)),
  sample_shape = n_cafes)</code></pre>
</div>
<p>Finally, we sample the actual waiting times. This code pulls out the correct intercepts and slopes from the multivariate normal and outputs the mean waiting time, dependent on what café we’re in and whether it’s morning or afternoon.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
        # waiting time
        # shape: batch size, 200
        function(mvn, a, b, sigma_cafe, sigma)
          tfd_independent(
            # need to pull out the correct cafe_id in the middle column
            tfd_normal(
              loc = (tf$gather(mvn[ , , 1], cafe_id, axis = -1L) +
                       tf$gather(mvn[ , , 2], cafe_id, axis = -1L) * afternoon), 
              scale=sigma), 
        reinterpreted_batch_ndims=1
        )</code></pre>
</div>
<p>Before running the sampling, it’s always a good idea to do a quick check on the model.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
n_cafes &lt;- 20
cafe_id &lt;- tf$cast((d$cafe - 1) %% 20, tf$int64)

afternoon &lt;- d$afternoon
wait &lt;- d$wait</code></pre>
</div>
<p>We sample from the model and then, check the log probability.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
m &lt;- model(cafe_id)

s &lt;- m %&gt;% tfd_sample(3)
m %&gt;% tfd_log_prob(s)</code></pre>
</div>
<p>We want a scalar log probability per member in the batch, which is what we get.</p>
<pre><code>
tf.Tensor([-466.1392  -149.92587 -196.51688], shape=(3,), dtype=float32)</code></pre>
<h2 id="running-the-chains">Running the chains</h2>
<p>The actual Monte Carlo sampling works just like in the previous post, with one exception. Sampling happens in unconstrained parameter space, but at the end we need to get valid correlation matrix parameters <code>rho</code> and valid variances <code>sigma</code> and <code>sigma_cafe</code>. Conversion between spaces is done via TFP bijectors. Luckily, this is not something we have to do as users; all we need to specify are appropriate bijectors. For the normal distributions in the model, there is nothing to do.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
constraining_bijectors &lt;- list(
  # make sure the rho[1:4] parameters are valid cholesky factors
  tfb_ransform_row_tri_l(row_bijector = tfb_half_sphere()),
  # make sure variance is positive
  tfb_exp(),
  # make sure variance is positive
  tfb_exp(),
  tfb_identity(),
  tfb_identity(),
  tfb_identity()
)</code></pre>
</div>
<p>Now we can set up the Hamiltonian Monte Carlo sampler.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
n_steps &lt;- 500
n_burnin &lt;- 500
n_chains &lt;- 4

# set up the optimization objective
logprob &lt;- function(rho, sigma, sigma_cafe, b, a, mvn)
  m %&gt;% tfd_log_prob(list(rho, sigma, sigma_cafe, b, a, mvn, wait))

# initial states for the sampling procedure
c(initial_rho, initial_sigma, initial_sigma_cafe, initial_b, initial_a, initial_mvn, .) %&lt;-% 
  (m %&gt;% tfd_sample(n_chains))

# HMC sampler, with the above bijectors and step size adaptation
hmc &lt;- mcmc_hamiltonian_monte_carlo(
  target_log_prob_fn = logprob,
  num_leapfrog_steps = 3,
  step_size = list(0.1, 0.1, 0.1, 0.1, 0.1, 0.1)
) %&gt;%
  mcmc_transformed_transition_kernel(bijector = constraining_bijectors) %&gt;%
  mcmc_simple_step_size_adaptation(target_accept_prob = 0.8,
                                   num_adaptation_steps = n_burnin)</code></pre>
</div>
<p>Again, we can obtain additional diagnostics (here: step sizes and acceptance rates) by registering a trace function:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
trace_fn &lt;- function(state, pkr) {
  list(pkr$inner_results$inner_results$is_accepted,
       pkr$inner_results$inner_results$accepted_results$step_size)
}</code></pre>
</div>
<p>Here, then, is the sampling function. Note how we use <code>tf_function</code> to put it on the graph. At least as of today, this makes a huge difference in sampling performance when using eager execution.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
run_mcmc &lt;- function(kernel) {
  kernel %&gt;% mcmc_sample_chain(
    num_results = n_steps,
    num_burnin_steps = n_burnin,
    current_state = list(initial_rho,
                         tf$ones_like(initial_sigma),
                         tf$ones_like(initial_sigma_cafe),
                         initial_b,
                         initial_a,
                         initial_mvn),
    trace_fn = trace_fn
  )
}

run_mcmc &lt;- tf_function(run_mcmc)
res &lt;- hmc %&gt;% run_mcmc()

mcmc_trace &lt;- res$all_states</code></pre>
</div>
<p>So how do our samples look, and what do we get in terms of posteriors? Let’s see.</p>
<h2 id="results">Results</h2>
<p>At this moment, <code>mcmc_trace</code> is a list of tensors of different shapes, dependent on how we defined the parameters. We need to do a bit of post-processing to be able to summarise and display the results.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# the actual mcmc samples
# for the trace plots, we want to have them in shape (500, 4, 49)
# that is: (number of steps, number of chains, number of parameters)
samples &lt;- abind(
  # rho 1:4
  as.array(mcmc_trace[[1]] %&gt;% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 4L))),
  # sigma
  as.array(mcmc_trace[[2]]),  
  # sigma_cafe 1:2
  as.array(mcmc_trace[[3]][ , , 1]),    
  as.array(mcmc_trace[[3]][ , , 2]), 
  # b
  as.array(mcmc_trace[[4]]),  
  # a
  as.array(mcmc_trace[[5]]),  
  # mvn 10:49
  as.array( mcmc_trace[[6]] %&gt;% tf$reshape(list(tf$cast(n_steps, tf$int32), tf$cast(n_chains, tf$int32), 40L))),
  along = 3) 

# the effective sample sizes
# we want them in shape (4, 49), which is (number of chains * number of parameters)
ess &lt;- mcmc_effective_sample_size(mcmc_trace) 
ess &lt;- cbind(
  # rho 1:4
  as.matrix(ess[[1]] %&gt;% tf$reshape(list(tf$cast(n_chains, tf$int32), 4L))),
  # sigma
  as.matrix(ess[[2]]),  
  # sigma_cafe 1:2
  as.matrix(ess[[3]][ , 1, drop = FALSE]),    
  as.matrix(ess[[3]][ , 2, drop = FALSE]), 
  # b
  as.matrix(ess[[4]]),  
  # a
  as.matrix(ess[[5]]),  
  # mvn 10:49
  as.matrix(ess[[6]] %&gt;% tf$reshape(list(tf$cast(n_chains, tf$int32), 40L)))
  ) 

# the rhat values
# we want them in shape (49), which is (number of parameters)
rhat &lt;- mcmc_potential_scale_reduction(mcmc_trace)
rhat &lt;- c(
  # rho 1:4
  as.double(rhat[[1]] %&gt;% tf$reshape(list(4L))),
  # sigma
  as.double(rhat[[2]]),  
  # sigma_cafe 1:2
  as.double(rhat[[3]][1]),    
  as.double(rhat[[3]][2]), 
  # b
  as.double(rhat[[4]]),  
  # a
  as.double(rhat[[5]]),  
  # mvn 10:49
  as.double(rhat[[6]] %&gt;% tf$reshape(list(40L)))
  ) </code></pre>
</div>
<h3 id="trace-plots">Trace plots</h3>
<p>How well do the chains mix?</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
prep_tibble &lt;- function(samples) {
  as_tibble(samples, .name_repair = ~ c(&quot;chain_1&quot;, &quot;chain_2&quot;, &quot;chain_3&quot;, &quot;chain_4&quot;)) %&gt;% 
    add_column(sample = 1:n_steps) %&gt;%
    gather(key = &quot;chain&quot;, value = &quot;value&quot;, -sample)
}

plot_trace &lt;- function(samples) {
  prep_tibble(samples) %&gt;% 
    ggplot(aes(x = sample, y = value, color = chain)) +
    geom_line() + 
    theme_light() +
    theme(legend.position = &quot;none&quot;,
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank())
}

plot_traces &lt;- function(sample_array, num_params) {
  plots &lt;- purrr::map(1:num_params, ~ plot_trace(sample_array[ , , .x]))
  do.call(grid.arrange, plots)
}

plot_traces(samples, 49)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/trace.png" width="600" /></p>
</div>
<p>Awesome! (The first two parameters of <code>rho</code>, the cholesky factor of the correlation matrix, need to stay fixed at 1 and 0, respectively.)</p>
<p>Now to some summary statistics on the posteriors of the parameters.</p>
<h3 id="parameters">Parameters</h3>
<p>Like last time, we display posterior means and standard deviations, as well as the highest posterior density interval (HPDI). We add effective sample sizes and <em>rhat</em> values.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
column_names &lt;- c(
  paste0(&quot;rho_&quot;, 1:4),
  &quot;sigma&quot;,
  paste0(&quot;sigma_cafe_&quot;, 1:2),
  &quot;b&quot;,
  &quot;a&quot;,
  c(rbind(paste0(&quot;a_cafe_&quot;, 1:20), paste0(&quot;b_cafe_&quot;, 1:20)))
)

all_samples &lt;- matrix(samples, nrow = n_steps * n_chains, ncol = 49)
all_samples &lt;- all_samples %&gt;%
  as_tibble(.name_repair = ~ column_names)

all_samples %&gt;% glimpse()

means &lt;- all_samples %&gt;% 
  summarise_all(list (~ mean)) %&gt;% 
  gather(key = &quot;key&quot;, value = &quot;mean&quot;)

sds &lt;- all_samples %&gt;% 
  summarise_all(list (~ sd)) %&gt;% 
  gather(key = &quot;key&quot;, value = &quot;sd&quot;)

hpdis &lt;-
  all_samples %&gt;%
  summarise_all(list(~ list(hdi(.) %&gt;% t() %&gt;% as_tibble()))) %&gt;% 
   unnest() 
 
 hpdis_lower &lt;- hpdis %&gt;% select(-contains(&quot;upper&quot;)) %&gt;%
   rename(lower0 = lower) %&gt;%
   gather(key = &quot;key&quot;, value = &quot;lower&quot;) %&gt;% 
   arrange(as.integer(str_sub(key, 6))) %&gt;%
   mutate(key = column_names)
 
 hpdis_upper &lt;- hpdis %&gt;% select(-contains(&quot;lower&quot;)) %&gt;%
   rename(upper0 = upper) %&gt;%
   gather(key = &quot;key&quot;, value = &quot;upper&quot;) %&gt;% 
   arrange(as.integer(str_sub(key, 6))) %&gt;%
   mutate(key = column_names)

summary &lt;- means %&gt;% 
  inner_join(sds, by = &quot;key&quot;) %&gt;% 
  inner_join(hpdis_lower, by = &quot;key&quot;) %&gt;%
  inner_join(hpdis_upper, by = &quot;key&quot;)

ess &lt;- apply(ess, 2, mean)

summary_with_diag &lt;- summary %&gt;% add_column(ess = ess, rhat = rhat)
print(summary_with_diag, n = 49)</code></pre>
</div>
<pre><code>
# A tibble: 49 x 7
   key            mean     sd  lower   upper   ess   rhat
   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 rho_1         1     0       1      1        NaN    NaN   
 2 rho_2         0     0       0      0       NaN     NaN   
 3 rho_3        -0.517 0.176  -0.831 -0.195   42.4   1.01
 4 rho_4         0.832 0.103   0.644  1.000   46.5   1.02
 5 sigma         0.473 0.0264  0.420  0.523  424.    1.00
 6 sigma_cafe_1  0.967 0.163   0.694  1.29    97.9   1.00
 7 sigma_cafe_2  0.607 0.129   0.386  0.861   42.3   1.03
 8 b            -1.14  0.141  -1.43  -0.864   95.1   1.00
 9 a             3.66  0.218   3.22   4.07    75.3   1.01
10 a_cafe_1      4.20  0.192   3.83   4.57    83.9   1.01
11 b_cafe_1     -1.13  0.251  -1.63  -0.664   63.6   1.02
12 a_cafe_2      2.17  0.195   1.79   2.54    59.3   1.01
13 b_cafe_2     -0.923 0.260  -1.42  -0.388   46.0   1.01
14 a_cafe_3      4.40  0.195   4.02   4.79    56.7   1.01
15 b_cafe_3     -1.97  0.258  -2.52  -1.51    43.9   1.01
16 a_cafe_4      3.22  0.199   2.80   3.57    58.7   1.02
17 b_cafe_4     -1.20  0.254  -1.70  -0.713   36.3   1.01
18 a_cafe_5      1.86  0.197   1.45   2.20    52.8   1.03
19 b_cafe_5     -0.113 0.263  -0.615  0.390   34.6   1.04
20 a_cafe_6      4.26  0.210   3.87   4.67    43.4   1.02
21 b_cafe_6     -1.30  0.277  -1.80  -0.713   41.4   1.05
22 a_cafe_7      3.61  0.198   3.23   3.98    44.9   1.01
23 b_cafe_7     -1.02  0.263  -1.51  -0.489   37.7   1.03
24 a_cafe_8      3.95  0.189   3.59   4.31    73.1   1.01
25 b_cafe_8     -1.64  0.248  -2.10  -1.13    60.7   1.02
26 a_cafe_9      3.98  0.212   3.57   4.37    76.3   1.03
27 b_cafe_9     -1.29  0.273  -1.83  -0.776   57.8   1.05
28 a_cafe_10     3.60  0.187   3.24   3.96   104.    1.01
29 b_cafe_10    -1.00  0.245  -1.47  -0.512   70.4   1.00
30 a_cafe_11     1.95  0.200   1.56   2.35    55.9   1.03
31 b_cafe_11    -0.449 0.266  -1.00   0.0619  42.5   1.04
32 a_cafe_12     3.84  0.195   3.46   4.22    76.0   1.02
33 b_cafe_12    -1.17  0.259  -1.65  -0.670   62.5   1.03
34 a_cafe_13     3.88  0.201   3.50   4.29    62.2   1.02
35 b_cafe_13    -1.81  0.270  -2.30  -1.29    48.3   1.03
36 a_cafe_14     3.19  0.212   2.82   3.61    65.9   1.07
37 b_cafe_14    -0.961 0.278  -1.49  -0.401   49.9   1.06
38 a_cafe_15     4.46  0.212   4.08   4.91    62.0   1.09
39 b_cafe_15    -2.20  0.290  -2.72  -1.59    47.8   1.11
40 a_cafe_16     3.41  0.193   3.02   3.78    62.7   1.02
41 b_cafe_16    -1.07  0.253  -1.54  -0.567   48.5   1.05
42 a_cafe_17     4.22  0.201   3.82   4.60    58.7   1.01
43 b_cafe_17    -1.24  0.273  -1.74  -0.703   43.8   1.01
44 a_cafe_18     5.77  0.210   5.34   6.18    66.0   1.02
45 b_cafe_18    -1.05  0.284  -1.61  -0.511   49.8   1.02
46 a_cafe_19     3.23  0.203   2.88   3.65    52.7   1.02
47 b_cafe_19    -0.232 0.276  -0.808  0.243   45.2   1.01
48 a_cafe_20     3.74  0.212   3.35   4.21    48.2   1.04
49 b_cafe_20    -1.09  0.281  -1.58  -0.506   36.5   1.05</code></pre>
<p>So what do we have? If you run this “live”, for the rows <code>a_cafe_n</code> resp. <code>b_cafe_n</code>, you see a nice alternation of white and red coloring: For all cafés, the inferred slopes are negative. The inferred slope prior (<code>b</code>) is around -1.14, which is not too far off from the value we used for sampling: 1.</p>
<p>The <code>rho</code> posterior estimates, admittedly, are less useful unless you are accustomed to compose cholesky factors in your head. We compute the resulting posterior correlations and their mean:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
rhos &lt;- all_samples[ , 1:4] %&gt;% tibble()

rhos &lt;- rhos %&gt;%
  apply(1, list) %&gt;%
  unlist(recursive = FALSE) %&gt;%
  lapply(function(x) matrix(x, byrow = TRUE, nrow = 2) %&gt;% tcrossprod())

rho &lt;- rhos %&gt;% purrr::map(~ .x[1,2]) %&gt;% unlist()

mean_rho &lt;- mean(rho)
mean_rho</code></pre>
</div>
<pre><code>
-0.5166775</code></pre>
<p>The value we used for sampling was -0.7, so we see the regularization effect. In case you’re wondering, for the same data <em>Stan</em> yields an estimate of -0.5.</p>
<p>Finally, let’s display equivalents to McElreath’s figures illustrating shrinkage on the parameter (café-specific intercepts and slopes) as well as the outcome (morning resp. afternoon waiting times) scales.</p>
<h2 id="shrinkage">Shrinkage</h2>
<p>As expected, we see that the individual intercepts and slopes are pulled towards the mean – the more, the further away they are from the center.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# just like McElreath, compute unpooled estimates directly from data
a_empirical &lt;- d %&gt;% 
  filter(afternoon == 0) %&gt;%
  group_by(cafe) %&gt;% 
  summarise(a = mean(wait)) %&gt;%
  select(a)

b_empirical &lt;- d %&gt;% 
  filter(afternoon == 1) %&gt;%
  group_by(cafe) %&gt;% 
  summarise(b = mean(wait)) %&gt;%
  select(b) -
  a_empirical

empirical_estimates &lt;- bind_cols(
  a_empirical,
  b_empirical,
  type = rep(&quot;data&quot;, 20))

posterior_estimates &lt;- tibble(
  a = means %&gt;% filter(
  str_detect(key, &quot;^a_cafe&quot;)) %&gt;% select(mean) %&gt;% pull(),
  b = means %&gt;% filter(
    str_detect(key, &quot;^b_cafe&quot;)) %&gt;% select(mean)  %&gt;% pull(),
  type = rep(&quot;posterior&quot;, 20))
  
all_estimates &lt;- bind_rows(empirical_estimates, posterior_estimates)

# compute posterior mean bivariate Gaussian
# again following McElreath
mu_est &lt;- c(means[means$key == &quot;a&quot;, 2], means[means$key == &quot;b&quot;, 2]) %&gt;% unlist()
rho_est &lt;- mean_rho
sa_est &lt;- means[means$key == &quot;sigma_cafe_1&quot;, 2] %&gt;% unlist()
sb_est &lt;- means[means$key == &quot;sigma_cafe_2&quot;, 2] %&gt;% unlist()
cov_ab &lt;- sa_est * sb_est * rho_est
sigma_est &lt;- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol=2) 

alpha_levels &lt;- c(0.1, 0.3, 0.5, 0.8, 0.99)
names(alpha_levels) &lt;- alpha_levels

contour_data &lt;- plyr::ldply(
  alpha_levels,
  ellipse,
  x = sigma_est,
  scale = c(1, 1),
  centre = mu_est
)

ggplot() +
  geom_point(data = all_estimates, mapping = aes(x = a, y = b, color = type)) + 
  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/shrinkage1.png" width="600" /></p>
</div>
<p>The same behavior is visible on the outcome scale.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
wait_times  &lt;- all_estimates %&gt;%
  mutate(morning = a, afternoon = a + b)

# simulate from posterior means
v &lt;- MASS::mvrnorm(1e4 , mu_est , sigma_est)
v[ ,2] &lt;- v[ ,1] + v[ ,2] # calculate afternoon wait
# construct empirical covariance matrix
sigma_est2 &lt;- cov(v)
mu_est2 &lt;- mu_est
mu_est2[2] &lt;- mu_est[1] + mu_est[2]

contour_data &lt;- plyr::ldply(
  alpha_levels,
  ellipse,
  x = sigma_est2 %&gt;% unname(),
  scale = c(1, 1),
  centre = mu_est2
)

ggplot() +
  geom_point(data = wait_times, mapping = aes(x = morning, y = afternoon, color = type)) + 
  geom_path(data = contour_data, mapping = aes(x = x, y = y, group = .id))</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/shrinkage2.png" width="600" /></p>
</div>
<h2 id="wrapping-up">Wrapping up</h2>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>cf. <a href="https://en.wikipedia.org/wiki/Multilevel_model">the Wikipedia article on multilevel models</a> for a collection of terms encountered when dealing with this subject, and e.g. <a href="https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/">Gelman’s</a> dissection of various ways <em>random effects</em> are defined<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>We won’t overload this post by explicitly comparing results here, but we did that when writing the code.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Disclaimer: We have not verified whether this is an adequate model of the world, but it really doesn’t matter either.<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
