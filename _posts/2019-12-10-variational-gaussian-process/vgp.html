<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Gaussian Process Regression with tfprobability</title>
  
  <meta property="description" itemprop="description" content="Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather &quot;normal&quot; Keras network, defined and trained in pretty much the usual way, with TFP&#39;s Variational Gaussian Process layer pulling off all the magic."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2019-12-10"/>
  <meta property="article:created" itemprop="dateCreated" content="2019-12-10"/>
  <meta name="article:author" content="Sigrid Keydana"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Gaussian Process Regression with tfprobability"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather &quot;normal&quot; Keras network, defined and trained in pretty much the usual way, with TFP&#39;s Variational Gaussian Process layer pulling off all the magic."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Gaussian Process Regression with tfprobability"/>
  <meta property="twitter:description" content="Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather &quot;normal&quot; Keras network, defined and trained in pretty much the usual way, with TFP&#39;s Variational Gaussian Process layer pulling off all the magic."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Gaussian processes for machine learning (adaptive computation and machine learning);citation_publication_date=2005;citation_publisher=The MIT Press;citation_author=Carl Edward Rasmussen;citation_author=Christopher K. I. Williams"/>
  <meta name="citation_reference" content="citation_title=Statistical modeling: The two cultures (with comments and a rejoinder by the author);citation_publication_date=2001;citation_publisher=The Institute of Mathematical Statistics;citation_volume=16;citation_doi=10.1214/ss/1009213726;citation_author=Leo Breiman"/>
  <meta name="citation_reference" content="citation_title=Information theory, inference &amp; learning algorithms;citation_publication_date=2002;citation_publisher=Cambridge University Press;citation_author=David J. C. MacKay"/>
  <meta name="citation_reference" content="citation_title=Variational learning of inducing variables in sparse gaussian processes;citation_publication_date=2009;citation_publisher=PMLR;citation_volume=5;citation_author=Michalis Titsias"/>
  <meta name="citation_reference" content="citation_title=Gaussian processes for big data;citation_publication_date=2013;citation_volume=abs/1309.6835;citation_author=James Hensman;citation_author=Nicolo Fusi;citation_author=Neil D. Lawrence"/>
  <meta name="citation_reference" content="citation_title=Bayesian learning for neural networks;citation_publication_date=1996;citation_publisher=Springer-Verlag;citation_author=Radford M. Neal"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","slug","bibliography","date","categories","output","preview"]}},"value":[{"type":"character","attributes":{},"value":["Gaussian Process Regression with tfprobability"]},{"type":"character","attributes":{},"value":["Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather \"normal\" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic. \n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Sigrid Keydana"]},{"type":"character","attributes":{},"value":["RStudio"]},{"type":"character","attributes":{},"value":["https://www.rstudio.com/"]}]}]},{"type":"character","attributes":{},"value":["keydana2019vgp"]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["12-10-2019"]},{"type":"character","attributes":{},"value":["Probabilistic ML/DL","TensorFlow/Keras"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["images/kernel_cookbook.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","images/history.png","images/kernel_cookbook.png","images/pairs.png","images/preds.png","images/preds2.png","images/visreg.png","vgp_files/bowser-1.9.3/bowser.min.js","vgp_files/distill-2.2.21/template.v2.js","vgp_files/jquery-1.11.3/jquery.min.js","vgp_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="vgp_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="vgp_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="vgp_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="vgp_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Gaussian Process Regression with tfprobability","description":"Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather \"normal\" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.","authors":[{"author":"Sigrid Keydana","authorURL":"#","affiliation":"RStudio","affiliationURL":"https://www.rstudio.com/"}],"publishedDate":"2019-12-10T00:00:00.000+01:00","citationText":"Keydana, 2019"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Gaussian Process Regression with tfprobability</h1>
<p><p>Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather “normal” Keras network, defined and trained in pretty much the usual way, with TFP’s Variational Gaussian Process layer pulling off all the magic.</p></p>
</div>

<div class="d-byline">
  Sigrid Keydana  (RStudio)<a href="https://www.rstudio.com/" class="uri">https://www.rstudio.com/</a>
  
<br/>12-10-2019
</div>

<div class="d-article">
<p>How do you motivate, or come up with a story around Gaussian Process Regression on a blog primarily dedicated to deep learning?</p>
<p>Easy. As demonstrated by seemingly unavoidable, reliably recurring Twitter “wars” surrounding AI, nothing attracts attention like controversy and antagonism. So, let’s go back twenty years and find citations of people saying, “here come Gaussian Processes, we don’t need to bother with those finicky, hard to tune neural networks anymore!”. And today, here we are; everyone knows <em>something</em> about deep learning but who’s heard of Gaussian Processes?</p>
<p>While similar tales tell a lot about history of science and development of opinions, we prefer a different angle here. In the preface to their 2006 book on <em>Gaussian Processes for Machine Learning</em> <span class="citation" data-cites="Rasmussen">(Rasmussen and Williams <a href="#ref-Rasmussen" role="doc-biblioref">2005</a>)</span>, Rasmussen and Williams say, referring to the “two cultures” – the disciplines of statistics and machine learning, respectively:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<blockquote>
<p>Gaussian process models in some sense bring together work in the two communities.</p>
</blockquote>
<p>In this post, that “in some sense” gets very concrete. We’ll see a Keras network, defined and trained the usual way, that has a Gaussian Process layer for its main constituent. The task will be “simple” multivariate regression.</p>
<p>As an aside, this “bringing together communities” – or ways of thinking, or solution strategies – makes for a good overall characterization of TensorFlow Probability as well.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<h2 id="gaussian-processes">Gaussian Processes</h2>
<p>A Gaussian Process is a <a href="https://math.stackexchange.com/a/2297480">distribution over functions, where the function values you sample are jointly Gaussian</a> - roughly speaking, a generalization to infinity of the multivariate Gaussian. Besides the reference book we already mentioned <span class="citation" data-cites="Rasmussen">(Rasmussen and Williams <a href="#ref-Rasmussen" role="doc-biblioref">2005</a>)</span>, there are a number of nice introductions on the net: see e.g. <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">https://distill.pub/2019/visual-exploration-gaussian-processes/</a> or <a href="https://peterroelants.github.io/posts/gaussian-process-tutorial/">https://peterroelants.github.io/posts/gaussian-process-tutorial/</a>. And like on everything cool, there is a chapter on Gaussian Processes in the late David MacKay’s <span class="citation" data-cites="MacKay">(MacKay <a href="#ref-MacKay" role="doc-biblioref">2002</a>)</span> <a href="http://www.inference.org.uk/itprnn/book.pdf">book</a>.</p>
<p>In this post, we’ll use TensorFlow Probability’s <em>Variational Gaussian Process</em> (VGP) layer, designed to efficiently work with “big data”. As Gaussian Process Regression (GPR, from now on) involves the inversion of a – possibly big – covariance matrix, attempts have been made to design approximate versions, often based on variational principles. The TFP implementation is based on papers by Titsias (2009) <span class="citation" data-cites="titsias09a">(Titsias <a href="#ref-titsias09a" role="doc-biblioref">2009</a>)</span> and Hensman et al. (2013) <span class="citation" data-cites="HensmanFL13">(Hensman, Fusi, and Lawrence <a href="#ref-HensmanFL13" role="doc-biblioref">2013</a>)</span>. Instead of <span class="math inline">\(p(\mathbf{y}|\mathbf{X})\)</span>, the actual probability of the target data given the actual input, we work with a variational distribution <span class="math inline">\(q(\mathbf{u})\)</span> that acts as a lower bound.</p>
<p>Here <span class="math inline">\(\mathbf{u}\)</span> are the function values at a set of so-called <em>inducing index points</em> specified by the user, chosen to well cover the range of the actual data. This algorithm is a lot faster than “normal” GPR, as only the covariance matrix of <span class="math inline">\(\mathbf{u}\)</span> has to be inverted. As we’ll see below, at least in this example (as well as in others not described here) it seems to be pretty robust as to the number of <em>inducing points</em> passed.</p>
<p>Let’s start.</p>
<h2 id="the-dataset">The dataset</h2>
<p>The <a href="https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength">Concrete Compressive Strength Data Set</a> is part of the UCI Machine Learning Repository. Its web page says:</p>
<blockquote>
<p>Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients.</p>
</blockquote>
<p><em>Highly nonlinear function</em> - doesn’t that sound intriguing? In any case, it should constitute an interesting test case for GPR.</p>
<p>Here is a first look.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(tidyverse)
library(GGally)
library(visreg)
library(readxl)
library(rsample)
library(reticulate)
library(tfdatasets)
library(keras)
library(tfprobability)

concrete &lt;- read_xls(
  &quot;Concrete_Data.xls&quot;,
  col_names = c(
    &quot;cement&quot;,
    &quot;blast_furnace_slag&quot;,
    &quot;fly_ash&quot;,
    &quot;water&quot;,
    &quot;superplasticizer&quot;,
    &quot;coarse_aggregate&quot;,
    &quot;fine_aggregate&quot;,
    &quot;age&quot;,
    &quot;strength&quot;
  ),
  skip = 1
)

concrete %&gt;% glimpse()</code></pre>
</div>
<pre><code>
Observations: 1,030
Variables: 9
$ cement             &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, 380.0, …
$ blast_furnace_slag &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0, 114.0,…
$ fly_ash            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ water              &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228, 192, 1…
$ superplasticizer   &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0…
$ coarse_aggregate   &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0, 932.0…
$ fine_aggregate     &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, 594.0, …
$ age                &lt;dbl&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 28, 270,…
$ strength           &lt;dbl&gt; 79.986111, 61.887366, 40.269535, 41.052780, 44.296075, 4…</code></pre>
<p>It is not that big – just a little more than 1000 rows –, but still, we will have room to experiment with different numbers of <em>inducing points</em>.</p>
<p>We have eight predictors, all numeric. With the exception of <code>age</code> (in <em>days</em>), these represent masses (in <em>kg</em>) in one cubic metre of concrete. The target variable, <code>strength</code>, is measured in megapascals.</p>
<p>Let’s get a quick overview of mutual relationships.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggpairs(concrete)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/pairs.png" width="700" /></p>
</div>
<p>Checking for a possible interaction (one that a layperson could easily think of), does cement concentration act differently on concrete strength depending on how much water there is in the mixture?</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
cement_ &lt;- cut(concrete$cement, 3, labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;))
fit &lt;- lm(strength ~ (.) ^ 2, data = cbind(concrete[, 2:9], cement_))
summary(fit)

visreg(fit, &quot;cement_&quot;, &quot;water&quot;, gg = TRUE) + theme_minimal()</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/visreg.png" width="700" /></p>
</div>
<p>To anchor our future perception of how well VGP does for this example, we fit a simple linear model, as well as one involving two-way interactions.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# scale predictors here already, so data are the same for all models
concrete[, 1:8] &lt;- scale(concrete[, 1:8])

# train-test split 
set.seed(777)
split &lt;- initial_split(concrete, prop = 0.8)
train &lt;- training(split)
test &lt;- testing(split)

# simple linear model with no interactions
fit1 &lt;- lm(strength ~ ., data = train)
fit1 %&gt;% summary()</code></pre>
</div>
<pre><code>
Call:
lm(formula = strength ~ ., data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-30.594  -6.075   0.612   6.694  33.032 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         35.6773     0.3596  99.204  &lt; 2e-16 ***
cement              13.0352     0.9702  13.435  &lt; 2e-16 ***
blast_furnace_slag   9.1532     0.9582   9.552  &lt; 2e-16 ***
fly_ash              5.9592     0.8878   6.712 3.58e-11 ***
water               -2.5681     0.9503  -2.702  0.00703 ** 
superplasticizer     1.9660     0.6138   3.203  0.00141 ** 
coarse_aggregate     1.4780     0.8126   1.819  0.06929 .  
fine_aggregate       2.2213     0.9470   2.346  0.01923 *  
age                  7.7032     0.3901  19.748  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 10.32 on 816 degrees of freedom
Multiple R-squared:  0.627, Adjusted R-squared:  0.6234 
F-statistic: 171.5 on 8 and 816 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# two-way interactions
fit2 &lt;- lm(strength ~ (.) ^ 2, data = train)
fit2 %&gt;% summary()</code></pre>
</div>
<pre><code>
Call:
lm(formula = strength ~ (.)^2, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-24.4000  -5.6093  -0.0233   5.7754  27.8489 

Coefficients:
                                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                          40.7908     0.8385  48.647  &lt; 2e-16 ***
cement                               13.2352     1.0036  13.188  &lt; 2e-16 ***
blast_furnace_slag                    9.5418     1.0591   9.009  &lt; 2e-16 ***
fly_ash                               6.0550     0.9557   6.336 3.98e-10 ***
water                                -2.0091     0.9771  -2.056 0.040090 *  
superplasticizer                      3.8336     0.8190   4.681 3.37e-06 ***
coarse_aggregate                      0.3019     0.8068   0.374 0.708333    
fine_aggregate                        1.9617     0.9872   1.987 0.047256 *  
age                                  14.3906     0.5557  25.896  &lt; 2e-16 ***
cement:blast_furnace_slag             0.9863     0.5818   1.695 0.090402 .  
cement:fly_ash                        1.6434     0.6088   2.700 0.007093 ** 
cement:water                         -4.2152     0.9532  -4.422 1.11e-05 ***
cement:superplasticizer              -2.1874     1.3094  -1.670 0.095218 .  
cement:coarse_aggregate               0.2472     0.5967   0.414 0.678788    
cement:fine_aggregate                 0.7944     0.5588   1.422 0.155560    
cement:age                            4.6034     1.3811   3.333 0.000899 ***
blast_furnace_slag:fly_ash            2.1216     0.7229   2.935 0.003434 ** 
blast_furnace_slag:water             -2.6362     1.0611  -2.484 0.013184 *  
blast_furnace_slag:superplasticizer  -0.6838     1.2812  -0.534 0.593676    
blast_furnace_slag:coarse_aggregate  -1.0592     0.6416  -1.651 0.099154 .  
blast_furnace_slag:fine_aggregate     2.0579     0.5538   3.716 0.000217 ***
blast_furnace_slag:age                4.7563     1.1148   4.266 2.23e-05 ***
fly_ash:water                        -2.7131     0.9858  -2.752 0.006054 ** 
fly_ash:superplasticizer             -2.6528     1.2553  -2.113 0.034891 *  
fly_ash:coarse_aggregate              0.3323     0.7004   0.474 0.635305    
fly_ash:fine_aggregate                2.6764     0.7817   3.424 0.000649 ***
fly_ash:age                           7.5851     1.3570   5.589 3.14e-08 ***
water:superplasticizer                1.3686     0.8704   1.572 0.116289    
water:coarse_aggregate               -1.3399     0.5203  -2.575 0.010194 *  
water:fine_aggregate                 -0.7061     0.5184  -1.362 0.173533    
water:age                             0.3207     1.2991   0.247 0.805068    
superplasticizer:coarse_aggregate     1.4526     0.9310   1.560 0.119125    
superplasticizer:fine_aggregate       0.1022     1.1342   0.090 0.928239    
superplasticizer:age                  1.9107     0.9491   2.013 0.044444 *  
coarse_aggregate:fine_aggregate       1.3014     0.4750   2.740 0.006286 ** 
coarse_aggregate:age                  0.7557     0.9342   0.809 0.418815    
fine_aggregate:age                    3.4524     1.2165   2.838 0.004657 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 8.327 on 788 degrees of freedom
Multiple R-squared:  0.7656,    Adjusted R-squared:  0.7549 
F-statistic: 71.48 on 36 and 788 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We also store the predictions on the test set, for later comparison.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
linreg_preds1 &lt;- fit1 %&gt;% predict(test[, 1:8])
linreg_preds2 &lt;- fit2 %&gt;% predict(test[, 1:8])

compare &lt;-
  data.frame(
    y_true = test$strength,
    linreg_preds1 = linreg_preds1,
    linreg_preds2 = linreg_preds2
  )</code></pre>
</div>
<p>With no further preprocessing required, the <a href="https://tensorflow.rstudio.com/guide/tfdatasets/introduction/">tfdatasets</a> input pipeline ends up nice and short:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
create_dataset &lt;- function(df, batch_size, shuffle = TRUE) {
  
  df &lt;- as.matrix(df)
  ds &lt;-
    tensor_slices_dataset(list(df[, 1:8], df[, 9, drop = FALSE]))
  if (shuffle)
    ds &lt;- ds %&gt;% dataset_shuffle(buffer_size = nrow(df))
  ds %&gt;%
    dataset_batch(batch_size = batch_size)
  
}

# just one possible choice for batch size ...
batch_size &lt;- 64
train_ds &lt;- create_dataset(train, batch_size = batch_size)
test_ds &lt;- create_dataset(test, batch_size = nrow(test), shuffle = FALSE)</code></pre>
</div>
<p>And on to model creation.</p>
<h2 id="the-model">The model</h2>
<p>Model definition is short as well, although there are a few things to expand on. Don’t execute this yet:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 8,
              input_shape = 8,
              use_bias = FALSE) %&gt;%
  layer_variational_gaussian_process(
    # number of inducing points
    num_inducing_points = num_inducing_points,
    # kernel to be used by the wrapped Gaussian Process distribution
    kernel_provider = RBFKernelFn(),
    # output shape 
    event_shape = 1, 
    # initial values for the inducing points
    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),
    unconstrained_observation_noise_variance_initializer =
      initializer_constant(array(0.1))
  )</code></pre>
</div>
<p>Two arguments to <code>layer_variational_gaussian_process()</code> need some preparation before we can actually run this. First, as the documentation tells us, <code>kernel_provider</code> should be</p>
<blockquote>
<p>a layer instance equipped with an @property, which yields a <code>PositiveSemidefiniteKernel</code> instance".</p>
</blockquote>
<p>In other words, the VGP layer wraps another Keras layer that, <em>itself</em>, wraps or bundles together the TensorFlow <code>Variables</code> containing the kernel parameters.</p>
<p>We can make use of <code>reticulate</code>’s new <code>PyClass</code> constructor to fulfill the above requirements. Using <code>PyClass</code>, we can directly inherit from a Python object, adding and/or overriding methods or fields as we like - and yes, even create a Python <a href="https://docs.python.org/3/library/functions.html?#property">property</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
bt &lt;- import(&quot;builtins&quot;)
RBFKernelFn &lt;- reticulate::PyClass(
  &quot;KernelFn&quot;,
  inherit = tensorflow::tf$keras$layers$Layer,
  list(
    `__init__` = function(self, ...) {
      kwargs &lt;- list(...)
      super()$`__init__`(kwargs)
      dtype &lt;- kwargs[[&quot;dtype&quot;]]
      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),
                                            dtype = dtype,
                                            name = &#39;amplitude&#39;)
      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),
                                               dtype = dtype,
                                               name = &#39;length_scale&#39;)
      NULL
    },
    
    call = function(self, x, ...) {
      x
    },
    
    kernel = bt$property(
      reticulate::py_func(
        function(self)
          tfp$math$psd_kernels$ExponentiatedQuadratic(
            amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),
            length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
          )
      )
    )
  )
)</code></pre>
</div>
<p>The Gaussian Process kernel used is one of several available in <code>tfp.math.psd_kernels</code> (<code>psd</code> standing for positive semidefinite), and probably the one that comes to mind first when thinking of GPR: the <em>squared exponential</em>, or <em>exponentiated quadratic</em>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The version used in TFP, with hyperparameters <em>amplitude</em> <span class="math inline">\(a\)</span> and <em>length scale</em> <span class="math inline">\(\lambda\)</span>, is</p>
<p><span class="math display">\[k(x,x&#39;) = 2 \ a \ exp (\frac{- 0.5 (x−x&#39;)^2}{\lambda^2}) \]</span></p>
<p>Here the interesting parameter is the length scale <span class="math inline">\(\lambda\)</span>. When we have several features, their length scales – as induced by the learning algorithm – reflect their importance: If, for some feature, <span class="math inline">\(\lambda\)</span> is large, the respective squared deviations from the mean don’t matter that much. The inverse length scale can thus be used for <em>automatic relevance determination</em> <span class="citation" data-cites="Neal">(Neal <a href="#ref-Neal" role="doc-biblioref">1996</a>)</span>.</p>
<p>The second thing to take care of is choosing the initial index points. From experiments, the exact choices don’t matter that much, as long as the data are sensibly covered. For instance, an alternative way we tried was to construct an empirical distribution (<a href="https://rstudio.github.io/tfprobability/reference/tfd_empirical.html">tfd_empirical</a>) from the data, and then sample from it. Here instead, we just use an – unnecessary, admittedly, given the availability of <code>sample</code> in R – fancy way to select random observations from the training data:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
num_inducing_points &lt;- 50

sample_dist &lt;- tfd_uniform(low = 1, high = nrow(train) + 1)
sample_ids &lt;- sample_dist %&gt;%
  tfd_sample(num_inducing_points) %&gt;%
  tf$cast(tf$int32) %&gt;%
  as.numeric()
sampled_points &lt;- train[sample_ids, 1:8]</code></pre>
</div>
<p>One interesting point to note before we start training: Computation of the posterior predictive parameters involves a Cholesky decomposition, which could fail if, due to numerical issues, the covariance matrix is no longer positive definite. A sufficient action to take in our case is to do all computations using <code>tf$float64</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
k_set_floatx(&quot;float64&quot;)</code></pre>
</div>
<p>Now we define (for real, this time) and run the model.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 8,
              input_shape = 8,
              use_bias = FALSE) %&gt;%
  layer_variational_gaussian_process(
    num_inducing_points = num_inducing_points,
    kernel_provider = RBFKernelFn(),
    event_shape = 1,
    inducing_index_points_initializer = initializer_constant(as.matrix(sampled_points)),
    unconstrained_observation_noise_variance_initializer =
      initializer_constant(array(0.1))
  )

# KL weight sums to one for one epoch
kl_weight &lt;- batch_size / nrow(train)

# loss that implements the VGP algorithm
loss &lt;- function(y, rv_y)
  rv_y$variational_loss(y, kl_weight = kl_weight)

model %&gt;% compile(optimizer = optimizer_adam(lr = 0.008),
                  loss = loss,
                  metrics = &quot;mse&quot;)

history &lt;- model %&gt;% fit(train_ds,
                         epochs = 100,
                         validation_data = test_ds)

plot(history)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="images/history.png" width="700" /></p>
</div>
<p>Interestingly, higher numbers of <em>inducing points</em> (we tried 100 and 200) did not have much impact on regression performance.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Nor does the exact choice of multiplication constants (<code>0.1</code> and <code>2</code>) applied to the trained kernel <code>Variables</code> (<code>_amplitude</code> and <code>_length_scale</code>)</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
tfp$math$psd_kernels$ExponentiatedQuadratic(
  amplitude = tf$nn$softplus(array(0.1) * self$`_amplitude`),
  length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
)</code></pre>
</div>
<p>make much of a difference to the end result.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<h2 id="predictions">Predictions</h2>
<p>We generate predictions on the test set and add them to the <code>data.frame</code> containing the linear models’ predictions. As with other probabilistic output layers, “the predictions” are in fact distributions; to obtain actual tensors we sample from them. Here, we average over 10 samples:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
yhats &lt;- model(tf$convert_to_tensor(as.matrix(test[, 1:8])))

yhat_samples &lt;-  yhats %&gt;%
  tfd_sample(10) %&gt;%
  tf$squeeze() %&gt;%
  tf$transpose()

sample_means &lt;- yhat_samples %&gt;% apply(1, mean)

compare &lt;- compare %&gt;%
  cbind(vgp_preds = sample_means)</code></pre>
</div>
<p>We plot the average VGP predictions against the ground truth, together with the predictions from the simple linear model (cyan) and the model including two-way interactions (violet):</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ggplot(compare, aes(x = y_true)) +
  geom_abline(slope = 1, intercept = 0) +
  geom_point(aes(y = vgp_preds, color = &quot;VGP&quot;)) +
  geom_point(aes(y = linreg_preds1, color = &quot;simple lm&quot;), alpha = 0.4) +
  geom_point(aes(y = linreg_preds2, color = &quot;lm w/ interactions&quot;), alpha = 0.4) +
  scale_colour_manual(&quot;&quot;, 
                      values = c(&quot;VGP&quot; = &quot;black&quot;, &quot;simple lm&quot; = &quot;cyan&quot;, &quot;lm w/ interactions&quot; = &quot;violet&quot;)) +
  coord_cartesian(xlim = c(min(compare$y_true), max(compare$y_true)), ylim = c(min(compare$y_true), max(compare$y_true))) +
  ylab(&quot;predictions&quot;) +
  theme(aspect.ratio = 1) </code></pre>
</div>
<div class="layout-chunk" data-layout="l-page">
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="images/preds.png" alt="Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black)." width="448" />
<p class="caption">
Figure 1: Predictions vs. ground truth for linear regression (no interactions; cyan), linear regression with 2-way interactions (violet), and VGP (black).
</p>
</div>
</div>
<p>Additionally, comparing MSEs for the three sets of predictions, we see</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
mse &lt;- function(y_true, y_pred) {
  sum((y_true - y_pred) ^ 2) / length(y_true)
}

lm_mse1 &lt;- mse(compare$y_true, compare$linreg_preds1) # 117.3111
lm_mse2 &lt;- mse(compare$y_true, compare$linreg_preds2) # 80.79726
vgp_mse &lt;- mse(compare$y_true, compare$vgp_preds)     # 58.49689</code></pre>
</div>
<p>So, the VGP does in fact outperform both baselines. Something else we might be interested in: How do its predictions vary? Not as much as we might want, were we to construct uncertainty estimates from them alone. Here we plot the 10 samples we drew before:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
samples_df &lt;-
  data.frame(cbind(compare$y_true, as.matrix(yhat_samples))) %&gt;%
  gather(key = run, value = prediction, -X1) %&gt;% 
  rename(y_true = &quot;X1&quot;)

ggplot(samples_df, aes(y_true, prediction)) +
  geom_point(aes(color = run),
             alpha = 0.2,
             size = 2) +
  geom_abline(slope = 1, intercept = 0) +
  theme(legend.position = &quot;none&quot;) +
  ylab(&quot;repeated predictions&quot;) +
  theme(aspect.ratio = 1)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-page">
<div class="figure"><span id="fig:unnamed-chunk-22"></span>
<img src="images/preds2.png" alt="Predictions from 10 consecutive samples from the VGP distribution." width="700" />
<p class="caption">
Figure 2: Predictions from 10 consecutive samples from the VGP distribution.
</p>
</div>
</div>
<h2 id="discussion-feature-relevance">Discussion: Feature Relevance</h2>
<p>As mentioned above, the inverse length scale can be used as an indicator of feature importance. When using the <code>ExponentiatedQuadratic</code> kernel alone, there will only be a single length scale; in our example, the initial <code>dense</code> layer takes of scaling (and additionally, recombining) the features.</p>
<p>Alternatively, we could wrap the <code>ExponentiatedQuadratic</code> in a <code>FeatureScaled</code> kernel. <code>FeatureScaled</code> has an additional <code>scale_diag</code> parameter related to exactly that: feature scaling. Experiments with <code>FeatureScaled</code> (and initial <code>dense</code> layer removed, to be “fair”) showed slightly worse performance, and the learned <code>scale_diag</code> values varied quite a bit from run to run. For that reason, we chose to present the other approach; however, we include the code for a wrapping <code>FeatureScaled</code> in case readers would like to experiment with this:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
ScaledRBFKernelFn &lt;- reticulate::PyClass(
  &quot;KernelFn&quot;,
  inherit = tensorflow::tf$keras$layers$Layer,
  list(
    `__init__` = function(self, ...) {
      kwargs &lt;- list(...)
      super()$`__init__`(kwargs)
      dtype &lt;- kwargs[[&quot;dtype&quot;]]
      self$`_amplitude` = self$add_variable(initializer = initializer_zeros(),
                                            dtype = dtype,
                                            name = &#39;amplitude&#39;)
      self$`_length_scale` = self$add_variable(initializer = initializer_zeros(),
                                               dtype = dtype,
                                               name = &#39;length_scale&#39;)
      self$`_scale_diag` = self$add_variable(
        initializer = initializer_ones(),
        dtype = dtype,
        shape = 8L,
        name = &#39;scale_diag&#39;
      )
      NULL
    },
    
    call = function(self, x, ...) {
      x
    },
    
    kernel = bt$property(
      reticulate::py_func(
        function(self)
          tfp$math$psd_kernels$FeatureScaled(
            kernel = tfp$math$psd_kernels$ExponentiatedQuadratic(
              amplitude = tf$nn$softplus(array(1) * self$`_amplitude`),
              length_scale = tf$nn$softplus(array(2) * self$`_length_scale`)
            ),
            scale_diag = tf$nn$softplus(array(1) * self$`_scale_diag`)
          )
      )
    )
  )
)</code></pre>
</div>
<p>Finally, if all you cared about was prediction performance, you could use <code>FeatureScaled</code> and keep the initial <code>dense</code> layer all the same. But in that case, you’d probably use a neural network – not a Gaussian Process – anyway …</p>
<p>Thanks for reading!</p>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-breiman2001">
<p>Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statist. Sci.</em> 16 (3): 199–231. <a href="https://doi.org/10.1214/ss/1009213726">https://doi.org/10.1214/ss/1009213726</a>.</p>
</div>
<div id="ref-HensmanFL13">
<p>Hensman, James, Nicolo Fusi, and Neil D. Lawrence. 2013. “Gaussian Processes for Big Data.” <em>CoRR</em> abs/1309.6835. <a href="http://arxiv.org/abs/1309.6835">http://arxiv.org/abs/1309.6835</a>.</p>
</div>
<div id="ref-MacKay">
<p>MacKay, David J. C. 2002. <em>Information Theory, Inference &amp; Learning Algorithms</em>. New York, NY, USA: Cambridge University Press.</p>
</div>
<div id="ref-Neal">
<p>Neal, Radford M. 1996. <em>Bayesian Learning for Neural Networks</em>. Berlin, Heidelberg: Springer-Verlag.</p>
</div>
<div id="ref-Rasmussen">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2005. <em>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</em>. The MIT Press.</p>
</div>
<div id="ref-titsias09a">
<p>Titsias, Michalis. 2009. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In <em>Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</em>, edited by David van Dyk and Max Welling, 5:567–74. Proceedings of Machine Learning Research. Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA: PMLR. <a href="http://proceedings.mlr.press/v5/titsias09a.html">http://proceedings.mlr.press/v5/titsias09a.html</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>In the book, it’s “two communities”, not “two cultures”; we choose L. Breiman’s <span class="citation" data-cites="breiman2001">(Breiman <a href="#ref-breiman2001" role="doc-biblioref">2001</a>)</span> more punchy expression just … because.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>So far, we have seen uses of TensorFlow Probability for such different applications as <a href="https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet/">adding uncertainty estimates to neural networks</a>, <a href="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/">Bayesian model estimation using Hamiltonian Monte Carlo</a>, or <a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/">linear-Gaussian state space models</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>See David Duvenaud’s <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/" class="uri">https://www.cs.toronto.edu/~duvenaud/cookbook/</a> for an excellent synopsis of kernels and kernel composition.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>In case you’re wondering about the <code>dense</code> layer up front: This will be discussed in the last section on feature relevance.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Performance being used in the sense of “lower regression error”, not running speed. As to the latter, there definitely is a (negative) relationship between number of inducing points and training speed.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>This may sound like a matter of course; it isn’t necessarily, as shown by prior experiments with variational layers e.g. in <a href="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/">Adding uncertainty estimates to Keras models with tfprobability</a>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<script id="distill-bibliography" type="text/bibtex">
@article{Yeh,
author = {Yeh, I-Cheng},
year = {1998},
month = {12},
pages = {1797-1808},
title = {Modeling of Strength of High-Performance Concrete Using Artificial Neural Networks},
volume = {28},
journal = {Cement and Concrete Research},
doi = {10.1016/S0008-8846(98)00165-3}
}

@book{Rasmussen,
 author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
 title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
 year = {2005},
 isbn = {026218253X},
 publisher = {The MIT Press},
} 

@article{breiman2001,
author = "Breiman, Leo",
doi = "10.1214/ss/1009213726",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "08",
number = "3",
pages = "199--231",
publisher = "The Institute of Mathematical Statistics",
title = "Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)",
url = "https://doi.org/10.1214/ss/1009213726",
volume = "16",
year = "2001"
}

@book{MacKay,
 author = {MacKay, David J. C.},
 title = {Information Theory, Inference & Learning Algorithms},
 year = {2002},
 isbn = {0521642981},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}



@InProceedings{titsias09a,
  title = 	 {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  author = 	 {Michalis Titsias},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {567--574},
  year = 	 {2009},
  editor = 	 {David van Dyk and Max Welling},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
  url = 	 {http://proceedings.mlr.press/v5/titsias09a.html},
}

@article{HensmanFL13,
  author    = {James Hensman and
               Nicolo Fusi and
               Neil D. Lawrence},
  title     = {Gaussian Processes for Big Data},
  journal   = {CoRR},
  volume    = {abs/1309.6835},
  year      = {2013},
  url       = {http://arxiv.org/abs/1309.6835},
  archivePrefix = {arXiv},
  eprint    = {1309.6835},
  timestamp = {Mon, 13 Aug 2018 16:48:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HensmanFL13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{Neal,
 author = {Neal, Radford M.},
 title = {Bayesian Learning for Neural Networks},
 year = {1996},
 isbn = {0387947248},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
