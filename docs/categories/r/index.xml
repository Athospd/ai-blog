<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
<<<<<<< HEAD
    <lastBuildDate>Mon, 17 Aug 2020 00:00:00 +0000</lastBuildDate>
=======
    <lastBuildDate>Thu, 30 Jul 2020 00:00:00 +0000</lastBuildDate>
>>>>>>> a0dd341f2363dbc5e008acf49e3adc0cafcd27a0
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop_vae(ds_train)&lt;/p&gt;
&lt;p&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next() encoded &amp;lt;- encoder(test_batch[[1]][1:1000]) test_var &amp;lt;- tf&lt;span class="math inline"&gt;\(math\)&lt;/span&gt;reduce_variance(encoded, axis = 0L) print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5)) } ```&lt;/p&gt;
&lt;div id="experimental-setup-and-data" class="section level2"&gt;
&lt;h2&gt;Experimental setup and data&lt;/h2&gt;
&lt;p&gt;The idea was to add white noise to a deterministic series. This time, the &lt;a href="https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor"&gt;Roessler system&lt;/a&gt; was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler.png" alt="Roessler attractor, two-dimensional projections." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Roessler attractor, two-dimensional projections.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like we did for the Lorenz system in the first part of this series, we use &lt;code&gt;deSolve&lt;/code&gt; to generate data from the Roessler equations.&lt;/p&gt;
&lt;p&gt;Then, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.&lt;/p&gt;
&lt;p&gt;Here you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler_noise.png" alt="Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Otherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="results" class="section level2"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an &lt;code&gt;fnn_multiplier&lt;/code&gt; of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.&lt;/p&gt;
&lt;p&gt;As a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In all cases&lt;/em&gt; here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.&lt;/p&gt;
&lt;div id="low-noise" class="section level4"&gt;
&lt;h4&gt;Low noise&lt;/h4&gt;
&lt;p&gt;Seeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (&lt;code&gt;x&lt;/code&gt;, 120 steps) and output (&lt;code&gt;y&lt;/code&gt;, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?&lt;/p&gt;
&lt;p&gt;Looking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we start to add noise?&lt;/p&gt;
&lt;/div&gt;
&lt;div id="substantial-noise" class="section level4"&gt;
&lt;h4&gt;Substantial noise&lt;/h4&gt;
&lt;p&gt;Between noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.&lt;/p&gt;
&lt;p&gt;Here first are predictions obtained from the unregularized models.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were &lt;em&gt;trained&lt;/em&gt; on the noisy version; predict fluctuations is what they learned.&lt;/p&gt;
&lt;p&gt;Do we see the same with the FNN models?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-9)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.&lt;/p&gt;
&lt;p&gt;“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.&lt;/p&gt;
&lt;p&gt;However, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.&lt;/p&gt;
&lt;p&gt;In the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, &lt;em&gt;MSEs have been normalized as fractions of the maximum MSE in a category&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we want to predict &lt;em&gt;signal plus noise&lt;/em&gt; (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/mses.png" alt="Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right)." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="summing-up" class="section level2"&gt;
&lt;h2&gt;Summing up&lt;/h2&gt;
&lt;p&gt;Our experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.&lt;/p&gt;
&lt;p&gt;With that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">931e811b30064bd6be23eefda987ed92</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop(ds_train)&lt;br /&gt;
}&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;After two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.&lt;/p&gt;
&lt;div id="obtaining-the-attractor-from-the-test-set" class="section level3"&gt;
&lt;h3&gt;Obtaining the attractor from the test set&lt;/h3&gt;
&lt;p&gt;We use the test set to inspect the latent code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,242 x 10
      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 
 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 
 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 
 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 
 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127
 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 
 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 
 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 
 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 
10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 
# … with 6,232 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_weight&lt;/code&gt; of 10, we do see a drop after the first two units:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 10
      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Here, this results in three projections of the set &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/predicted_attractors.png" alt="Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="wrapping-up-for-this-time" class="section level2"&gt;
&lt;h2&gt;Wrapping up (for this time)&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom &lt;em&gt;false nearest neighbors&lt;/em&gt; loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.&lt;/p&gt;
&lt;p&gt;This is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again &lt;em&gt;of course&lt;/em&gt;, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;? There is a lot to explore, stay tuned – and as always, thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;As per author recommendation (personal communication).&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See &lt;span class="citation"&gt;[@Kantz]&lt;/span&gt; for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">5503b1da2d7660fbcd2bc810fb123a30</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="2250" height="1140"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="2378" height="1563"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
