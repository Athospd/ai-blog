<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 28 Sep 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Please allow me to introduce myself: torch for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;Last January at &lt;em&gt;rstudio::conf&lt;/em&gt;, in that distant past when conferences still used to take place at some physical location, my colleague Daniel gave a talk introducing new features and ongoing development in the &lt;code&gt;tensorflow&lt;/code&gt; ecosystem. In the Q&amp;amp;A part, he was asked something unexpected: Were we going to build support for PyTorch? He hesitated; that was in fact the plan, and he had already played around with natively implementing &lt;code&gt;torch&lt;/code&gt; tensors at a prior time, but he was not completely certain how well “it” would work.&lt;/p&gt;
&lt;p&gt;“It”, that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via &lt;code&gt;reticulate&lt;/code&gt;. Instead, we delegate to the underlying C++ library &lt;code&gt;libtorch&lt;/code&gt; for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, &lt;code&gt;torch&lt;/code&gt; does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.&lt;/p&gt;
&lt;p&gt;So why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against &lt;code&gt;libtorch&lt;/code&gt; would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable; at some point, my colleague and &lt;code&gt;torch&lt;/code&gt;-coauthor Javier might want to tell you about it.)&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that &lt;code&gt;torch&lt;/code&gt; can be useful to the R community. Thus, without further ado, let’s train a neural network.&lt;/p&gt;
&lt;p&gt;You’re not at your laptop now? Just follow along in the &lt;a href="https://colab.research.google.com/drive/1NdiN9n_a7NEvFpvjPDvxKTshrSWgxZK5?usp=sharing"&gt;companion notebook on Colaboratory&lt;/a&gt;.&lt;/p&gt;
&lt;div id="installation" class="section level2"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;div id="torch" class="section level4"&gt;
&lt;h4&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Installing &lt;code&gt;torch&lt;/code&gt; is as straightforward as typing&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will detect whether you have CUDA installed, and either download the CPU or the GPU version of &lt;code&gt;libtorch&lt;/code&gt;. Then, it will install the R package from CRAN. To make use of the very newest features, you can also install the development version from GitHub:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;devtools::install_github(&amp;quot;mlverse/torch&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To quickly check the installation, and whether GPU support works fine (assuming that there &lt;em&gt;is&lt;/em&gt; a GPU), create a tensor &lt;em&gt;on the CUDA device&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;torch_tensor(1, device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
[ CUDAFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If all our &lt;em&gt;hello torch&lt;/em&gt; example did was running a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="torchvision" class="section level4"&gt;
&lt;h4&gt;&lt;code&gt;torchvision&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Whereas &lt;code&gt;torch&lt;/code&gt; is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.&lt;/p&gt;
&lt;p&gt;As of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan”, because &lt;code&gt;torchtext&lt;/code&gt; and &lt;code&gt;torchaudio&lt;/code&gt; are still to be created. Right now, &lt;code&gt;torchvision&lt;/code&gt; is all we need:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;torchvision&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: As &lt;code&gt;torchvision&lt;/code&gt; relies on &lt;a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html"&gt;magick&lt;/a&gt;, which itself needs &lt;a href="https://imagemagick.org/"&gt;ImageMagick&lt;/a&gt; installed, you need to ensure the latter is available on your system. &lt;a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html"&gt;The magick vignette&lt;/a&gt; has an excellent description on how to do this on different operating systems.&lt;/p&gt;
&lt;p&gt;And we’re ready to load the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="data-loading-and-pre-processing" class="section level2"&gt;
&lt;h2&gt;Data loading and pre-processing&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(torchvision)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The list of vision datasets bundled with PyTorch is long, and they’re continually being added to &lt;code&gt;torchvision&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin”, &lt;a href="https://github.com/rois-codh/kmnist"&gt;Kuzushiji-MNIST&lt;/a&gt; &lt;span class="citation"&gt;[@clanuwat2018deep]&lt;/span&gt;. Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution &lt;code&gt;28x28&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are the first 32 characters:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-09-29-introducing-torch-for-r/images/kmnist.png" alt="Kuzushiji MNIST." width="768" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Kuzushiji MNIST.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="dataset" class="section level4"&gt;
&lt;h4&gt;Dataset&lt;/h4&gt;
&lt;p&gt;The following code will download the data, separately for training and test sets.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dir &amp;lt;- &amp;quot;.&amp;quot;

train_ds &amp;lt;- kmnist_dataset(
  dir,
  download = TRUE,
  train = TRUE,
  transform = function(x) {
    x &amp;lt;- x$to(dtype = torch_float())/256
    x[newaxis,..]
  }
)

test_ds &amp;lt;- kmnist_dataset(
  dir,
  download = TRUE,
  train = FALSE,
  transform = function(x) {
    x &amp;lt;- x$to(dtype = torch_float())/256
    x[newaxis,..]
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;transform&lt;/code&gt; argument. The passed-in function applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?&lt;/p&gt;
&lt;p&gt;Contrary to what you might expect – if until now, you’ve been using &lt;code&gt;keras&lt;/code&gt; – the additional dimension is &lt;em&gt;not&lt;/em&gt; the batch dimension. Batching will be taken care of by the &lt;code&gt;dataloader&lt;/code&gt;, to be introduced next. Instead, this is the &lt;em&gt;channels&lt;/em&gt; dimension that in &lt;code&gt;torch&lt;/code&gt;, is found &lt;em&gt;before&lt;/em&gt; the width and height dimensions by default.&lt;/p&gt;
&lt;p&gt;One thing I’ve found to be extremely useful about &lt;code&gt;torch&lt;/code&gt; is how easy it is to inspect objects. Even though we’re dealing with a &lt;code&gt;dataset&lt;/code&gt;, a custom object, and not an R array or even a &lt;code&gt;torch&lt;/code&gt; tensor, we can easily peek at what’s inside. Indexing in &lt;code&gt;torch&lt;/code&gt; is 1-based, conforming to the R user’s intuitions. Consequently,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives us the first element in the dataset, an R &lt;em&gt;list&lt;/em&gt; of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)&lt;/p&gt;
&lt;p&gt;Let’s inspect the shape of the input tensor:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds[1][[1]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 28 28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In &lt;code&gt;torch&lt;/code&gt;, this is the task of data loaders.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="data-loader" class="section level4"&gt;
&lt;h4&gt;Data loader&lt;/h4&gt;
&lt;p&gt;Each of training and test sets get their own data loader:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dl &amp;lt;- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl &amp;lt;- dataloader(test_ds, batch_size = 32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, &lt;code&gt;torch&lt;/code&gt; makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_iter &amp;lt;- train_dl$.iter()
train_iter$.next()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functionality like this may not seem indispensable when working with a well-known dataset, but will turn out to be very useful when a lot of domain-specific pre-processing is required.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(4,8), mar = rep(0, 4))
images &amp;lt;- train_dl$.iter()$.next()[[1]][1:32, 1, , ] 
images %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::map(as.raster) %&amp;gt;%
  purrr::iwalk(~{plot(.x)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re ready to define our network – a simple convnet.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="network" class="section level2"&gt;
&lt;h2&gt;Network&lt;/h2&gt;
&lt;p&gt;If you’ve been using &lt;code&gt;keras&lt;/code&gt; &lt;em&gt;custom models&lt;/em&gt; (or have some experience with &lt;em&gt;Py&lt;/em&gt;Torch), the following way of defining a network may not look too surprising.&lt;/p&gt;
&lt;p&gt;You use &lt;code&gt;nn_module()&lt;/code&gt; to define an R6 class that will hold the network’s components. Its layers are created in &lt;code&gt;initialize()&lt;/code&gt;; &lt;code&gt;forward()&lt;/code&gt; describes what happens during the network’s forward pass. One thing on terminology: In &lt;code&gt;torch&lt;/code&gt;, layers are called &lt;em&gt;modules&lt;/em&gt;, as are networks. This makes sense: The design is truly &lt;em&gt;modular&lt;/em&gt; in that any module can be used as a component in a larger one.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;net &amp;lt;- nn_module(
  
  &amp;quot;KMNIST-CNN&amp;quot;,
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    self$conv1 &amp;lt;- nn_conv2d(1, 32, 3)
    self$conv2 &amp;lt;- nn_conv2d(32, 64, 3)
    self$dropout1 &amp;lt;- nn_dropout2d(0.25)
    self$dropout2 &amp;lt;- nn_dropout2d(0.5)
    self$fc1 &amp;lt;- nn_linear(9216, 128)
    self$fc2 &amp;lt;- nn_linear(128, 10)
  },
  
  forward = function(x) {
    x %&amp;gt;% 
      self$conv1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$conv2() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      nnf_max_pool2d(2) %&amp;gt;%
      self$dropout1() %&amp;gt;%
      torch_flatten(start_dim = 2) %&amp;gt;%
      self$fc1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$dropout2() %&amp;gt;%
      self$fc2()
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The layers – apologies: modules – themselves may look familiar. Unsurprisingly, &lt;code&gt;nn_conv2d()&lt;/code&gt; performs two-dimensional convolution; &lt;code&gt;nn_linear()&lt;/code&gt; multiplies by a weight matrix and adds a vector of biases. But what are those numbers: &lt;code&gt;nn_linear(128, 10)&lt;/code&gt;, say?&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, &lt;code&gt;nn_linear(128, 10)&lt;/code&gt; has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about this module’s predecessor? How do we arrive at &lt;code&gt;9216&lt;/code&gt; input connections?&lt;/p&gt;
&lt;p&gt;Here, a bit of calculation is necessary. We go through all actions that happen in &lt;code&gt;forward()&lt;/code&gt; – if they change shapes, we keep track of the transformation; if they don’t, we ignore them.&lt;/p&gt;
&lt;p&gt;So, we start with input tensors of shape &lt;code&gt;batch_size x 1 x 28 x 28&lt;/code&gt;. Then,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(1, 32, 3)&lt;/code&gt; , or equivalently, &lt;code&gt;nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),&lt;/code&gt;applies a convolution with kernel size 3, stride 1 (the default) and no padding (the default). We can consult the &lt;a href="https://mlverse.github.io/torch/reference/nn_conv2d.html"&gt;documentation&lt;/a&gt; to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of &lt;code&gt;26 x 26&lt;/code&gt;. &lt;em&gt;Per channel&lt;/em&gt;, that is. Thus, the actual output shape is &lt;code&gt;batch_size x 32 x 26 x 26&lt;/code&gt; . Next,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; applies ReLU activation, in no way touching the shape. Next is&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_conv2d(32, 64, 3)&lt;/code&gt;, another convolution with zero padding and kernel size 3. Output size now is &lt;code&gt;batch_size x 64 x 24 x 24&lt;/code&gt; . Now, the second&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; again does nothing to the output shape, but&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_max_pool2d(2)&lt;/code&gt; (equivalently: &lt;code&gt;nnf_max_pool2d(kernel_size = 2)&lt;/code&gt;) does: It applies max pooling over regions of extension &lt;code&gt;2 x 2&lt;/code&gt;, thus downsizing the output to a format of &lt;code&gt;batch_size x 32 x 12 x 12&lt;/code&gt; . Now,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_dropout2d(0.25)&lt;/code&gt; is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the &lt;em&gt;channels&lt;/em&gt;, &lt;em&gt;height&lt;/em&gt; and &lt;em&gt;width&lt;/em&gt; axes into a single dimension. This is done in&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;torch_flatten(start_dim = 2)&lt;/code&gt;. Output shape is now &lt;code&gt;batch_size * 9216&lt;/code&gt; , since &lt;code&gt;64 * 12 * 12 = 9216&lt;/code&gt; . Thus here we have the &lt;code&gt;9216&lt;/code&gt; input connections fed into the&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(9216, 128)&lt;/code&gt; discussed above. Again,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nnf_relu()&lt;/code&gt; and &lt;code&gt;nn_dropout2d(0.5)&lt;/code&gt; leave dimensions as they are, and finally,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;nn_linear(128, 10)&lt;/code&gt; gives us the desired output scores, one for each of the ten classes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with &lt;code&gt;torch&lt;/code&gt;’s flexibility, there is another way. Since every layer is callable &lt;em&gt;in isolation&lt;/em&gt;, we can just … create some sample data and see what happens!&lt;/p&gt;
&lt;p&gt;Here is a sample “image” – or more precisely, a one-item batch containing it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- torch_randn(c(1, 1, 28, 28))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we call the first &lt;em&gt;conv2d&lt;/em&gt; module on it?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv1 &amp;lt;- nn_conv2d(1, 32, 3)
conv1(x)$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 32 26 26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or both &lt;em&gt;conv2d&lt;/em&gt; modules?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;conv2 &amp;lt;- nn_conv2d(32, 64, 3)
(conv1(x) %&amp;gt;% conv2())$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  1 64 24 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And so on. This is just one example illustrating how &lt;code&gt;torch&lt;/code&gt;s flexibility makes developing neural nets easier.&lt;/p&gt;
&lt;p&gt;Back to the main thread. We instantiate the model, and we ask &lt;code&gt;torch&lt;/code&gt; to allocate its weights (parameters) on the GPU:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- net()$to(device = &amp;quot;cuda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="training" class="section level2"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the loss function? For classification with more than two classes, we use &lt;em&gt;cross entropy&lt;/em&gt;, in &lt;code&gt;torch&lt;/code&gt;: &lt;code&gt;nnf_cross_entropy(prediction, ground_truth)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# this will be called for every batch, see training loop below
loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike categorical cross entropy in &lt;code&gt;keras&lt;/code&gt; , which would expect &lt;code&gt;prediction&lt;/code&gt; to contain probabilities, as obtained by applying a &lt;em&gt;softmax&lt;/em&gt; activation, &lt;code&gt;torch&lt;/code&gt;’s &lt;code&gt;nnf_cross_entropy()&lt;/code&gt; works with the raw outputs (the &lt;em&gt;logits&lt;/em&gt;). This is why the network’s last linear layer was not followed by any activation.&lt;/p&gt;
&lt;p&gt;The training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;for (epoch in 1:5) {

  l &amp;lt;- c()

  for (b in enumerate(train_dl)) {
    # make sure each batch&amp;#39;s gradient updates are calculated from a fresh start
    optimizer$zero_grad()
    # get model predictions
    output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate loss
    loss &amp;lt;- nnf_cross_entropy(output, b[[2]]$to(device = &amp;quot;cuda&amp;quot;))
    # calculate gradient
    loss$backward()
    # apply weight updates
    optimizer$step()
    # track losses
    l &amp;lt;- c(l, loss$item())
  }

  cat(sprintf(&amp;quot;Loss at epoch %d: %3f\n&amp;quot;, epoch, mean(l)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: 1.795564
Loss at epoch 2: 1.540063
Loss at epoch 3: 1.495343
Loss at epoch 4: 1.461649
Loss at epoch 5: 1.446628&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there is a lot more that &lt;em&gt;could&lt;/em&gt; be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a &lt;code&gt;torch&lt;/code&gt; training loop.&lt;/p&gt;
&lt;p&gt;The optimizer-related idioms in particular&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$zero_grad()
# ...
loss$backward()
# ...
optimizer$step()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you’ll keep encountering over and over.&lt;/p&gt;
&lt;p&gt;Finally, let’s evaluate model performance on the test set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="evaluation" class="section level2"&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Putting a model in &lt;code&gt;eval&lt;/code&gt; mode tells &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;not&lt;/em&gt; to calculate gradients and perform backprop during the operations that follow:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We iterate over the test set, keeping track of losses and accuracies obtained on the batches.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_losses &amp;lt;- c()
total &amp;lt;- 0
correct &amp;lt;- 0

for (b in enumerate(test_dl)) {
  output &amp;lt;- model(b[[1]]$to(device = &amp;quot;cuda&amp;quot;))
  labels &amp;lt;- b[[2]]$to(device = &amp;quot;cuda&amp;quot;)
  loss &amp;lt;- nnf_cross_entropy(output, labels)
  test_losses &amp;lt;- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values 
  # and position 2 containing the respective indices
  predicted &amp;lt;- torch_max(output$data(), dim = 2)[[2]]
  total &amp;lt;- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct &amp;lt;- correct + (predicted == labels)$sum()$item()
}

mean(test_losses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.53784480643349&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is mean accuracy, computed as proportion of correct classifications:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_accuracy &amp;lt;-  correct/total
test_accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9449&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for our first &lt;code&gt;torch&lt;/code&gt; example. Where to from here?&lt;/p&gt;
&lt;/div&gt;
&lt;div id="learn" class="section level2"&gt;
&lt;h2&gt;Learn&lt;/h2&gt;
&lt;p&gt;To learn more, check out our vignettes on the &lt;code&gt;torch&lt;/code&gt; &lt;a href="https://mlverse.github.io/torch"&gt;website&lt;/a&gt;. To begin, you may want to check out these in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Getting started” series: Build a simple neural network from scratch, starting from &lt;a href="https://mlverse.github.io/tohttps://mlverse.github.io/torch/articles/getting-started/tensors.html"&gt;low-level tensor manipulation&lt;/a&gt; and gradually adding in higher-level features like &lt;a href="https://mlverse.github.io/torch/articles/getting-started/tensors-and-autograd.html"&gt;automatic differentiation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/getting-started/nn.html"&gt;network modules&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;More on tensors: &lt;a href="https://mlverse.github.io/torch/articles/tensor-creation.html"&gt;Tensor creation&lt;/a&gt; and &lt;a href="https://mlverse.github.io/torch/articles/indexing.html"&gt;indexing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Backpropagation in &lt;code&gt;torch&lt;/code&gt;: &lt;a href="https://mlverse.github.io/torch/articles/using-autograd.html"&gt;autograd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have questions, or run into problems, please feel free to ask on &lt;a href="https://github.com/mlverse/torch"&gt;GitHub&lt;/a&gt; or on the &lt;a href="https://community.rstudio.com/"&gt;RStudio community forum&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="we-need-you" class="section level2"&gt;
&lt;h2&gt;We need you&lt;/h2&gt;
&lt;p&gt;We very much wish that the R community will find the new functionality useful. But that’s not all. Instead, we hope that you, many of you, will take part in the journey.&lt;/p&gt;
&lt;p&gt;There is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.&lt;/p&gt;
&lt;p&gt;There is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps &lt;em&gt;the&lt;/em&gt; essential factor in how usable a framework is.&lt;/p&gt;
&lt;p&gt;Then, there is the ever-expanding ecosystem of libraries built on top of PyTorch: &lt;a href="https://github.com/OpenMined/PySyft"&gt;PySyft&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/CrypTen"&gt;CrypTen&lt;/a&gt; for privacy-preserving machine learning, &lt;a href="https://github.com/rusty1s/pytorch_geometric"&gt;PyTorch Geometric&lt;/a&gt; for deep learning on manifolds, and &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt; for probabilistic programming, to name just a few.&lt;/p&gt;
&lt;p&gt;All this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely &lt;em&gt;any&lt;/em&gt; scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add or improve documentation, add introductory examples&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement missing layers (modules), activations, helper functions…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Implement model architectures&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Port some of the PyTorch ecosystem&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One component that should be of special interest to the R community is &lt;a href="https://pytorch.org/docs/stable/distributions.html"&gt;Torch distributions&lt;/a&gt;, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned &lt;a href="http://pyro.ai/"&gt;Pyro&lt;/a&gt;; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.&lt;/p&gt;
&lt;p&gt;To reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with &lt;code&gt;torch&lt;/code&gt;, and thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Lest there be any misunderstandings – coauthor not to yours truly, but to Daniel, creator and main author of the package.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">badabdb56d35c151b49117b5db3f7f00</distill:md5>
      <category>Packages/Releases</category>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</guid>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/torch.png" medium="image" type="image/png" width="347" height="400"/>
    </item>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;Same with weekly climatology: Looking back at how warm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.&lt;/p&gt;
&lt;p&gt;Second, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and powerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical models (cf. especially Rasp and Thuerey &lt;span class="citation"&gt;[@rasp2020purely]&lt;/span&gt; already mentioned above). Unfortunately, models like that need to be trained on &lt;em&gt;a lot&lt;/em&gt; of data.&lt;/p&gt;
&lt;p&gt;However, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for individuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!&lt;/p&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c7601d73f03ebbb921610b708b22b1ab</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="1667" height="923"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;```&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;MirroredStrategy&lt;/code&gt; can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on &lt;a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/"&gt;Training Imagenet in 18 Minutes&lt;/a&gt;). So where do we go from here?&lt;/p&gt;
&lt;p&gt;Welcome to &lt;code&gt;MultiWorkerMirroredStrategy&lt;/code&gt;: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a &lt;code&gt;TF_CONFIG&lt;/code&gt; environment variable with the right addresses and run the exact same code in each compute instance.&lt;/p&gt;
&lt;p&gt;Please note that &lt;code&gt;partition&lt;/code&gt; must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, &lt;code&gt;data&lt;/code&gt; should point to a different partition of ImageNet, which we can retrieve with &lt;code&gt;pins&lt;/code&gt;; although, for convenience, &lt;code&gt;alexnet&lt;/code&gt; contains similar code under &lt;code&gt;alexnet::imagenet_partition()&lt;/code&gt;. Other than that, the code that you need to run in each compute instance is exactly the same.&lt;/p&gt;
&lt;p&gt;However, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/#barrier-execution"&gt;barrier execution&lt;/a&gt;. If you are new to Spark, there are many resources available at &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;. To learn just about running Spark and TensorFlow together, watch our &lt;a href="https://www.youtube.com/watch?v=Zm20P3ADa14"&gt;Deep Learning with Spark, TensorFlow and R&lt;/a&gt; video.&lt;/p&gt;
&lt;p&gt;Putting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:&lt;/p&gt;
&lt;p&gt;We hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!&lt;/p&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">73f5c8c15bde1f040b537394d5db0936</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop_vae(ds_train)&lt;/p&gt;
&lt;p&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next() encoded &amp;lt;- encoder(test_batch[[1]][1:1000]) test_var &amp;lt;- tf&lt;span class="math inline"&gt;\(math\)&lt;/span&gt;reduce_variance(encoded, axis = 0L) print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5)) } ```&lt;/p&gt;
&lt;div id="experimental-setup-and-data" class="section level2"&gt;
&lt;h2&gt;Experimental setup and data&lt;/h2&gt;
&lt;p&gt;The idea was to add white noise to a deterministic series. This time, the &lt;a href="https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor"&gt;Roessler system&lt;/a&gt; was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler.png" alt="Roessler attractor, two-dimensional projections." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Roessler attractor, two-dimensional projections.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like we did for the Lorenz system in the first part of this series, we use &lt;code&gt;deSolve&lt;/code&gt; to generate data from the Roessler equations.&lt;/p&gt;
&lt;p&gt;Then, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.&lt;/p&gt;
&lt;p&gt;Here you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler_noise.png" alt="Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Otherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="results" class="section level2"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an &lt;code&gt;fnn_multiplier&lt;/code&gt; of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.&lt;/p&gt;
&lt;p&gt;As a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In all cases&lt;/em&gt; here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.&lt;/p&gt;
&lt;div id="low-noise" class="section level4"&gt;
&lt;h4&gt;Low noise&lt;/h4&gt;
&lt;p&gt;Seeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (&lt;code&gt;x&lt;/code&gt;, 120 steps) and output (&lt;code&gt;y&lt;/code&gt;, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?&lt;/p&gt;
&lt;p&gt;Looking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we start to add noise?&lt;/p&gt;
&lt;/div&gt;
&lt;div id="substantial-noise" class="section level4"&gt;
&lt;h4&gt;Substantial noise&lt;/h4&gt;
&lt;p&gt;Between noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.&lt;/p&gt;
&lt;p&gt;Here first are predictions obtained from the unregularized models.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were &lt;em&gt;trained&lt;/em&gt; on the noisy version; predict fluctuations is what they learned.&lt;/p&gt;
&lt;p&gt;Do we see the same with the FNN models?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-9)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.&lt;/p&gt;
&lt;p&gt;“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.&lt;/p&gt;
&lt;p&gt;However, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.&lt;/p&gt;
&lt;p&gt;In the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, &lt;em&gt;MSEs have been normalized as fractions of the maximum MSE in a category&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we want to predict &lt;em&gt;signal plus noise&lt;/em&gt; (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/mses.png" alt="Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right)." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="summing-up" class="section level2"&gt;
&lt;h2&gt;Summing up&lt;/h2&gt;
&lt;p&gt;Our experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.&lt;/p&gt;
&lt;p&gt;With that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">931e811b30064bd6be23eefda987ed92</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop(ds_train)&lt;br /&gt;
}&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;After two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.&lt;/p&gt;
&lt;div id="obtaining-the-attractor-from-the-test-set" class="section level3"&gt;
&lt;h3&gt;Obtaining the attractor from the test set&lt;/h3&gt;
&lt;p&gt;We use the test set to inspect the latent code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,242 x 10
      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 
 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 
 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 
 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 
 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127
 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 
 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 
 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 
 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 
10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 
# … with 6,232 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_weight&lt;/code&gt; of 10, we do see a drop after the first two units:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 10
      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Here, this results in three projections of the set &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/predicted_attractors.png" alt="Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="wrapping-up-for-this-time" class="section level2"&gt;
&lt;h2&gt;Wrapping up (for this time)&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom &lt;em&gt;false nearest neighbors&lt;/em&gt; loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.&lt;/p&gt;
&lt;p&gt;This is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again &lt;em&gt;of course&lt;/em&gt;, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;? There is a lot to explore, stay tuned – and as always, thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;As per author recommendation (personal communication).&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See &lt;span class="citation"&gt;[@Kantz]&lt;/span&gt; for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">5503b1da2d7660fbcd2bc810fb123a30</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="2250" height="1140"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="2378" height="1563"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
