<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 17 Aug 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop(ds_train)&lt;/p&gt;
&lt;p&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next() encoded &amp;lt;- encoder(test_batch[[1]]) test_var &amp;lt;- tf&lt;span class="math inline"&gt;\(math\)&lt;/span&gt;reduce_variance(encoded, axis = 0L) print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5)) }&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
On to what we&amp;#39;ll use as a baseline for comparison.

#### Vanilla LSTM

Here is the vanilla LSTM, stacking two layers, each, again, of size 32. Dropout and recurrent dropout were chosen individually
per dataset, as was the learning rate.



### Data preparation

For all experiments, data were prepared in the same way.

In every case, we used the first 10000 measurements available in the respective `.pkl` files [provided by Gilpin in his GitHub
repository](https://github.com/williamgilpin/fnn/tree/master/datasets). To save on file size and not depend on an external
data source, we extracted those first 10000 entries to `.csv` files downloadable directly from this blog&amp;#39;s repo:



Should you want to access the complete time series (of considerably greater lengths), just download them from Gilpin&amp;#39;s repo
and load them using `reticulate`:



Here is the data preparation code for the first dataset, `geyser` - all other datasets were treated the same way.



Now we&amp;#39;re ready to look at how forecasting goes on our four datasets.

## Experiments

### Geyser dataset

People working with time series may have heard of [Old Faithful](https://en.wikipedia.org/wiki/Old_Faithful), a geyser in
Wyoming, US that has continually been erupting every 44 minutes to two hours since the year 2004. For the subset of data
Gilpin extracted[^3],

[^3]: see dataset descriptions in the [repository\&amp;#39;s README](https://github.com/williamgilpin/fnn)

&amp;gt; `geyser_train_test.pkl` corresponds to detrended temperature readings from the main runoff pool of the Old Faithful geyser
&amp;gt; in Yellowstone National Park, downloaded from the [GeyserTimes database](https://geysertimes.org/). Temperature measurements
&amp;gt; start on April 13, 2015 and occur in one-minute increments.

Like we said above, `geyser.csv` is a subset of these measurements, comprising the first 10000 data points. To choose an
adequate timestep for the LSTMs, we inspect the series at various resolutions:

&amp;lt;div class=&amp;quot;figure&amp;quot;&amp;gt;
&amp;lt;img src=&amp;quot;images/geyser_ts.png&amp;quot; alt=&amp;quot;Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.&amp;quot; width=&amp;quot;600&amp;quot; /&amp;gt;
&amp;lt;p class=&amp;quot;caption&amp;quot;&amp;gt;(\#fig:unnamed-chunk-5)Geyer dataset. Top: First 1000 observations. Bottom: Zooming in on the first 200.&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;

It seems like the behavior is periodic with a period of about 40-50; a timestep of 60 thus seemed like a good try.

Having trained both FNN-LSTM and the vanilla LSTM for 200 epochs, we first inspect the variances of the latent variables on
the test set. The value of `fnn_multiplier` corresponding to this run was `0.7`.



```{}
   V1     V2        V3          V4       V5       V6       V7       V8       V9      V10
0.258 0.0262 0.0000627 0.000000600 0.000533 0.000362 0.000238 0.000121 0.000518 0.000365&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a drop in importance between the first two variables and the rest; however, unlike in the Lorenz system, &lt;code&gt;V1&lt;/code&gt; and &lt;code&gt;V2&lt;/code&gt; variances also differ by an order of magnitude.&lt;/p&gt;
&lt;p&gt;Now, it’s interesting to compare prediction errors for both models. We are going to make an observation that will carry through to all three datasets to come.&lt;/p&gt;
&lt;p&gt;Keeping up the suspense for a while, here is the code used to compute per-timestep prediction errors from both models. The same code will be used for all other datasets.&lt;/p&gt;
&lt;p&gt;And here is the actual comparison. One thing especially jumps to the eye: FNN-LSTM forecast error is significantly lower for initial timesteps, first and foremost, for the very first prediction, which from this graph we expect to be pretty good!&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see “jumps” in prediction error, for FNN-LSTM, between the very first forecast and the second, and then between the second and the ensuing ones, reminding of the similar jumps in variable importance for the latent code! After the first ten timesteps, vanilla LSTM has caught up with FNN-LSTM, and we won’t interpret further development of the losses based on just a single run’s output.&lt;/p&gt;
&lt;p&gt;Instead, let’s inspect actual predictions. We randomly pick sequences from the test set, and ask both FNN-LSTM and vanilla LSTM for a forecast. The same procedure will be followed for the other datasets.&lt;/p&gt;
&lt;p&gt;Here are sixteen random picks of predictions on the test set. The ground truth is displayed in pink; blue forecasts are from FNN-LSTM, green ones from vanilla LSTM.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/geyser_preds.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What we expect from the error inspection comes true: FNN-LSTM yields significantly better predictions for immediate continuations of a given sequence.&lt;/p&gt;
&lt;p&gt;Let’s move on to the second dataset on our list.&lt;/p&gt;
&lt;div id="electricity-dataset" class="section level3"&gt;
&lt;h3&gt;Electricity dataset&lt;/h3&gt;
&lt;p&gt;This is a dataset on power consumption, aggregated over 321 different households and fifteen-minute-intervals.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;electricity_train_test.pkl&lt;/code&gt; corresponds to average power consumption by 321 Portuguese households between 2012 and 2014, in units of kilowatts consumed in fifteen minute increments. This dataset is from the &lt;a href="http://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"&gt;UCI machine learning database&lt;/a&gt;. &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, we see a very regular pattern:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_ts.png" alt="Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-11)Electricity dataset. Top: First 2000 observations. Bottom: Zooming in on 500 observations, skipping the very beginning of the series.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With such regular behavior, we immediately tried to predict a higher number of timesteps (&lt;code&gt;120&lt;/code&gt;) – and didn’t have to retract behind that aspiration.&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;0.5&lt;/code&gt;, latent variable variances look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;V1          V2            V3       V4       V5            V6       V7         V8      V9     V10
0.390 0.000637 0.00000000288 1.48e-10 2.10e-11 0.00000000119 6.61e-11 0.00000115 1.11e-4 1.40e-4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We definitely see a sharp drop already after the first variable.&lt;/p&gt;
&lt;p&gt;How do prediction errors compare on the two architectures?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-12)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here, FNN-LSTM performs better over a long range of timesteps, but again, the difference is most visible for immediate predictions. Will an inspection of actual predictions confirm this view?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/electricity_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-13)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It does! In fact, forecasts from FNN-LSTM are very impressive on all time scales.&lt;/p&gt;
&lt;p&gt;Now that we’ve seen the easy and predictable, let’s approach the weird and difficult.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ecg-dataset" class="section level3"&gt;
&lt;h3&gt;ECG dataset&lt;/h3&gt;
&lt;p&gt;Says Gilpin,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;ecg_train.pkl&lt;/code&gt; and &lt;code&gt;ecg_test.pkl&lt;/code&gt; correspond to ECG measurements for two different patients, taken from the &lt;a href="https://physionet.org/content/qtdb/1.0.0/"&gt;PhysioNet QT database&lt;/a&gt;. &lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How do these look?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_ts.png" alt="ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-14)ECG dataset. Top: First 1000 observations. Bottom: Zooming in on the first 400 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To the layperson that I am, these do not look nearly as regular as expected. First experiments showed that both architectures are not capable of dealing with a high number of timesteps. In every try, FNN-LSTM performed better for the very first timestep.&lt;/p&gt;
&lt;p&gt;This is also the case for &lt;code&gt;n_timesteps = 12&lt;/code&gt;, the final try (after &lt;code&gt;120&lt;/code&gt;, &lt;code&gt;60&lt;/code&gt; and &lt;code&gt;30&lt;/code&gt;). With an &lt;code&gt;fnn_multiplier&lt;/code&gt; of &lt;code&gt;1&lt;/code&gt;, the latent variances obtained amounted to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     V1        V2          V3        V4         V5       V6       V7         V8         V9       V10
  0.110  1.16e-11     3.78e-9 0.0000992    9.63e-9  4.65e-5  1.21e-4    9.91e-9    3.81e-9   2.71e-8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There &lt;em&gt;is&lt;/em&gt; a gap between the first variable and all other ones; but not much variance is explained by &lt;code&gt;V1&lt;/code&gt; either.&lt;/p&gt;
&lt;p&gt;Apart from the very first prediction, vanilla LSTM shows lower forecast errors this time; however, we have to add that this was not consistently observed when experimenting with other timestep settings.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-15)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Looking at actual predictions, both architectures perform best when a persistence forecast is adequate – in fact, they produce one even when it is &lt;em&gt;not&lt;/em&gt;.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/ecg_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-16)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On this dataset, we certainly would want to explore other architectures better able to capture the presence of high &lt;em&gt;and&lt;/em&gt; low frequencies in the data, such as mixture models. But – were we forced to stay with one of these, and could do a one-step-ahead, rolling forecast, we’d go with FNN-LSTM.&lt;/p&gt;
&lt;p&gt;Speaking of mixed frequencies – we haven’t seen the extremes yet …&lt;/p&gt;
&lt;/div&gt;
&lt;div id="mouse-dataset" class="section level3"&gt;
&lt;h3&gt;Mouse dataset&lt;/h3&gt;
&lt;p&gt;“Mouse”, that’s spike rates recorded from a mouse thalamus.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;mouse.pkl&lt;/code&gt; A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from &lt;a href="http://crcns.org/data-sets/thalamus/th-1/about-th-1"&gt;CRCNS&lt;/a&gt; and processed with the authors' code in order to generate a spike rate time series. &lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_ts.png" alt="Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-17)Mouse dataset. Top: First 2000 observations. Bottom: Zooming in on the first 500 observations.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Obviously, this dataset will be very hard to predict. How, after “long” silence, do you know that a neuron is going to fire?&lt;/p&gt;
&lt;p&gt;As usual, we inspect latent code variances (&lt;code&gt;fnn_multiplier&lt;/code&gt; was set to &lt;code&gt;0.4&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;Again, we don’t see the first variable explaining much variance. Still, interestingly, when inspecting forecast errors we get a picture very similar to the one obtained on our first, &lt;code&gt;geyser&lt;/code&gt;, dataset:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_mses.png" alt="Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-19)Per-timestep prediction error as obtained by FNN-LSTM and a vanilla stacked LSTM. Green: LSTM. Blue: FNN-LSTM.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So here, the latent code definitely seems to help! With every timestep “more” that we try to predict, prediction performance goes down &lt;em&gt;continuously&lt;/em&gt; – or put the other way round, short-time predictions are expected to be pretty good!&lt;/p&gt;
&lt;p&gt;Let’s see:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-20-fnn-lstm/images/mouse_predictions.png" alt="60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-20)60-step ahead predictions from FNN-LSTM (blue) and vanilla LSTM (green) on randomly selected sequences from the test set. Pink: the ground truth.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In fact on this dataset, the difference in behavior between both architectures is striking. When nothing is “supposed to happen”, vanilla LSTM produces “flat” curves at about the mean of the data, while FNN-LSTM takes the effort to “stay on track” as long as possible before also converging to the mean. Choosing FNN-LSTM – had we to choose one of these two – would be an obvious decision with this dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="discussion" class="section level2"&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;When, in timeseries forecasting, would we consider FNN-LSTM? Judging by the above experiments, conducted on four very different datasets: Whenever we consider a deep learning approach. Of course, this has been a casual exploration – and it was meant to be, as – hopefully – was evident from the nonchalant and bloomy (sometimes) writing style.&lt;/p&gt;
&lt;p&gt;Throughout the text, we’ve emphasized &lt;em&gt;utility&lt;/em&gt; – how could this technique be used to improve predictions? But, looking at the above results, a number of interesting questions come to mind. We already speculated (though in an indirect way) whether the number of high-variance variables in the latent code was relatable to how far we could sensibly forecast into the future. However, even more intriguing is the question of how characteristics of the &lt;em&gt;dataset itself&lt;/em&gt; affect FNN efficiency.&lt;/p&gt;
&lt;p&gt;Such characteristics could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How nonlinear is the dataset? (Put differently, how incompatible, as indicated by some form of test algorithm, is it with the hypothesis that the data generation mechanism was a linear one?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To what degree does the system appear to be sensitively dependent on initial conditions? In other words, what is the value of its (estimated, from the observations) highest &lt;a href="https://en.wikipedia.org/wiki/Lyapunov_exponent"&gt;Lyapunov exponent&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is its (estimated) dimensionality, for example, in terms of &lt;a href="https://en.wikipedia.org/wiki/Correlation_dimension"&gt;correlation dimension&lt;/a&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While it is easy to obtain those estimates, using, for instance, the &lt;a href="https://cran.r-project.org/web/packages/nonlinearTseries/"&gt;nonlinearTseries&lt;/a&gt; package explicitly modeled after practices described in Kantz &amp;amp; Schreiber’s classic &lt;span class="citation"&gt;[@Kantz]&lt;/span&gt;, we don’t want to extrapolate from our tiny sample of datasets, and leave such explorations and analyses to further posts, and/or the interested reader’s ventures :-). In any case, we hope you enjoyed the demonstration of practical usability of an approach that in the preceding post, was mainly introduced in terms of its conceptual attractivity.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;again, citing from Gilpin’s repository’s &lt;a href="https://github.com/williamgilpin/fnn"&gt;README&lt;/a&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">f032bca01115194a15e0d2845bde9625</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="2250" height="1140"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="2378" height="1563"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
