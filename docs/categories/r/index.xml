<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 21 Apr 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;Behold the glory that is &lt;a href="https://sparklyr.ai"&gt;sparklyr&lt;/a&gt; 1.2! In this release, the following new hotnesses have emerged into spotlight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;registerDoSpark&lt;/code&gt; method to create a &lt;a href="#foreach"&gt;foreach&lt;/a&gt; parallel backend powered by Spark that enables hundreds of existing R packages to run in Spark.&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="#databricks-connect"&gt;Databricks Connect&lt;/a&gt;, allowing &lt;code&gt;sparklyr&lt;/code&gt; to connect to remote Databricks clusters.&lt;/li&gt;
&lt;li&gt;Improved support for Spark &lt;a href="#structures"&gt;structures&lt;/a&gt; when collecting and querying their nested attributes with &lt;code&gt;dplyr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of inter-op issues observed with &lt;code&gt;sparklyr&lt;/code&gt; and Spark 3.0 preview were also addressed recently, in hope that by the time Spark 3.0 officially graces us with its presence, &lt;code&gt;sparklyr&lt;/code&gt; will be fully ready to work with it. Most notably, key features such as &lt;code&gt;spark_submit&lt;/code&gt;, &lt;code&gt;sdf_bind_rows&lt;/code&gt;, and standalone connections are now finally working with Spark 3.0 preview.&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;sparklyr&lt;/code&gt; 1.2 from CRAN run,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full list of changes are available in the sparklyr &lt;a href="https://github.com/sparklyr/sparklyr/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;div id="foreach" class="section level2"&gt;
&lt;h2&gt;Foreach&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; package provides the &lt;code&gt;%dopar%&lt;/code&gt; operator to iterate over elements in a collection in parallel. Using &lt;code&gt;sparklyr&lt;/code&gt; 1.2, you can now register Spark as a backend using &lt;code&gt;registerDoSpark()&lt;/code&gt; and then easily iterate over R objects using Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)
library(foreach)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4&amp;quot;)

registerDoSpark(sc)
foreach(i = 1:3, .combine = &amp;#39;c&amp;#39;) %dopar% {
  sqrt(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.000000 1.414214 1.732051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since many R packages are based on &lt;code&gt;foreach&lt;/code&gt; to perform parallel computation, we can now make use of all those great packages in Spark as well!&lt;/p&gt;
&lt;p&gt;For instance, we can use &lt;a href="https://tidymodels.github.io/parsnip/"&gt;parsnip&lt;/a&gt; and the &lt;a href="https://tidymodels.github.io/tune/"&gt;tune&lt;/a&gt; package with data from &lt;a href="https://CRAN.R-project.org/package=mlbench"&gt;mlbench&lt;/a&gt; to perform hyperparameter tuning in Spark with ease:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tune)
library(parsnip)
library(mlbench)

data(Ionosphere)
svm_rbf(cost = tune(), rbf_sigma = tune()) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;kernlab&amp;quot;) %&amp;gt;%
  tune_grid(Class ~ .,
    resamples = rsample::bootstraps(dplyr::select(Ionosphere, -V2), times = 30),
    control = control_grid(verbose = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Bootstrap sampling
# A tibble: 30 x 4
   splits            id          .metrics          .notes
 * &amp;lt;list&amp;gt;            &amp;lt;chr&amp;gt;       &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;
 1 &amp;lt;split [351/124]&amp;gt; Bootstrap01 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 2 &amp;lt;split [351/126]&amp;gt; Bootstrap02 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 3 &amp;lt;split [351/125]&amp;gt; Bootstrap03 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 4 &amp;lt;split [351/135]&amp;gt; Bootstrap04 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 5 &amp;lt;split [351/127]&amp;gt; Bootstrap05 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 6 &amp;lt;split [351/131]&amp;gt; Bootstrap06 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 7 &amp;lt;split [351/141]&amp;gt; Bootstrap07 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 8 &amp;lt;split [351/123]&amp;gt; Bootstrap08 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
 9 &amp;lt;split [351/118]&amp;gt; Bootstrap09 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
10 &amp;lt;split [351/136]&amp;gt; Bootstrap10 &amp;lt;tibble [10 × 5]&amp;gt; &amp;lt;tibble [0 × 1]&amp;gt;
# … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Spark connection was already registered, so the code ran in Spark without any additional changes. We can verify this was the case by navigating to the Spark web interface:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-backend-foreach-package.png" /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id="databricks-connect" class="section level2"&gt;
&lt;h2&gt;Databricks Connect&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://docs.databricks.com/dev-tools/databricks-connect.html"&gt;Databricks Connect&lt;/a&gt; allows you to connect your favorite IDE (like &lt;a href="https://rstudio.com/products/rstudio/download/"&gt;RStudio&lt;/a&gt;!) to a Spark &lt;a href="https://databricks.com/"&gt;Databricks&lt;/a&gt; cluster.&lt;/p&gt;
&lt;p&gt;You will first have to install the &lt;code&gt;databricks-connect&lt;/code&gt; package as described in our &lt;a href="https://github.com/sparklyr/sparklyr#connecting-through-databricks-connect"&gt;README&lt;/a&gt; and start a Databricks cluster, but once that’s ready, connecting to the remote cluster is as easy as running:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sc &amp;lt;- spark_connect(
  method = &amp;quot;databricks&amp;quot;,
  spark_home = system2(&amp;quot;databricks-connect&amp;quot;, &amp;quot;get-spark-home&amp;quot;, stdout = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-04-21-sparklyr-1.2.0-released/images/spark-databricks-connect-rstudio.png" /&gt;&lt;/p&gt;
&lt;p&gt;That’s about it, you are now remotely connected to a Databricks cluster from your local R session.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="structures" class="section level2"&gt;
&lt;h2&gt;Structures&lt;/h2&gt;
&lt;p&gt;If you previously used &lt;code&gt;collect&lt;/code&gt; to deserialize structurally complex Spark dataframes into their equivalents in R, you likely have noticed Spark SQL struct columns were only mapped into JSON strings in R, which was non-ideal. You might also have run into a much dreaded &lt;code&gt;java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt; error when using &lt;code&gt;dplyr&lt;/code&gt; to query nested attributes from any struct column of a Spark dataframe in sparklyr.&lt;/p&gt;
&lt;p&gt;Unfortunately, often times in real-world Spark use cases, data describing entities comprising of sub-entities (e.g., a product catalog of all hardware components of some computers) needs to be denormalized / shaped in an object-oriented manner in the form of Spark SQL structs to allow efficient read queries. When sparklyr had the limitations mentioned above, users often had to invent their own workarounds when querying Spark struct columns, which explained why there was a mass popular demand for sparklyr to have better support for such use cases.&lt;/p&gt;
&lt;p&gt;The good news is with &lt;code&gt;sparklyr&lt;/code&gt; 1.2.0, those limitations no longer exist any more when working running with Spark 2.4 or above.&lt;/p&gt;
&lt;p&gt;As a concrete example, consider the following catalog of computers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

computers &amp;lt;- tibble::tibble(
  id = seq(1, 2),
  attributes = list(
    list(
      processor = list(freq = 2.4, num_cores = 256),
      price = 100
   ),
   list(
     processor = list(freq = 1.6, num_cores = 512),
     price = 133
   )
  )
)

computers &amp;lt;- copy_to(sc, computers, overwrite = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A typical &lt;code&gt;dplyr&lt;/code&gt; use case involving &lt;code&gt;computers&lt;/code&gt; would be the following:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;high_freq_computers &amp;lt;- computers %&amp;gt;%
                       filter(attributes.processor.freq &amp;gt;= 2) %&amp;gt;%
                       collect()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As previously mentioned, before &lt;code&gt;sparklyr&lt;/code&gt; 1.2.0, such query would fail with &lt;code&gt;Error: java.lang.IllegalArgumentException: Invalid type list&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Whereas with &lt;code&gt;sparklyr&lt;/code&gt; 1.2.0, the expected result is returned in the following form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 2
     id attributes
  &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;
1     1 &amp;lt;named list [2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;high_freq_computers$attributes&lt;/code&gt; is what we would expect:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[[1]]$price
[1] 100

[[1]]$processor
[[1]]$processor$freq
[1] 2.4

[[1]]$processor$num_cores
[1] 256&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="and-more" class="section level2"&gt;
&lt;h2&gt;And More!&lt;/h2&gt;
&lt;p&gt;Last but not least, we heard about a number of pain points &lt;code&gt;sparklyr&lt;/code&gt; users have run into, and have addressed many of them in this release as well. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date type in R is now correctly serialized into Spark SQL date type by &lt;code&gt;copy_to&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;spark dataframe&amp;gt; %&amp;gt;% print(n = 20)&lt;/code&gt; now actually prints 20 rows as expected instead of 10&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spark_connect(master = "local")&lt;/code&gt; will emit a more informative error message if it’s failing because the loopback interface is not up&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;… to just name a few. We want to thank the open source community for their continuous feedback on &lt;code&gt;sparklyr&lt;/code&gt;, and are looking forward to incorporating more of that feedback to make &lt;code&gt;sparklyr&lt;/code&gt; even better in the future.&lt;/p&gt;
&lt;p&gt;Finally, in chronological order, we wish to thank the following individuals for contributing to &lt;code&gt;sparklyr&lt;/code&gt; 1.2.0: &lt;a href="https://github.com/zero323"&gt;zero323&lt;/a&gt;, &lt;a href="https://github.com/Loquats"&gt;Andy Zhang&lt;/a&gt;, &lt;a href="https://github.com/yl790"&gt;Yitao Li&lt;/a&gt;, &lt;a href="https://github.com/javierluraschi"&gt;Javier Luraschi&lt;/a&gt;, &lt;a href="https://github.com/falaki"&gt;Hossein Falaki&lt;/a&gt;, &lt;a href="https://github.com/lu-wang-dl"&gt;Lu Wang&lt;/a&gt;, and &lt;a href="https://github.com/samuelmacedo83"&gt;Samuel Macedo&lt;/a&gt;. Great job everyone!&lt;/p&gt;
&lt;p&gt;If you need to catch up on &lt;code&gt;sparklyr&lt;/code&gt;, please visit &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, or some of the previous release posts: &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/"&gt;sparklyr 1.1&lt;/a&gt; and &lt;a href="https://blog.rstudio.com/2019/03/15/sparklyr-1-0/"&gt;sparklyr 1.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for reading this post.&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">0b64678686a51ab5985c0e352bd0f2b7</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
