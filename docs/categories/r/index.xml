<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 17 Sep 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>AI ethics is not an optimization problem</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-18-ai-ethics-optimization-problem</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;Disclaimer: This post was written to reflect perspective and opinions of the author, not of my team or RStudio as a whole. All responsibility for the content is exclusively mine.&lt;/p&gt;
&lt;div id="why-this-post-why-now" class="section level2"&gt;
&lt;h2&gt;Why this post, why now&lt;/h2&gt;
&lt;p&gt;When you work in a field as intellectually-satisfying, challenging and inspiring as software design for machine learning, it is easy to focus on the technical, keeping out of sight the broader context. Some would even say it is required. How else can you keep up the necessary level of concentration?&lt;/p&gt;
&lt;p&gt;But even for someone who hasn’t been in the field that long, it is evident that with every year that passes, with deep-learning-based technologies&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; progressing faster and faster over ever-shorter time spans, misuse of these technologies has increased as well, not just in selected countries but all over the world.&lt;/p&gt;
&lt;p&gt;To eliminate one possible misunderstanding right from the start: When I’m talking about “faster and faster” progress, I’m not over-hyping things. I’m far from thinking that &lt;em&gt;AI&lt;/em&gt; is close to “solving” problems like language understanding, concept learning and their likes – the kind of problems some would argue hybrid models were needed for &lt;span class="citation"&gt;[@2020arXiv200206177M]&lt;/span&gt;. The thing is that it doesn’t matter. It is exactly the kinds of things &lt;em&gt;AI&lt;/em&gt; &lt;em&gt;does&lt;/em&gt; do so well that lend themselves to misuse. It is people we should be afraid of, not machines&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Back to the why. Over time, it increasingly appeared to me that writing regularly about &lt;em&gt;AI&lt;/em&gt; in a technical way, but &lt;em&gt;not ever&lt;/em&gt; writing about its misuse, was ethically questionable in itself. However, I also became increasingly conscious of the fact that with a topic like this, once you enter the political realm – and that we &lt;em&gt;have to&lt;/em&gt; is the main point of this text –, likelihood rises that people will disagree, or worse, feel offended for reasons not anticipated by the writer. Some will find this too radical, some not radical (&lt;em&gt;explicit&lt;/em&gt;) enough.&lt;/p&gt;
&lt;p&gt;But when the alternative is to stay silent, it seems better to try and do one’s best.&lt;/p&gt;
&lt;p&gt;Let’s start with two terms whose recent rise in popularity matches that of &lt;em&gt;AI&lt;/em&gt; as a whole: bias and fairness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="but-we-are-doing-a-lot-to-improve-fairness-and-remove-bias-arent-we" class="section level2"&gt;
&lt;h2&gt;But we &lt;em&gt;are&lt;/em&gt; doing a lot to improve fairness and remove bias, aren’t we?&lt;/h2&gt;
&lt;p&gt;Search for “algorithmic fairness”, “deep learning fairness” or something similar, and you’ll find lots of papers, tools, guidelines … more than you have time to read. And bias: Haven’t we all heard about image recognition models failing on black people, women, and black women in particular; about machine translation incorporating gender stereotypes; about search engine results reflecting racism and discrimination? Given enough media attention, the respective algorithms get “fixed”; what’s more, we may also safely assume that overall, researchers developing new models will probe for these exact kinds of failures. So as a community, we &lt;em&gt;do&lt;/em&gt; care about bias, don’t we?&lt;/p&gt;
&lt;p&gt;Re: fairness. It’s true that there are many attempts to increase algorithmic fairness. But as Ben Green &lt;span class="citation"&gt;[@AlgorithmicRealism]&lt;/span&gt;, who I’ll cite a lot in this text, points out, most of this work does so via rigid formalization, as if fairness – its &lt;a href="https://www.youtube.com/watch?v=jIXIuYdnyyk"&gt;conflicting definitions&lt;/a&gt; notwithstanding – were an objective quantity, a metric that could be optimized just like the metrics we usually optimize in machine learning.&lt;/p&gt;
&lt;p&gt;Assume we were willing to stay in the formalist frame. Even then there is no satisfying solution to this optimization problem. Take the perspective of &lt;em&gt;group fairness&lt;/em&gt;, where a fair algorithm is one that results in equal outcomes between groups. Chouldechova &lt;span class="citation"&gt;[@Chouldechova]&lt;/span&gt; then shows that when an algorithm achieves predictive parity (a.k.a. precision), but prevalence differs between groups, it is not possible to also have &lt;em&gt;both&lt;/em&gt; equal false positive &lt;em&gt;and&lt;/em&gt; equal false negative rates. – That said, here I mainly want to focus on why pure formalism is not enough.&lt;/p&gt;
&lt;p&gt;What with bias? That datasets can be (or rather: are) biased is hardly something anyone working in &lt;em&gt;AI&lt;/em&gt; would object to. What amount of bias is being admitted to in other components/stages of the machine learning process varies between people.&lt;/p&gt;
&lt;p&gt;Harini and Guttag &lt;span class="citation"&gt;[@2019arXiv190110002S]&lt;/span&gt; map sources of bias to the model development and deployment process. Forms of bias they distinguish include representation (think: dataset), measurement (think: metrics&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;), aggregation (think: model&lt;a href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;), and evaluation (similar to the two preceding ones, but at test time) bias. The paper is written in a technical style; in fact, there is even a diagram where the authors attempt to formalize the process in a mathematical way. (Personally, I find it hard to see what value is added by this diagram; its existence can probably best be explained by [perceived] requirements of the genre, namely, &lt;em&gt;research paper&lt;/em&gt;.)&lt;/p&gt;
&lt;p&gt;Now, so far I’ve left out the remaining two sources of bias they name. Mapped to the end of the process is deployment bias, when a system “is used or interpreted in inappropriate ways”. That raises a question. Is this about the &lt;em&gt;end&lt;/em&gt; of a process, or is what happens now a &lt;em&gt;completely different process&lt;/em&gt; (or: processes)? For an in-house data scientist, it may well be the end of a process; for a scientist, or for a consultant, it is not. Once a scientist has developed and published a model, they have no control over who uses it and how. From the collection of humankind’s empirical truths: Once a technology exists, and it is useful, it &lt;em&gt;will be used&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Things get further out of control at the other side of the timeline. Here we have historical bias:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Historical bias arises when there is a misalignment between the world as it is and the values or objectives to be encoded and propagated in a model. It is a normative concern with the state of the world, and exists even given perfect sampling and feature selection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’ll get back to why this is called “historical bias” later; the definition is a lot broader though. Now definitely we’re beyond the realm of formalization: We’re entering the realm of normativity, of ways of viewing the world, of ethics.&lt;/p&gt;
&lt;p&gt;Please don’t get me wrong. I’m not criticizing the paper; in fact, it provides a useful categorization that may be used as a “check list” by practitioners. But given the formalist style, a reader is likely to focus on the readily-formalizable parts; it is then easy to dismiss the two others as not really being relevant to one’s work, and certainly not something one could exert influence on. (I’ll get back to that later.)&lt;/p&gt;
&lt;p&gt;One little aside before we leave formalism: I do believe that a lot of the formalist work on &lt;em&gt;AI&lt;/em&gt; ethics is done in good intent; however, work in this area also benefits organizations that undertake it. Citing &lt;a href="https://tomslee.github.io/publication/oup_private_sector_ai/"&gt;Tom Slee&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Standards set public benchmarks and provide protection from future accusations. Auditable criteria incorporated into product development and release processes can confirm compliance. There are also financial incentives to adopt a technical approach: standards that demand expertise and investment create barriers to entry by smaller firms, just as risk management regulations create barriers to entry in the financial and healthcare industries.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id="not-everything-in-life-is-an-optimization-problem" class="section level2"&gt;
&lt;h2&gt;Not everything in life is an optimization problem&lt;/h2&gt;
&lt;p&gt;Stephen Boyd, who teaches convex optimization at Stanford, is said to often start the introductory lecture with the phrase “everything is an optimization problem”. &lt;a href="#fn5" class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; It sounds intriguing at first; certainly a lot of things in my life can be thought of like that. It may become awkward once you start to compare, say, time spent with loved ones and time spent working; it becomes completely unfeasible when comparing &lt;em&gt;across people&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We saw that even under formalist thinking, there is no impartial way to optimize for fairness. But it’s not just about choosing between different types of fairness. How do you weigh fairness against stakeholder interests? No algorithm will be deployed that does not serve an organization’s purpose.&lt;/p&gt;
&lt;p&gt;There is thus an intimate link between &lt;em&gt;metrics&lt;/em&gt;, &lt;em&gt;objectives&lt;/em&gt; and &lt;em&gt;power&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="metrics-and-power" class="section level2"&gt;
&lt;h2&gt;Metrics and power&lt;/h2&gt;
&lt;p&gt;Even though terminologically, in deep learning, we distinguish between optimization and metrics, the metrics really are what we are optimizing for &lt;span class="citation"&gt;[@2020arXiv200208512T]&lt;/span&gt;: Goodhart’s law – &lt;em&gt;When a measure becomes a target, it ceases to be a good measure&lt;/em&gt;&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; – does not seem to apply. Still, they deserve the same questioning and inspection as metrics in other contexts.&lt;/p&gt;
&lt;p&gt;In AI as elsewhere, optimization objectives are proxies; they “stand in” for the thing we’re really interested in. That proxying process could fail in many ways &lt;span class="citation"&gt;[@2018arXiv180304585M]&lt;/span&gt;, but failure is not the only applicable category to think in here.&lt;/p&gt;
&lt;p&gt;For one, objectives are chosen according to the dominant paradigm. Dotan and Milli &lt;span class="citation"&gt;[@dotan2019valueladen]&lt;/span&gt; show how a technology’s perceived success feeds back into the criteria used to evaluate other technologies (as well as future instances of itself). Imagine a world where models were ranked not just for classification accuracy, but also for, say, climate friendliness, robustness to adversarial attacks, or reliable quantification of uncertainty.&lt;/p&gt;
&lt;p&gt;Objectives thus do not emerge out of nothing. That they reflect paradigms may still sound abstract; that they serve existing power structures less so. Power structures are complex; they reflect more than just who “has the say”. As pointed out in various “classics” of critical race theory, intersectionalist feminism and related areas&lt;a href="#fn7" class="footnote-ref" id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;, we should ask ourselves: &lt;em&gt;Who profits?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is an all but simple question, comparable in difficulty, it seems to me, to the request that we question the unspoken premises that underlie our theories. The main point about such premises is that we aren’t aware of them: If we were, we could have stated them explicitly. Similarly, if I noticed I was profiting from someone, I would – hopefully – take some action. Hard as the task may be, though, we have to do our best.&lt;/p&gt;
&lt;p&gt;If it’s hard to see how one is privileged, can’t we just be neutral? Objective? Isn’t this getting too &lt;em&gt;political&lt;/em&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id="there-is-no-objectivity-and-all-is-politics" class="section level2"&gt;
&lt;h2&gt;There is no objectivity, and all is politics&lt;/h2&gt;
&lt;p&gt;To some people, the assertion that objectivity cannot exist is too self-evident to require much dwelling on. Are numbers objective? Maybe in some formal way; but once I use them &lt;em&gt;in communication&lt;/em&gt;, they convey a message, independently of my intention. Let’s say I want to convey the fact, taken from &lt;a href="https://www.ipcc.ch/sr15/chapter/spm/"&gt;the IPCC website&lt;/a&gt;, that between 2030 and 2052, global warming is likely to reach 1.5°C. Let’s also assume that I look up the pre-industrial average for the place where I happen to live (15°C, say), and that I want to show off my impressive meteorological knowledge. Thus I say, “… so here, guys, temperature will rise from 288.15 Kelvin to 289.65 Kelvin, on average”. This surely is an objective statement. But what if the people I’m talking to don’t know that – even though the absolute values, when expressed in Kelvin, are so much higher than when expressed in degrees Celsius – the relative differences are the same? They might get the impression, totally unintended, that not much warming is going to happen.&lt;/p&gt;
&lt;p&gt;If even numbers, once used in communication, lose their objectivity, this must hold even more for anything that involves more design choices: visualizations, APIs, and, of course, written text. For visualizations, this is nicely illustrated in d’Ignazio and Klein’s Data Feminism &lt;span class="citation"&gt;[@DataFeminism]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;That book is also an exemplary exercise in what appears like the only way to “deal with” the fact that objectivity is impossible: Trying to be as clear as possible about where we stand, who we are, what are the circumstances that have influenced our way of thinking about something. Of course, like with the unspoken premises and assumptions discussed above, this is not easy; in fact, it’s impossible to do in perfection. But one can try.&lt;/p&gt;
&lt;p&gt;In fact, the above “Trying to be as clear as possible …” is deliberately ambiguous. It refers to two things: For one, to &lt;em&gt;me&lt;/em&gt; striving to analyze how I’m privileged, and secondly, to me giving information to &lt;em&gt;others&lt;/em&gt;. The first alone is laudable but necessarily limited; the second, as exercised by d’Ignazio and Klein, opens the door not just for better mutual understanding, but also, for feedback and learning. The person I’m talking to might lead me to insights I wouldn’t have gotten otherwise.&lt;/p&gt;
&lt;p&gt;Putting things slightly differently, there is no objectivity because there’s always a context. Focus on metrics and formalization detract from that context. Green &lt;span class="citation"&gt;[@AlgorithmicRealism]&lt;/span&gt; relates an interesting parallel from American law history. Until the early twentieth century, US law was dominated by a formalist ethos. Ideally, all rules should be traceable to a small number of universal principles, derived from natural rights. Autonomy being such a right, in a famous 1905 case, the U.S. Supreme Court concluded that a law limiting the working hours of employees represented “unreasonable, unnecessary and arbitrary interference with the right and liberty of the individual to contract”. However,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In his dissent, Justice Oliver Wendell Holmes argued that the Court failed to consider the context of the case, noting, “General propositions do not decide concrete cases”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus, the &lt;em&gt;context&lt;/em&gt; matters. Autonomy is good; but it can’t be used as an excuse for exploitation.&lt;/p&gt;
&lt;p&gt;The same context dependence holds in &lt;em&gt;AI&lt;/em&gt;. It is always developed and deployed in a context, - a context shaped by &lt;em&gt;history&lt;/em&gt;. History determines what datasets we work with; what we optimize for; &lt;em&gt;who&lt;/em&gt; tells us what to optimize for. An daunting example is so-called “predictive policing”. Datasets used to train prediction algorithms incorporate a history of racial injustice: The very definition of “crime” they rely on was shaped by racist and classist practice&lt;span class="citation"&gt;[@AlgorithmicRealism]&lt;/span&gt;. The new method then perpetuates – more than that: exacerbates – the current system, creating a vicious cycle of positive feedback that makes it look like the algorithm was successful.&lt;/p&gt;
&lt;p&gt;Summing up: When there is no neutrality, everything is politics. Not acting is acting. Citing Green&lt;span class="citation"&gt;[@Green2019GoodIG]&lt;/span&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But efforts for reform are no more political than efforts to resist reform or even the choice simply to not act, both of which preserve existing systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id="but-im-just-an-engineer" class="section level2"&gt;
&lt;h2&gt;But I’m just an engineer&lt;/h2&gt;
&lt;p&gt;When machine learning people, or computer science people in general, are asked about their views on the societal impact of modern &lt;em&gt;AI&lt;/em&gt;, an often-heard answer is: “But I’m just an engineer…”. This is completely understandable. Most of us are just tiny cogs in those big machines, wired together in a complex network, that run the world. We’re not in control; how could we be accountable?&lt;/p&gt;
&lt;p&gt;For the &lt;em&gt;AI&lt;/em&gt; scientist, though normally all but sitting in an “ivory tower”, it’s nevertheless the &lt;em&gt;ML engineers&lt;/em&gt; who are responsible of how a model gets deployed, and to what consequences&lt;a href="#fn8" class="footnote-ref" id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. The ML engineer may delegate to the head of IT, who in turn had no choice but implement what was requested “by business”. And so on and so forth, ad infinitum.&lt;/p&gt;
&lt;p&gt;That said, it is hard to come up with a moral imperative here. In the line of thinking exercised above: There is always a context. In some parts of the world, you have more choices than in others. Options vary based on race, gender, abledness, and more. Maybe you can just quit and get another job; maybe you can’t.&lt;/p&gt;
&lt;p&gt;There is another – related – point though, on which I’d like to dwell a little longer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="technology-optimism" class="section level2"&gt;
&lt;h2&gt;Technology optimism&lt;/h2&gt;
&lt;p&gt;Sometimes, it’s not that the people working in AI are fully aware of, but don’t see how to counter, the harm that is being done in their field. On the contrary. They are convinced that they’re doing good. Just do a quick search for “AI for good”, and you’ll be presented with a great number of projects and initiatives. But how, actually, is decided what &lt;em&gt;is&lt;/em&gt; good? Who decides? Who profits?&lt;/p&gt;
&lt;p&gt;Ben Green, again, relates an instructive example &lt;span class="citation"&gt;[@AlgorithmicRealism]&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;USC’s Center for Artificial Intelligence in Society (CAIS) is emblematic of how computer science projects labeled as promoting “social good” can cause harm by wading into hotly contested political territory with a regressive perspective. One of the group’s projects involved deploying game theory and machine learning to predict and prevent behavior from “adversarial groups.” Although CAIS motivated the project by discussing “extremist organizations such as ISIS and Jabhat al-Nusra,” it quickly slipped into focusing on “criminal street gangs” [43]. In fact, the project’s only publication was a controversial paper that used neural networks to classify crimes in Los Angeles as gang-related [28, 41].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Predictive policing, already mentioned above, can also be seen in this category. At first thought, isn’t it a good thing? Wouldn’t it be nice if we could make our world a bit more secure?&lt;/p&gt;
&lt;p&gt;Phillip Rogaway, who I’ll mention again in the concluding section, talks a lot about how technology optimism dominates among his students &lt;span class="citation"&gt;[@Rogaway]&lt;/span&gt;; he seems to be as intrigued by it as I am. Personally, I think that whether someone “intrinsically” tends to be a technology optimist or a pessimist is a persistent trait; it seems to be a matter of personality and socialization (or just call it &lt;em&gt;fate&lt;/em&gt;: I don’t want to go into any nature-nurture debates here). That glass, is it half full or half empty?&lt;/p&gt;
&lt;p&gt;All I could say to a hardcore technology optimist is that their utopia may be another person’s dystopia. Especially if that other person is poor, or black, or poor and a black woman … and so on.&lt;/p&gt;
&lt;p&gt;Let me just end this section with a citation from a &lt;a href="https://ali-alkhatib.com/blog/anthropological-intelligence"&gt;blog post&lt;/a&gt; by Ali Alkhatib centered around the launch of the &lt;a href="https://hai.stanford.edu/"&gt;Stanford institute for human-centered artificial intelligence (HAI)&lt;/a&gt;. Referring to the &lt;a href="https://hai.stanford.edu/blog/smart-interfaces-human-centered-ai"&gt;director’s accompanying blog post&lt;/a&gt;, he writes&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;James opens with a story of an office that senses you slouching, registers that you’re fatigued, intuits that your mood has shifted, and alters the ambiance accordingly to keep you alert throughout the day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can head to Alkhatib’s post (very worth reading) and read the continuation, expanding on the scenario. But for some people, this single sentence may already be enough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="and-now" class="section level2"&gt;
&lt;h2&gt;And now?&lt;/h2&gt;
&lt;p&gt;At some point, an author is expected to wrap up and present ideas for improvement. With most of the topics touched upon here, this is yet another intimidating task. I’ll give it a try anyway. The best synthesis I can come up with at the moment looks about like this.&lt;/p&gt;
&lt;p&gt;First, some approaches filed by Green under “formalist” can still make for a good start, or rather, can constitute a set of default measures, to be taken routinely. Most prominently, these include dataset &lt;span class="citation"&gt;[@2018arXiv180309010G]&lt;/span&gt; and model &lt;span class="citation"&gt;[@2018arXiv181003993M]&lt;/span&gt; documentation.&lt;/p&gt;
&lt;p&gt;Beyond the technical, I like the advice given in Baumer and Silberman’s &lt;em&gt;When the Implication is Not to Design&lt;/em&gt; &lt;span class="citation"&gt;[@Baumer]&lt;/span&gt;. If the consequences, especially on socially marginalized groups, of a technological approach are unforeseeable, think whether the problem can be solved in a “low-tech” way. By all means, do &lt;em&gt;not&lt;/em&gt; start with the solution and then, go find a problem.&lt;/p&gt;
&lt;p&gt;In some cases, even that may not be enough. With some goals, don’t look for alternative ways to achieve them. Sometimes the goal itself has to be questioned.&lt;/p&gt;
&lt;p&gt;The same can be said for the other direction. With some technologies, there is no goal that could justify their application. This is because such a technology is &lt;em&gt;certain&lt;/em&gt; to get misused. Facial recognition is one example.&lt;/p&gt;
&lt;p&gt;Lastly, let me finish on a speculative note. Rogaway, already referred to above for his comments on technology optimism, calls on his colleagues, fellow cryptographers, to devise protocols in such a way that private communication stays private, that breaches are, in plain terms, impossible&lt;span class="citation"&gt;[@Rogaway]&lt;/span&gt;. While I personally can’t think see how to port the analogy to &lt;em&gt;AI&lt;/em&gt;, maybe others will be able to, drawing inspiration from his text. Until then, changes in politics and legislation seem to be the only recourse.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;For brevity, I’ll be subsuming deep learning and other contemporary machine learning methods under &lt;em&gt;AI&lt;/em&gt;, following common(-ish) usage.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;I can’t hope to express this better than Maciej Cegłowski did &lt;a href="https://idlewords.com/talks/superintelligence.htm"&gt;here&lt;/a&gt;, so I won’t elaborate on that topic any further.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;More on that below. Metrics used in machine learning mostly are proxies for things we really care about; there are lots of ways this can go wrong.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;Meaning, a one-model-fits-all approach puts some groups at a disadvantage.&lt;a href="#fnref4" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;At least he does so here: &lt;a href="https://www.youtube.com/watch?v=McLq1hEq3UY" class="uri"&gt;https://www.youtube.com/watch?v=McLq1hEq3UY&lt;/a&gt;&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;cited after &lt;span class="citation"&gt;[@2020arXiv200208512T]&lt;/span&gt;&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt;see e.g., &lt;span class="citation"&gt;[@DataFeminism]&lt;/span&gt; and &lt;span class="citation"&gt;[@RaceAfterTechnology]&lt;/span&gt;&lt;a href="#fnref7" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt;Of course, this is meant generically, not including every member of the set. But I prefer putting it like this instead of citing actual utterances, recently seen on Twitter.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">533e1b59af4e7db240bbcc0fd98f0932</distill:md5>
      <category>R</category>
      <category>Ethics and Societal Impact</category>
      <category>Meta</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-18-ai-ethics-optimization-problem</guid>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing sparklyr.flint: A time-series extension for sparklyr</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</link>
      <description>We are pleased to announce that sparklyr.flint, a sparklyr extension for analyzing time series at scale with Flint, is now available on CRAN. Flint is an open-source library for working with time-series in Apache Spark which supports aggregates and joins on time-series datasets.</description>
      <category>R</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</guid>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint/images/thumb.png" medium="image" type="image/png" width="126" height="77"/>
    </item>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the "black-box end" of the continuum.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="600" height="332"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="400" height="203"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="600" height="394"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
  </channel>
</rss>
