<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 31 Aug 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;Same with weekly climatology: Looking back at how warm it was, at a given location, that same week two years ago, does not in general sound like a bad strategy.&lt;/p&gt;
&lt;p&gt;Second, the DL baseline shown is as basic as it can get, architecture- as well as parameter-wise. More sophisticated and powerful architectures have been developed that not just by far surpass the baselines, but can even compete with physical models (cf. especially Rasp and Thuerey &lt;span class="citation"&gt;[@rasp2020purely]&lt;/span&gt; already mentioned above). Unfortunately, models like that need to be trained on &lt;em&gt;a lot&lt;/em&gt; of data.&lt;/p&gt;
&lt;p&gt;However, other weather-related applications (other than medium-range forecasting, that is) may be more in reach for individuals interested in the topic. For those, we hope we have given a useful introduction. Thanks for reading!&lt;/p&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">c7601d73f03ebbb921610b708b22b1ab</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="1667" height="923"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;```&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;MirroredStrategy&lt;/code&gt; can help us scale up to about 8 GPUs per compute instance; however, we are likely to need 16 instances with 8 GPUs each to train ImageNet in a reasonable time (see Jeremy Howard’s post on &lt;a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/"&gt;Training Imagenet in 18 Minutes&lt;/a&gt;). So where do we go from here?&lt;/p&gt;
&lt;p&gt;Welcome to &lt;code&gt;MultiWorkerMirroredStrategy&lt;/code&gt;: This strategy can use not only multiple GPUs, but also multiple GPUs across multiple computers. To configure them, all we have to do is define a &lt;code&gt;TF_CONFIG&lt;/code&gt; environment variable with the right addresses and run the exact same code in each compute instance.&lt;/p&gt;
&lt;p&gt;Please note that &lt;code&gt;partition&lt;/code&gt; must change for each compute instance to uniquely identify it, and that the IP addresses also need to be adjusted. In addition, &lt;code&gt;data&lt;/code&gt; should point to a different partition of ImageNet, which we can retrieve with &lt;code&gt;pins&lt;/code&gt;; although, for convenience, &lt;code&gt;alexnet&lt;/code&gt; contains similar code under &lt;code&gt;alexnet::imagenet_partition()&lt;/code&gt;. Other than that, the code that you need to run in each compute instance is exactly the same.&lt;/p&gt;
&lt;p&gt;However, if we were to use 16 machines with 8 GPUs each to train ImageNet, it would be quite time-consuming and error-prone to manually run code in each R session. So instead, we should think of making use of cluster-computing frameworks, like Apache Spark with &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/#barrier-execution"&gt;barrier execution&lt;/a&gt;. If you are new to Spark, there are many resources available at &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;. To learn just about running Spark and TensorFlow together, watch our &lt;a href="https://www.youtube.com/watch?v=Zm20P3ADa14"&gt;Deep Learning with Spark, TensorFlow and R&lt;/a&gt; video.&lt;/p&gt;
&lt;p&gt;Putting it all together, training ImageNet in R with TensorFlow and Spark looks as follows:&lt;/p&gt;
&lt;p&gt;We hope this post gave you a reasonable overview of what training large-datasets in R looks like – thanks for reading along!&lt;/p&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">73f5c8c15bde1f040b537394d5db0936</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop_vae(ds_train)&lt;/p&gt;
&lt;p&gt;test_batch &amp;lt;- as_iterator(ds_test) %&amp;gt;% iter_next() encoded &amp;lt;- encoder(test_batch[[1]][1:1000]) test_var &amp;lt;- tf&lt;span class="math inline"&gt;\(math\)&lt;/span&gt;reduce_variance(encoded, axis = 0L) print(test_var %&amp;gt;% as.numeric() %&amp;gt;% round(5)) } ```&lt;/p&gt;
&lt;div id="experimental-setup-and-data" class="section level2"&gt;
&lt;h2&gt;Experimental setup and data&lt;/h2&gt;
&lt;p&gt;The idea was to add white noise to a deterministic series. This time, the &lt;a href="https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor"&gt;Roessler system&lt;/a&gt; was chosen, mainly for the prettiness of its attractor, apparent even in its two-dimensional projections:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler.png" alt="Roessler attractor, two-dimensional projections." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-1)Roessler attractor, two-dimensional projections.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like we did for the Lorenz system in the first part of this series, we use &lt;code&gt;deSolve&lt;/code&gt; to generate data from the Roessler equations.&lt;/p&gt;
&lt;p&gt;Then, noise is added, to the desired degree, by drawing from a normal distribution, centered at zero, with standard deviations varying between 1 and 2.5.&lt;/p&gt;
&lt;p&gt;Here you can compare effects of not adding any noise (left), standard deviation-1 (middle), and standard deviation-2.5 Gaussian noise:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/roessler_noise.png" alt="Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-4)Roessler series with added noise. Top: none. Middle: SD = 1. Bottom: SD = 2.5.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Otherwise, preprocessing proceeds as in the previous posts. In the upcoming results section, we’ll compare forecasts not just to the “real”, after noise addition, test split of the data, but also to the underlying Roessler system – that is, the thing we’re really interested in. (Just that in the real world, we can’t do that check.) This second test set is prepared for forecasting just like the other one; to avoid duplication we don’t reproduce the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="results" class="section level2"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The LSTM used for comparison with the VAE described above is identical to the architecture employed in the previous post. While with the VAE, an &lt;code&gt;fnn_multiplier&lt;/code&gt; of 1 yielded sufficient regularization for all noise levels, some more experimentation was needed for the LSTM: At noise levels 2 and 2.5, that multiplier was set to 5.&lt;/p&gt;
&lt;p&gt;As a result, in all cases, there was one latent variable with high variance and a second one of minor importance. For all others, variance was close to 0.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In all cases&lt;/em&gt; here means: In all cases where FNN regularization was used. As already hinted at in the introduction, the main regularizing factor providing robustness to noise here seems to be FNN loss, not KL divergence. So for all noise levels, besides FNN-regularized LSTM and VAE models we also tested their non-constrained counterparts.&lt;/p&gt;
&lt;div id="low-noise" class="section level4"&gt;
&lt;h4&gt;Low noise&lt;/h4&gt;
&lt;p&gt;Seeing how all models did superbly on the original deterministic series, a noise level of 1 can almost be treated as a baseline. Here you see sixteen 120-timestep predictions from both regularized models, FNN-VAE (dark blue), and FNN-LSTM (orange). The noisy test data, both input (&lt;code&gt;x&lt;/code&gt;, 120 steps) and output (&lt;code&gt;y&lt;/code&gt;, 120 steps) are displayed in (blue-ish) grey. In green, also spanning the whole sequence, we have the original Roessler data, the way they would look had no noise been added.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-6)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the noise, forecasts from both models look excellent. Is this due to the FNN regularizer?&lt;/p&gt;
&lt;p&gt;Looking at forecasts from their unregularized counterparts, we have to admit these do not look any worse. (For better comparability, the sixteen sequences to forecast were initiallly picked at random, but used to test all models and conditions.)&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_1_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-7)Roessler series with added Gaussian noise of standard deviation 1. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What happens when we start to add noise?&lt;/p&gt;
&lt;/div&gt;
&lt;div id="substantial-noise" class="section level4"&gt;
&lt;h4&gt;Substantial noise&lt;/h4&gt;
&lt;p&gt;Between noise levels 1.5 and 2, something changed, or became noticeable from visual inspection. Let’s jump directly to the highest-used level though: 2.5.&lt;/p&gt;
&lt;p&gt;Here first are predictions obtained from the unregularized models.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5_nofnn.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-8)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from unregularized LSTM. Dark blue: Predictions from unregularized VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Both LSTM and VAE get “distracted” a bit too much by the noise, the latter to an even higher degree. This leads to cases where predictions strongly “overshoot” the underlying non-noisy rhythm. This is not surprising, of course: They were &lt;em&gt;trained&lt;/em&gt; on the noisy version; predict fluctuations is what they learned.&lt;/p&gt;
&lt;p&gt;Do we see the same with the FNN models?&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/noise_2.5.png" alt="Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-9)Roessler series with added Gaussian noise of standard deviation 2.5. Grey: actual (noisy) test data. Green: underlying Roessler system. Orange: Predictions from FNN-LSTM. Dark blue: Predictions from FNN-VAE.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly, we see a much better fit to the underlying Roessler system now! Especially the VAE model, FNN-VAE, surprises with a whole new smoothness of predictions; but FNN-LSTM turns up much smoother forecasts as well.&lt;/p&gt;
&lt;p&gt;“Smooth, fitting the system…” – by now you may be wondering, when are we going to come up with more quantitative assertions? If quantitative implies “mean squared error” (MSE), and if MSE is taken to be some divergence between forecasts and the true target from the test set, the answer is that this MSE doesn’t differ much between any of the four architectures. Put differently, it is mostly a function of noise level.&lt;/p&gt;
&lt;p&gt;However, we could argue that what we’re really interested in is how well a model forecasts the underlying process. And there, we see differences.&lt;/p&gt;
&lt;p&gt;In the following plot, we contrast MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). The rows reflect noise levels (1, 1.5, 2, 2.5); the columns represent MSE in relation to the noisy(“real”) target (left) on the one hand, and in relation to the underlying system on the other (right). For better visibility of the effect, &lt;em&gt;MSEs have been normalized as fractions of the maximum MSE in a category&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we want to predict &lt;em&gt;signal plus noise&lt;/em&gt; (left), it is not extremely critical whether we use FNN or not. But if we want to predict the signal only (right), with increasing noise in the data FNN loss becomes increasingly effective. This effect is far stronger for VAE vs. FNN-VAE than for LSTM vs. FNN-LSTM: The distance between the grey line (VAE) and the dark blue one (FNN-VAE) becomes larger and larger as we add more noise.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/mses.png" alt="Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right)." width="600" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-10)Normalized MSEs obtained for the four model types (grey: VAE; orange: LSTM; dark blue: FNN-VAE; green: FNN-LSTM). Rows are noise levels (1, 1.5, 2, 2.5); columns are MSE as related to the real target (left) and the underlying system (right).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="summing-up" class="section level2"&gt;
&lt;h2&gt;Summing up&lt;/h2&gt;
&lt;p&gt;Our experiments show that when noise is likely to obscure measurements from an underlying deterministic system, FNN regularization can strongly improve forecasts. This is the case especially for convolutional VAEs, and probably convolutional autoencoders in general. And if an FNN-constrained VAE performs as well, for time series prediction, as an LSTM, there is a strong incentive to use the convolutional model: It trains significantly faster.&lt;/p&gt;
&lt;p&gt;With that, we conclude our mini-series on FNN-regularized models. As always, we’d love to hear from you if you were able to make use of this in your own work!&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">931e811b30064bd6be23eefda987ed92</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;") training_loop(ds_train)&lt;br /&gt;
}&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;After two hundred epochs, overall loss is at 2.67, with the MSE component at 1.8 and FNN at 0.09.&lt;/p&gt;
&lt;div id="obtaining-the-attractor-from-the-test-set" class="section level3"&gt;
&lt;h3&gt;Obtaining the attractor from the test set&lt;/h3&gt;
&lt;p&gt;We use the test set to inspect the latent code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,242 x 10
      V1    V2         V3        V4        V5         V6        V7        V8       V9       V10
   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
 1 0.439 0.401 -0.000614  -0.0258   -0.00176  -0.0000276  0.000276  0.00677  -0.0239   0.00906 
 2 0.415 0.504  0.0000481 -0.0279   -0.00435  -0.0000970  0.000921  0.00509  -0.0214   0.00921 
 3 0.389 0.619  0.000848  -0.0240   -0.00661  -0.000171   0.00106   0.00454  -0.0150   0.00794 
 4 0.363 0.729  0.00137   -0.0143   -0.00652  -0.000244   0.000523  0.00450  -0.00594  0.00476 
 5 0.335 0.809  0.00128   -0.000450 -0.00338  -0.000307  -0.000561  0.00407   0.00394 -0.000127
 6 0.304 0.828  0.000631   0.0126    0.000889 -0.000351  -0.00167   0.00250   0.0115  -0.00487 
 7 0.274 0.769 -0.000202   0.0195    0.00403  -0.000367  -0.00220  -0.000308  0.0145  -0.00726 
 8 0.246 0.657 -0.000865   0.0196    0.00558  -0.000359  -0.00208  -0.00376   0.0134  -0.00709 
 9 0.224 0.535 -0.00121    0.0162    0.00608  -0.000335  -0.00169  -0.00697   0.0106  -0.00576 
10 0.211 0.434 -0.00129    0.0129    0.00606  -0.000306  -0.00134  -0.00927   0.00820 -0.00447 
# … with 6,232 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result of the FNN regularizer, the latent code units should be ordered roughly by decreasing variance, with a sharp drop appearing some place (if the FNN weight has been chosen adequately).&lt;/p&gt;
&lt;p&gt;For an &lt;code&gt;fnn_weight&lt;/code&gt; of 10, we do see a drop after the first two units:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 10
      V1     V2      V3      V4      V5      V6      V7      V8      V9     V10
   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
1 0.0739 0.0582 1.12e-6 3.13e-4 1.43e-5 1.52e-8 1.35e-6 1.86e-4 1.67e-4 4.39e-5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the model indicates that the Lorenz attractor can be represented in two dimensions. If we nonetheless want to plot the complete (reconstructed) state space of three dimensions, we should reorder the remaining variables by magnitude of variance&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Here, this results in three projections of the set &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-06-24-deep-attractors/images/predicted_attractors.png" alt="Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen." width="500" /&gt;
&lt;p class="caption"&gt;
(#fig:unnamed-chunk-3)Attractors as predicted from the latent code (test set). The three highest-variance variables were chosen.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="wrapping-up-for-this-time" class="section level2"&gt;
&lt;h2&gt;Wrapping up (for this time)&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen how to reconstruct the Lorenz attractor from data we did not train on (the test set), using an autoencoder regularized by a custom &lt;em&gt;false nearest neighbors&lt;/em&gt; loss. It is important to stress that at no point was the network presented with the expected solution (attractor) – training was purely unsupervised.&lt;/p&gt;
&lt;p&gt;This is a fascinating result. Of course, thinking practically, the next step is to obtain predictions on heldout data. Given how long this text has become already, we reserve that for a follow-up post. And again &lt;em&gt;of course&lt;/em&gt;, we’re thinking about other datasets, especially ones where the true state space is not known beforehand. What about measurement noise? What about datasets that are not completely deterministic&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;? There is a lot to explore, stay tuned – and as always, thanks for reading!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;As per author recommendation (personal communication).&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;See &lt;span class="citation"&gt;[@Kantz]&lt;/span&gt; for detailed discussions on using methodology from nonlinear deterministic systems analysis for noisy and/or partly-stochastic data.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">5503b1da2d7660fbcd2bc810fb123a30</distill:md5>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="2250" height="1140"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="2378" height="1563"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>A first look at federated learning with TensorFlow</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</link>
      <description>The term "federated learning" was coined to describe a form of distributed model training where the data remains on client devices, i.e., is never shipped to the coordinating server. In this post, we introduce central concepts and run first experiments with TensorFlow Federated, using R.</description>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</guid>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/images/federated_learning.png" medium="image" type="image/png" width="1122" height="570"/>
    </item>
    <item>
      <title>NumPy-style broadcasting for R TensorFlow users</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</link>
      <description>Broadcasting, as done by Python's scientific computing library NumPy, involves dynamically extending shapes so that arrays of different sizes may be passed to operations that expect conformity - such as adding or multiplying elementwise. In NumPy, the way broadcasting works is specified exactly; the same rules apply to TensorFlow operations. For anyone who finds herself, occasionally, consulting Python code, this post strives to explain.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</guid>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>First experiments with TensorFlow mixed-precision training</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</link>
      <description>TensorFlow 2.1, released last week, allows for mixed-precision training, making use of the Tensor Cores available in the most recent NVidia GPUs. In this post, we report first experimental results and provide some background on what this is all about.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</guid>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training/images/tc.png" medium="image" type="image/png" width="589" height="399"/>
    </item>
    <item>
      <title>Differential Privacy with TensorFlow</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</link>
      <description>Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.</description>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</guid>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/images/cat.png" medium="image" type="image/png" width="1920" height="1206"/>
    </item>
    <item>
      <title>tfhub: R interface to TensorFlow Hub</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</link>
      <description>TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</guid>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" medium="image" type="image/png" width="1365" height="909"/>
    </item>
    <item>
      <title>Gaussian Process Regression with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</link>
      <description>Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather "normal" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</guid>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png" medium="image" type="image/png" width="818" height="352"/>
    </item>
    <item>
      <title>Getting started with Keras from R - the 2020 edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</link>
      <description>Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</guid>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020/images/website.png" medium="image" type="image/png" width="1591" height="725"/>
    </item>
    <item>
      <title>Variational convnets with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</link>
      <description>In a Bayesian neural network, layer weights are distributions, not tensors. Using tfprobability, the R wrapper to TensorFlow Probability, we can build regular Keras models that have probabilistic layers, and thus get uncertainty estimates "for free". In this post, we show how to define, train and obtain predictions from a probabilistic convolutional neural network.</description>
      <category>Probabilistic ML/DL</category>
      <category>Time Series</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</guid>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet/images/bbb.png" medium="image" type="image/png" width="796" height="378"/>
    </item>
    <item>
      <title>tfprobability 0.8 on CRAN: Now how can you use it?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</link>
      <description>Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.</description>
      <category>Probabilistic ML/DL</category>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</guid>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran/images/tfprobability.png" medium="image" type="image/png" width="518" height="600"/>
    </item>
    <item>
      <title>TensorFlow 2.0 is here - what changes for R users?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</link>
      <description>TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</guid>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/images/thumb.png" medium="image" type="image/png" width="400" height="400"/>
    </item>
    <item>
      <title>BERT from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Turgut Abdullayev</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</link>
      <description>A deep learning model - BERT from Google AI Research - has yielded state-of-the-art results in a wide variety of Natural Language Processing (NLP) tasks. In this tutorial, we will show how to load and train the BERT model from R, using Keras.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</guid>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r/images/bert.png" medium="image" type="image/png" width="437" height="367"/>
    </item>
    <item>
      <title>So, how come we can use TensorFlow from R?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</link>
      <description>Have you ever wondered why you can call TensorFlow - mostly known as a Python framework - from R? If not - that's how it should be, as the R packages keras and tensorflow aim to make this process as transparent as possible to the user. But for them to be those helpful genies, someone else first has to tame the Python.</description>
      <category>TensorFlow/Keras</category>
      <category>Meta</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</guid>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/images/thumb.png" medium="image" type="image/png" width="739" height="516"/>
    </item>
    <item>
      <title>Image segmentation with U-Net</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</link>
      <description>In image segmentation, every pixel of an image is assigned a class. Depending on the application, classes could be different cell types; or the task could be binary, as in "cancer cell yes or no?". Area of application notwithstanding, the established neural network architecture of choice is U-Net. In this post, we show how to preprocess data and train a U-Net model on the Kaggle Carvana image segmentation data.</description>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</guid>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet/images/unet.png" medium="image" type="image/png" width="1400" height="932"/>
    </item>
    <item>
      <title>Modeling censored data with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</link>
      <description>In this post we use tfprobability, the R interface to TensorFlow Probability, to model censored data. Again, the exposition is inspired by the treatment of this topic in Richard McElreath's Statistical Rethinking. Instead of cute cats though, we model immaterial entities from the cold world of technology: This post explores durations of CRAN package checks, a dataset that comes with Max Kuhn's parsnip.</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</guid>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/images/thumb_cropped.png" medium="image" type="image/png" width="955" height="396"/>
    </item>
    <item>
      <title>TensorFlow feature columns: Transforming your data recipes-style</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</link>
      <description>TensorFlow feature columns provide useful functionality for preprocessing categorical data and chaining transformations, like bucketization or feature crossing. From R, we use them in popular "recipes" style, creating and subsequently refining a feature specification. In this post, we show how using feature specs frees cognitive resources and lets you focus on what you really want to accomplish. What's more, because of its elegance, feature-spec code reads nice and is fun to write as well.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</guid>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/images/feature_cols_hier.png" medium="image" type="image/png" width="1172" height="678"/>
    </item>
    <item>
      <title>Adding uncertainty estimates to Keras models with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</link>
      <description>As of today, there is no mainstream road to obtaining uncertainty estimates from neural networks. All that can be said is that, normally, approaches tend to be Bayesian in spirit, involving some way of putting a prior over model weights. This holds true as well for the method presented in this post: We show how to use tfprobability, the R interface to TensorFlow Probability, to add uncertainty estimates to a Keras model in an elegant and conceptually plausible way.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</guid>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png" medium="image" type="image/png" width="2020" height="1020"/>
    </item>
    <item>
      <title>Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</link>
      <description>This post builds on our recent introduction to multi-level modeling with tfprobability, the R wrapper to TensorFlow Probability. We show how to pool not just mean values ("intercepts"), but also relationships ("slopes"), thus enabling models to learn from data in an even broader way. Again, we use an example from Richard McElreath's "Statistical Rethinking"; the terminology as well as the way we present this topic are largely owed to this book.</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</guid>
      <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/images/thumb.png" medium="image" type="image/png" width="509" height="249"/>
    </item>
    <item>
      <title>Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</link>
      <description>This post is a first introduction to MCMC modeling with tfprobability, the R interface to TensorFlow Probability (TFP). Our example is a multi-level model describing tadpole mortality, which may be known to the reader from Richard McElreath's wonderful "Statistical Rethinking".</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</guid>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png" medium="image" type="image/png" width="1612" height="659"/>
    </item>
    <item>
      <title>Experimenting with autoregressive flows in TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</link>
      <description>Continuing from the recent introduction to bijectors in TensorFlow Probability (TFP), this post brings autoregressivity to the table. Using TFP through the new R package tfprobability, we look at the implementation of masked autoregressive flows (MAF) and put them to use on two different datasets.</description>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</guid>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows/images/made.png" medium="image" type="image/png" width="686" height="398"/>
    </item>
    <item>
      <title>Auto-Keras: Tuning-free deep learning from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Cruz Rodriguez</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</link>
      <description>Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</guid>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/images/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Getting into the flow: Bijectors in TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</link>
      <description>Normalizing flows are one of the lesser known, yet fascinating and successful architectures in unsupervised deep learning. In this post we provide a basic introduction to flows using tfprobability, an R wrapper to TensorFlow Probability. Upcoming posts will build on this, using more complex flows on more complex data.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</guid>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/images/flows.png" medium="image" type="image/png" width="904" height="325"/>
    </item>
    <item>
      <title>Audio classification with Keras: Looking closer at the non-deep learning parts</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</link>
      <description>Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</guid>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background/images/seven2.png" medium="image" type="image/png" width="1714" height="846"/>
    </item>
    <item>
      <title>Discrete Representation Learning with VQ-VAE and TensorFlow Probability</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</link>
      <description>Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's "Neural Discrete Representation Learning" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.</description>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</guid>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/images/thumb1.png" medium="image" type="image/png" width="510" height="287"/>
    </item>
    <item>
      <title>Getting started with TensorFlow Probability from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</link>
      <description>TensorFlow Probability offers a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. It works seamlessly with core TensorFlow and (TensorFlow) Keras. In this post, we provide a short introduction to the distributions layer and then, use it for sampling and calculating probabilities in a Variational Autoencoder.</description>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</guid>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png" medium="image" type="image/png" width="884" height="584"/>
    </item>
    <item>
      <title>Concepts in object detection</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</link>
      <description>As shown in a previous post, naming and locating a single object in an image is a task that may be approached in a straightforward way. This is not the same with general object detection, though - naming and locating several objects at once, with no prior information about how many objects are supposed to be detected.
In this post, we explain the steps involved in coding a basic single-shot object detector: Not unlike SSD (Single-shot Multibox Detector), but simplified and designed not for best performance, but comprehensibility.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</guid>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts/images/results.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Entity embeddings for fun and profit</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</link>
      <description>Embedding layers are not just useful when working with language data. As "entity embeddings", they've recently become famous for applications on tabular, small-scale data. In this post, we exemplify two possible use cases, also drawing attention to what not to expect.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</guid>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png" medium="image" type="image/png" width="820" height="410"/>
    </item>
    <item>
      <title>You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</link>
      <description>In deep learning, there is no obvious way of obtaining uncertainty estimates. In 2016, Gal and Ghahramani proposed a method that is both theoretically grounded and practical: use dropout at test time. In this post, we introduce a refined version of this method (Gal et al. 2017) that has the network itself learn how uncertain it is.</description>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</guid>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png" medium="image" type="image/png" width="2046" height="872"/>
    </item>
    <item>
      <title>Naming and locating objects in images</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</link>
      <description>Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</guid>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/images/preds_train.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Representation learning with MMD-VAE</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</link>
      <description>Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</guid>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/images/thumb.png" medium="image" type="image/png" width="468" height="178"/>
    </item>
    <item>
      <title>Winner takes all: A look at activations and cost functions</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</link>
      <description>Why do we use the activations we use, and how do they relate to the cost functions they tend to co-appear with? In this post we provide a conceptual introduction.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</guid>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/images/output.png" medium="image" type="image/png" width="800" height="384"/>
    </item>
    <item>
      <title>More flexible models with TensorFlow eager execution and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</link>
      <description>Advanced applications like generative adversarial networks, neural style transfer, and the attention mechanism ubiquitous in natural language processing used to be not-so-simple to implement with the Keras declarative coding paradigm. Now, with the advent of TensorFlow eager execution, things have changed. This post explores using eager execution with R.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</guid>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/images/m.png" medium="image" type="image/png" width="384" height="126"/>
    </item>
    <item>
      <title>Collaborative filtering with embeddings</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</link>
      <description>Embeddings are not just for use in natural language processing. Here we apply embeddings to a common task in collaborative filtering - predicting user ratings - and on our way, strive for a better understanding of what an embedding layer really does.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</guid>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender/images/m.png" medium="image" type="image/png" width="700" height="402"/>
    </item>
    <item>
      <title>Image-to-image translation with pix2pix</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</link>
      <description>Conditional GANs (cGANs) may be used to generate one type of object based on another - e.g., a map based on a photo, or a color video based on black-and-white. Here, we show how to implement the pix2pix approach with Keras and eager execution.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</guid>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png" medium="image" type="image/png" width="842" height="536"/>
    </item>
    <item>
      <title>Attention-based Image Captioning with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</link>
      <description>Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</guid>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png" medium="image" type="image/png" width="627" height="269"/>
    </item>
    <item>
      <title>Neural style transfer with eager execution and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</link>
      <description>Continuing our series on combining Keras with TensorFlow eager execution, we show how to implement neural style transfer in a straightforward way. Based on this easy-to-adapt example, you can easily perform style transfer on your own images.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</guid>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/images/preview.png" medium="image" type="image/png" width="344" height="231"/>
    </item>
    <item>
      <title>Getting started with deep learning in R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</link>
      <description>Many fields are benefiting from the use of deep learning, and with the R keras, tensorflow and related packages, you can now easily do state of the art deep learning in R. In this post, we want to give some orientation as to how to best get started.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</guid>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started/images/digits.png" medium="image" type="image/png" width="557" height="317"/>
    </item>
    <item>
      <title>Generating images with Keras and TensorFlow eager execution</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</link>
      <description>Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities (often but not always images). We show how to code them using Keras and TensorFlow eager execution.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</guid>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/images/thumb.png" medium="image" type="image/png" width="240" height="144"/>
    </item>
    <item>
      <title>Attention-based Neural Machine Translation with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</link>
      <description>As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</guid>
      <pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/images/attention.png" medium="image" type="image/png" width="606" height="448"/>
    </item>
    <item>
      <title>Predicting Sunspot Frequency with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Dancho</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</link>
      <description>In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.</description>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</guid>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/images/backtested_test.png" medium="image" type="image/png" width="800" height="416"/>
    </item>
    <item>
      <title>Simple Audio Classification with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</link>
      <description>In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.</description>
      <category>TensorFlow/Keras</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</guid>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png" medium="image" type="image/png"/>
    </item>
    <item>
      <title>lime v0.4: The Kitten Picture Edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Lin Pedersen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</link>
      <description>A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <category>Explainability</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</guid>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1344" height="672"/>
    </item>
    <item>
      <title>Deep Learning for Cancer Immunotherapy</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leon Eyrich Jessen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</link>
      <description>The aim of this post is to illustrate how deep learning is being applied in cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient's own immune system to fight the cancer.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</guid>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png" medium="image" type="image/png" width="3000" height="1800"/>
    </item>
    <item>
      <title>Predicting Fraud with Autoencoders and Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</link>
      <description>In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML. The basis of our model will be the Kaggle Credit Card Fraud Detection dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Cloud</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</guid>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder/images/preview.png" medium="image" type="image/png" width="790" height="537"/>
    </item>
    <item>
      <title>Analyzing rtweet Data with kerasformula</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pete Mohanty</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</link>
      <description>The kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices. We use kerasformula to predict how popular tweets will be based on how often the tweet was retweeted and favorited.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</guid>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png" medium="image" type="image/png" width="672" height="480"/>
    </item>
    <item>
      <title>Deep Learning With Keras To Predict Customer Churn</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Dancho</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</link>
      <description>Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <category>Explainability</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</guid>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png" medium="image" type="image/png" width="2696" height="1696"/>
    </item>
    <item>
      <title>Classifying Duplicate Questions from Quora with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</link>
      <description>In this post we will use Keras to classify duplicated questions from Quora. Our implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors)</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</guid>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png" medium="image" type="image/png" width="1302" height="788"/>
    </item>
    <item>
      <title>Word Embeddings with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</link>
      <description>Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</guid>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png" medium="image" type="image/png" width="700" height="450"/>
    </item>
    <item>
      <title>Time Series Forecasting with Recurrent Neural Networks</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</link>
      <description>In this post, we'll review three advanced techniques for improving the performance and generalization power of recurrent neural networks.  We'll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building.</description>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</guid>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png" medium="image" type="image/png" width="6000" height="4000"/>
    </item>
    <item>
      <title>Image Classification on Small Datasets with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</link>
      <description>Having to train an image-classification model using very little data is a common situation, in this article we review three techniques for tackling this problem including feature extraction and fine tuning from a pretrained network.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</guid>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png" medium="image" type="image/png" width="678" height="453"/>
    </item>
    <item>
      <title>Deep Learning for Text Classification with Keras</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">François Chollet</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</link>
      <description>Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this excerpt from the book Deep Learning with R, you'll learn to classify movie reviews as positive or negative, based on the text content of the reviews.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</guid>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras/images/training-history.png" medium="image" type="image/png" width="1400" height="865"/>
    </item>
    <item>
      <title>Keras for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</link>
      <description>We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</guid>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r/preview.png" medium="image" type="image/png" width="669" height="414"/>
    </item>
  </channel>
</rss>
