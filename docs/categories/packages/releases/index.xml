<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 16 Jul 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</link>
      <description>


&lt;div class="container-fluid main-container"&gt;




&lt;div class="fluid-row" id="header"&gt;




&lt;/div&gt;


&lt;p&gt;&lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; 1.3 is now available on &lt;a href="https://cran.r-project.org/web/packages/sparklyr/index.html"&gt;CRAN&lt;/a&gt;, with the following major new features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#higher-order-functions"&gt;Higher-order Functions&lt;/a&gt; to easily manipulate arrays and structs&lt;/li&gt;
&lt;li&gt;Support for Apache &lt;a href="#avro"&gt;Avro&lt;/a&gt;, a row-oriented data serialization framework&lt;/li&gt;
&lt;li&gt;&lt;a href="#custom-serialization"&gt;Custom Serialization&lt;/a&gt; using R functions to read and write any data format&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-improvements"&gt;Other Improvements&lt;/a&gt; such as compatibility with EMR 6.0 &amp;amp; Spark 3.0, and initial support for Flint time series library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install &lt;code&gt;sparklyr&lt;/code&gt; 1.3 from CRAN, run&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;install.packages(&amp;quot;sparklyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this post, we shall highlight some major new features introduced in sparklyr 1.3, and showcase scenarios where such features come in handy. While a number of enhancements and bug fixes (especially those related to &lt;code&gt;spark_apply()&lt;/code&gt;, &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and secondary Spark connections) were also an important part of this release, they will not be the topic of this post, and it will be an easy exercise for the reader to find out more about them from the sparklyr &lt;a href="https://github.com/sparklyr/sparklyr/blob/master/NEWS.md"&gt;NEWS&lt;/a&gt; file.&lt;/p&gt;
&lt;div id="higher-order-functions" class="section level2"&gt;
&lt;h2&gt;Higher-order Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/SPARK-19480"&gt;Higher-order functions&lt;/a&gt; are built-in Spark SQL constructs that allow user-defined lambda expressions to be applied efficiently to complex data types such as arrays and structs. As a quick demo to see why higher-order functions are useful, let’s say one day Scrooge McDuck dove into his huge vault of money and found large quantities of pennies, nickels, dimes, and quarters. Having an impeccable taste in data structures, he decided to store the quantities and face values of everything into two Spark SQL array columns:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4.5&amp;quot;)
coins_tbl &amp;lt;- copy_to(
  sc,
  tibble::tibble(
    quantities = list(c(4000, 3000, 2000, 1000)),
    values = list(c(1, 5, 10, 25))
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus declaring his net worth of 4k pennies, 3k nickels, 2k dimes, and 1k quarters. To help Scrooge McDuck calculate the total value of each type of coin in sparklyr 1.3 or above, we can apply &lt;code&gt;hof_zip_with()&lt;/code&gt;, the sparklyr equivalent of &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html#zip_with"&gt;ZIP_WITH&lt;/a&gt;, to &lt;code&gt;quantities&lt;/code&gt; column and &lt;code&gt;values&lt;/code&gt; column, combining pairs of elements from arrays in both columns. As you might have guessed, we also need to specify how to combine those elements, and what better way to accomplish that than a concise one-sided formula   &lt;code&gt;~ .x * .y&lt;/code&gt;   in R, which says we want (quantity * value) for each type of coin? So, we have the following:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result_tbl &amp;lt;- coins_tbl %&amp;gt;%
  hof_zip_with(~ .x * .y, dest_col = total_values) %&amp;gt;%
  dplyr::select(total_values)

result_tbl %&amp;gt;% dplyr::pull(total_values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  4000 15000 20000 25000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the result &lt;code&gt;4000 15000 20000 25000&lt;/code&gt; telling us there are in total $40 dollars worth of pennies, $150 dollars worth of nickels, $200 dollars worth of dimes, and $250 dollars worth of quarters, as expected.&lt;/p&gt;
&lt;p&gt;Using another sparklyr function named &lt;code&gt;hof_aggregate()&lt;/code&gt;, which performs an &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html#aggregate"&gt;AGGREGATE&lt;/a&gt; operation in Spark, we can then compute the net worth of Scrooge McDuck based on &lt;code&gt;result_tbl&lt;/code&gt;, storing the result in a new column named &lt;code&gt;total&lt;/code&gt;. Notice for this aggregate operation to work, we need to ensure the starting value of aggregation has data type (namely, &lt;code&gt;BIGINT&lt;/code&gt;) that is consistent with the data type of &lt;code&gt;total_values&lt;/code&gt; (which is &lt;code&gt;ARRAY&amp;lt;BIGINT&amp;gt;&lt;/code&gt;), as shown below:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;result_tbl %&amp;gt;%
  dplyr::mutate(zero = dplyr::sql(&amp;quot;CAST (0 AS BIGINT)&amp;quot;)) %&amp;gt;%
  hof_aggregate(start = zero, ~ .x + .y, expr = total_values, dest_col = total) %&amp;gt;%
  dplyr::select(total) %&amp;gt;%
  dplyr::pull(total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 64000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So Scrooge McDuck’s net worth is $640 dollars.&lt;/p&gt;
&lt;p&gt;Other higher-order functions supported by Spark SQL so far include &lt;code&gt;transform&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;exists&lt;/code&gt;, as documented in &lt;a href="https://spark.apache.org/docs/latest/api/sql/index.html"&gt;here&lt;/a&gt;, and similar to the example above, their counterparts (namely, &lt;code&gt;hof_transform()&lt;/code&gt;, &lt;code&gt;hof_filter()&lt;/code&gt;, and &lt;code&gt;hof_exists()&lt;/code&gt;) all exist in sparklyr 1.3, so that they can be integrated with other &lt;code&gt;dplyr&lt;/code&gt; verbs in an idiomatic manner in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="avro" class="section level2"&gt;
&lt;h2&gt;Avro&lt;/h2&gt;
&lt;p&gt;Another highlight of the sparklyr 1.3 release is its built-in support for Avro data sources. Apache Avro is a widely used data serialization protocol that combines the efficiency of a binary data format with the flexibility of JSON schema definitions. To make working with Avro data sources simpler, in sparklyr 1.3, as soon as a Spark connection is instantiated with &lt;code&gt;spark_connect(..., package = &amp;quot;avro&amp;quot;)&lt;/code&gt;, sparklyr will automatically figure out which version of &lt;code&gt;spark-avro&lt;/code&gt; package to use with that connection, saving a lot of potential headaches for sparklyr users trying to determine the correct version of &lt;code&gt;spark-avro&lt;/code&gt; by themselves. Similar to how &lt;code&gt;spark_read_csv()&lt;/code&gt; and &lt;code&gt;spark_write_csv()&lt;/code&gt; are in place to work with CSV data, &lt;code&gt;spark_read_avro()&lt;/code&gt; and &lt;code&gt;spark_write_avro()&lt;/code&gt; methods were implemented in sparklyr 1.3 to facilitate reading and writing Avro files through an Avro-capable Spark connection, as illustrated in the example below:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

# The `package = &amp;quot;avro&amp;quot;` option is only supported in Spark 2.4 or higher
sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, version = &amp;quot;2.4.5&amp;quot;, package = &amp;quot;avro&amp;quot;)

sdf &amp;lt;- sdf_copy_to(
  sc,
  tibble::tibble(
    a = c(1, NaN, 3, 4, NaN),
    b = c(-2L, 0L, 1L, 3L, 2L),
    c = c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;d&amp;quot;)
  )
)

# This example Avro schema is a JSON string that essentially says all columns
# (&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;) of `sdf` are nullable.
avro_schema &amp;lt;- jsonlite::toJSON(list(
  type = &amp;quot;record&amp;quot;,
  name = &amp;quot;topLevelRecord&amp;quot;,
  fields = list(
    list(name = &amp;quot;a&amp;quot;, type = list(&amp;quot;double&amp;quot;, &amp;quot;null&amp;quot;)),
    list(name = &amp;quot;b&amp;quot;, type = list(&amp;quot;int&amp;quot;, &amp;quot;null&amp;quot;)),
    list(name = &amp;quot;c&amp;quot;, type = list(&amp;quot;string&amp;quot;, &amp;quot;null&amp;quot;))
  )
), auto_unbox = TRUE)

# persist the Spark data frame from above in Avro format
spark_write_avro(sdf, &amp;quot;/tmp/data.avro&amp;quot;, as.character(avro_schema))

# and then read the same data frame back
spark_read_avro(sc, &amp;quot;/tmp/data.avro&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;data&amp;gt; [?? x 3]
      a     b c
  &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;
  1     1    -2 &amp;quot;a&amp;quot;
  2   NaN     0 &amp;quot;b&amp;quot;
  3     3     1 &amp;quot;c&amp;quot;
  4     4     3 &amp;quot;&amp;quot;
  5   NaN     2 &amp;quot;d&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="custom-serialization" class="section level2"&gt;
&lt;h2&gt;Custom Serialization&lt;/h2&gt;
&lt;p&gt;In addition to commonly used data serialization formats such as CSV, JSON, Parquet, and Avro, starting from sparklyr 1.3, customized data frame serialization and deserialization procedures implemented in R can also be run on Spark workers via the newly implemented &lt;code&gt;spark_read()&lt;/code&gt; and &lt;code&gt;spark_write()&lt;/code&gt; methods. We can see both of them in action through a quick example below, where &lt;code&gt;saveRDS()&lt;/code&gt; is called from a user-defined writer function to save all rows within a Spark data frame into 2 RDS files on disk, and &lt;code&gt;readRDS()&lt;/code&gt; is called from a user-defined reader function to read the data from the RDS files back to Spark:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(sparklyr)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
sdf &amp;lt;- sdf_len(sc, 7)
paths &amp;lt;- c(&amp;quot;/tmp/file1.RDS&amp;quot;, &amp;quot;/tmp/file2.RDS&amp;quot;)

spark_write(sdf, writer = function(df, path) saveRDS(df, path), paths = paths)
spark_read(sc, paths, reader = function(path) readRDS(path), columns = c(id = &amp;quot;integer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Source: spark&amp;lt;?&amp;gt; [?? x 1]
     id
  &amp;lt;int&amp;gt;
1     1
2     2
3     3
4     4
5     5
6     6
7     7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="other-improvements" class="section level2"&gt;
&lt;h2&gt;Other Improvements&lt;/h2&gt;
&lt;div id="sparklyr.flint" class="section level3"&gt;
&lt;h3&gt;Sparklyr.flint&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/r-spark/sparklyr.flint"&gt;Sparklyr.flint&lt;/a&gt; is a sparklyr extension that aims to make functionalities from the &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; time-series library easily accessible from R. It is currently under active development. One piece of good news is that, while the original &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; library was designed to work with Spark 2.x, a slightly modified &lt;a href="https://github.com/yl790/flint"&gt;fork&lt;/a&gt; of it will work well with Spark 3.0, and within the existing sparklyr extension framework. &lt;code&gt;sparklyr.flint&lt;/code&gt; can automatically determine which version of the Flint library to load based on the version of Spark it’s connected to. Another bit of good news is, as previously mentioned, &lt;code&gt;sparklyr.flint&lt;/code&gt; doesn’t know too much about its own destiny yet. Maybe you can play an active part in shaping its future!&lt;/p&gt;
&lt;/div&gt;
&lt;div id="emr-6.0" class="section level3"&gt;
&lt;h3&gt;EMR 6.0&lt;/h3&gt;
&lt;p&gt;This release also features a small but important change that allows sparklyr to correctly connect to the version of Spark 2.4 that is included in Amazon EMR 6.0.&lt;/p&gt;
&lt;p&gt;Previously, sparklyr automatically assumed any Spark 2.x it was connecting to was built with Scala 2.11 and attempted to load any required Scala artifacts built with Scala 2.11 as well. This became problematic when connecting to Spark 2.4 from Amazon EMR 6.0, which is built with Scala 2.12. Starting from sparklyr 1.3, such problem can be fixed by simply specifying &lt;code&gt;scala_version = &amp;quot;2.12&amp;quot;&lt;/code&gt; when calling &lt;code&gt;spark_connect()&lt;/code&gt; (e.g., &lt;code&gt;spark_connect(master = &amp;quot;yarn-client&amp;quot;, scala_version = &amp;quot;2.12&amp;quot;)&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id="spark-3.0" class="section level3"&gt;
&lt;h3&gt;Spark 3.0&lt;/h3&gt;
&lt;p&gt;Last but not least, it is worthwhile to mention sparklyr 1.3.0 is known to be fully compatible with the recently released Spark 3.0. We highly recommend upgrading your copy of sparklyr to 1.3.0 if you plan to have Spark 3.0 as part of your data workflow in future.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="acknowledgement" class="section level2"&gt;
&lt;h2&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;In chronological order, we want to thank the following individuals for submitting pull requests towards sparklyr 1.3:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jozefhajnala"&gt;Jozef Hajnala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/falaki"&gt;Hossein Falaki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/samuelmacedo83"&gt;Samuel Macêdo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yl790"&gt;Yitao Li&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Loquats"&gt;Andy Zhang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/javierluraschi"&gt;Javier Luraschi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nealrichardson"&gt;Neal Richardson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are also grateful for valuable input on the sparklyr 1.3 roadmap, &lt;a href="https://github.com/sparklyr/sparklyr/pull/2434"&gt;#2434&lt;/a&gt;, and &lt;a href="https://github.com/sparklyr/sparklyr/pull/2551"&gt;#2551&lt;/a&gt; from &lt;span class="citation"&gt;[@javierluraschi]&lt;/span&gt;(&lt;a href="https://github.com/javierluraschi" class="uri"&gt;https://github.com/javierluraschi&lt;/a&gt;), and great spiritual advice on &lt;a href="https://github.com/sparklyr/sparklyr/issues/1773"&gt;#1773&lt;/a&gt; and &lt;a href="https://github.com/sparklyr/sparklyr/issues/2514"&gt;#2514&lt;/a&gt; from &lt;a href="https://github.com/mattpollock"&gt;@mattpollock&lt;/a&gt; and &lt;a href="https://github.com/benmwhite"&gt;@benmwhite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note if you believe you are missing from the acknowledgement above, it may be because your contribution has been considered part of the next sparklyr release rather than part of the current release. We do make every effort to ensure all contributors are mentioned in this section. In case you believe there is a mistake, please feel free to contact the author of this blog post via e-mail (yitao at rstudio dot com) and request a correction.&lt;/p&gt;
&lt;p&gt;If you wish to learn more about &lt;code&gt;sparklyr&lt;/code&gt;, we recommend visiting &lt;a href="https://sparklyr.ai"&gt;sparklyr.ai&lt;/a&gt;, &lt;a href="https://spark.rstudio.com"&gt;spark.rstudio.com&lt;/a&gt;, and some of the previous release posts such as &lt;a href="https://blogs.rstudio.com/ai/posts/2020-04-21-sparklyr-1.2.0-released/"&gt;sparklyr 1.2&lt;/a&gt; and &lt;a href="https://blog.rstudio.com/2020/01/29/sparklyr-1-1/"&gt;sparklyr 1.1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;/div&gt;




&lt;/div&gt;

&lt;script&gt;

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


&lt;/script&gt;

&lt;!-- tabsets --&gt;

&lt;script&gt;
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown &gt; .nav-tabs &gt; li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
&lt;/script&gt;

&lt;!-- code folding --&gt;


&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
&lt;/script&gt;

</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">09fc22263e7dc47a10599d02dd44ddb3</distill:md5>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</guid>
      <pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>tfhub: R interface to TensorFlow Hub</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</link>
      <description>TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</guid>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" medium="image" type="image/png" width="1365" height="909"/>
    </item>
    <item>
      <title>Getting started with Keras from R - the 2020 edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</link>
      <description>Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</guid>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020/images/website.png" medium="image" type="image/png" width="1591" height="725"/>
    </item>
    <item>
      <title>tfprobability 0.8 on CRAN: Now how can you use it?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</link>
      <description>Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.</description>
      <category>Probabilistic ML/DL</category>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</guid>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran/images/tfprobability.png" medium="image" type="image/png" width="518" height="600"/>
    </item>
    <item>
      <title>Innocent unicorns considered harmful? How to experiment with GPT-2 from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</link>
      <description>Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.</description>
      <category>Natural Language Processing</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</guid>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>TensorFlow 2.0 is here - what changes for R users?</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</link>
      <description>TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</guid>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/images/thumb.png" medium="image" type="image/png" width="400" height="400"/>
    </item>
    <item>
      <title>Auto-Keras: Tuning-free deep learning from R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Cruz Rodriguez</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</link>
      <description>Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</guid>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/images/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>lime v0.4: The Kitten Picture Edition</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Lin Pedersen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</link>
      <description>A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <category>Explainability</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</guid>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1344" height="672"/>
    </item>
    <item>
      <title>R Interface to Google CloudML</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</link>
      <description>We are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including on-demand access to training on GPUs and hyperparameter tuning to optimize key attributes of model architectures.</description>
      <category>Cloud</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</guid>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png" medium="image" type="image/png" width="394" height="211"/>
    </item>
    <item>
      <title>tfruns: Tools for TensorFlow Training Runs</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</link>
      <description>The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</guid>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns/preview.png" medium="image" type="image/png" width="2006" height="1116"/>
    </item>
    <item>
      <title>Keras for R</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</link>
      <description>We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</guid>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r/preview.png" medium="image" type="image/png" width="669" height="414"/>
    </item>
    <item>
      <title>TensorFlow Estimators</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yuan Tang</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</link>
      <description>The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</guid>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png" medium="image" type="image/png" width="1198" height="796"/>
    </item>
    <item>
      <title>TensorFlow v1.3 Released</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</link>
      <description>The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</guid>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png" medium="image" type="image/png" width="3876" height="741"/>
    </item>
  </channel>
</rss>
