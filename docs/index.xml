<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>RStudio AI Blog</title>
    <link>https://blogs.rstudio.com/tensorflow/</link>
    <atom:link href="https://blogs.rstudio.com/tensorflow/index.xml" rel="self" type="application/rss+xml"/>
    <description>News, concepts, and applications as regards deep learning, probabilistic computation, distributed computing and machine learning automation from R.
</description>
    <image>
      <title>RStudio AI Blog</title>
      <url>https://blogs.rstudio.com/tensorflow/images/favicon.png</url>
      <link>https://blogs.rstudio.com/tensorflow/</link>
    </image>
    <generator>Distill</generator>
    <lastBuildDate>Mon, 02 Nov 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>torch for tabular data</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular</link>
      <description>


&lt;p&gt;Machine learning on image-like data can be many things: fun (dogs vs. cats), societally useful (medical imaging), or societally harmful (surveillance). In comparison, tabular data – the bread and butter of data science – may seem more mundane.&lt;/p&gt;
&lt;p&gt;What’s more, if you’re particularly interested in deep learning (DL), and looking for the extra benefits to be gained from big data, big architectures, and big compute, you’re much more likely to build an impressive showcase on the former instead of the latter.&lt;/p&gt;
&lt;p&gt;So for tabular data, why not just go with random forests, or gradient boosting, or other classical methods? I can think of at least a few reasons to learn about DL for tabular data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Even if all your features are interval-scale or ordinal, thus requiring “just” some form of (not necessarily linear) regression, applying DL may result in performance benefits due to sophisticated optimization algorithms, activation functions, layer depth, and more (plus interactions of all of these).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If, in addition, there are categorical features, DL models may profit from &lt;em&gt;embedding&lt;/em&gt; those in continuous space, discovering similarities and relationships that go unnoticed in one-hot encoded representations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What if most features are numeric or categorical, but there’s also text in column F and an image in column G? With DL, different modalities can be worked on by different modules that feed their outputs into a common module, to take over from there.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="agenda"&gt;Agenda&lt;/h2&gt;
&lt;p&gt;In this introductory post, we keep the architecture straightforward. We don’t experiment with fancy optimizers or nonlinearities. Nor do we add in text or image processing. However, we do make use of embeddings, and pretty prominently at that. Thus from the above bullet list, we’ll shed a light on the second, while leaving the other two for future posts.&lt;/p&gt;
&lt;p&gt;In a nutshell, what we’ll see is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How to create a custom &lt;em&gt;dataset&lt;/em&gt;, tailored to the specific data you have.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to handle a mix of numeric and categorical data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to extract continuous-space representations from the embedding modules.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset, &lt;a href="https://archive.ics.uci.edu/ml/datasets/Mushroom"&gt;Mushrooms&lt;/a&gt;, was chosen for its abundance of categorical columns. It is an unusual dataset to use in DL: It was designed for machine learning models to infer logical rules, as in: IF &lt;em&gt;a&lt;/em&gt; AND NOT &lt;em&gt;b&lt;/em&gt; OR &lt;em&gt;c&lt;/em&gt; […], then it’s an &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Mushrooms are classified into two groups: edible and non-edible. The dataset description lists five possible rules with their resulting accuracies. While the least we want to go into here is the hotly debated topic of whether DL is suited to, or how it could be made more suited to rule learning, we’ll allow ourselves some curiosity and check out what happens if we successively remove all columns used to construct those five rules.&lt;/p&gt;
&lt;p&gt;Oh, and before you start copy-pasting: Here is the example in a &lt;a href="https://colab.research.google.com/drive/1bfyoD13YaLPLVcQVOLimv6vq9y4CKMM2?usp=sharing"&gt;Google Colaboratory notebook&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(purrr)
library(readr)
library(dplyr)
library(ggplot2)

download.file(
  &amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data&amp;quot;,
  destfile = &amp;quot;agaricus-lepiota.data&amp;quot;
)

mushroom_data &amp;lt;- read_csv(
  &amp;quot;agaricus-lepiota.data&amp;quot;,
  col_names = c(
    &amp;quot;poisonous&amp;quot;,
    &amp;quot;cap-shape&amp;quot;,
    &amp;quot;cap-surface&amp;quot;,
    &amp;quot;cap-color&amp;quot;,
    &amp;quot;bruises&amp;quot;,
    &amp;quot;odor&amp;quot;,
    &amp;quot;gill-attachment&amp;quot;,
    &amp;quot;gill-spacing&amp;quot;,
    &amp;quot;gill-size&amp;quot;,
    &amp;quot;gill-color&amp;quot;,
    &amp;quot;stalk-shape&amp;quot;,
    &amp;quot;stalk-root&amp;quot;,
    &amp;quot;stalk-surface-above-ring&amp;quot;,
    &amp;quot;stalk-surface-below-ring&amp;quot;,
    &amp;quot;stalk-color-above-ring&amp;quot;,
    &amp;quot;stalk-color-below-ring&amp;quot;,
    &amp;quot;veil-type&amp;quot;,
    &amp;quot;veil-color&amp;quot;,
    &amp;quot;ring-type&amp;quot;,
    &amp;quot;ring-number&amp;quot;,
    &amp;quot;spore-print-color&amp;quot;,
    &amp;quot;population&amp;quot;,
    &amp;quot;habitat&amp;quot;
  ),
  col_types = rep(&amp;quot;c&amp;quot;, 23) %&amp;gt;% paste(collapse = &amp;quot;&amp;quot;)
) %&amp;gt;%
  # can as well remove because there&amp;#39;s just 1 unique value
  select(-`veil-type`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;dataset()&lt;/code&gt; creates an R6 class. As with most R6 classes, there will usually be a need for an &lt;code&gt;initialize()&lt;/code&gt; method. Below, we use &lt;code&gt;initialize()&lt;/code&gt; to preprocess the data and store it in convenient pieces. More on that in a minute. Prior to that, please note the two other methods a &lt;code&gt;dataset&lt;/code&gt; has to implement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;.getitem(i)&lt;/code&gt; . This is the whole purpose of a &lt;code&gt;dataset&lt;/code&gt;: Retrieve and return the observation located at some index it is asked for. Which index? That’s to be decided by the caller, a &lt;code&gt;dataloader&lt;/code&gt;. During training, usually we want to permute the order in which observations are used, while not caring about order in case of validation or test data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;.length()&lt;/code&gt;. This method, again for use of a &lt;code&gt;dataloader&lt;/code&gt;, indicates how many observations there are.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, both methods are straightforward to implement. &lt;code&gt;.getitem(i)&lt;/code&gt; directly uses its argument to index into the data, and &lt;code&gt;.length()&lt;/code&gt; returns the number of observations:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;mushroom_dataset &amp;lt;- dataset(
  name = &amp;quot;mushroom_dataset&amp;quot;,

  initialize = function(indices) {
    data &amp;lt;- self$prepare_mushroom_data(mushroom_data[indices, ])
    self$xcat &amp;lt;- data[[1]][[1]]
    self$xnum &amp;lt;- data[[1]][[2]]
    self$y &amp;lt;- data[[2]]
  },

  .getitem = function(i) {
    xcat &amp;lt;- self$xcat[i, ]
    xnum &amp;lt;- self$xnum[i, ]
    y &amp;lt;- self$y[i, ]
    
    list(x = list(xcat, xnum), y = y)
  },
  
  .length = function() {
    dim(self$y)[1]
  },
  
  prepare_mushroom_data = function(input) {
    
    input &amp;lt;- input %&amp;gt;%
      mutate(across(.fns = as.factor)) 
    
    target_col &amp;lt;- input$poisonous %&amp;gt;% 
      as.integer() %&amp;gt;%
      `-`(1) %&amp;gt;%
      as.matrix()
    
    categorical_cols &amp;lt;- input %&amp;gt;% 
      select(-poisonous) %&amp;gt;%
      select(where(function(x) nlevels(x) != 2)) %&amp;gt;%
      mutate(across(.fns = as.integer)) %&amp;gt;%
      as.matrix()

    numerical_cols &amp;lt;- input %&amp;gt;%
      select(-poisonous) %&amp;gt;%
      select(where(function(x) nlevels(x) == 2)) %&amp;gt;%
      mutate(across(.fns = as.integer)) %&amp;gt;%
      as.matrix()
    
    list(list(torch_tensor(categorical_cols), torch_tensor(numerical_cols)),
         torch_tensor(target_col))
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As for data storage, there is a field for the target, &lt;code&gt;self$y&lt;/code&gt;, but instead of the expected &lt;code&gt;self$x&lt;/code&gt; we see separate fields for numerical features (&lt;code&gt;self$xnum&lt;/code&gt;) and categorical ones (&lt;code&gt;self$xcat&lt;/code&gt;). This is just for convenience: The latter will be passed into embedding modules, which require its inputs to be of type &lt;code&gt;torch_long()&lt;/code&gt;, as opposed to most other modules that, by default, work with &lt;code&gt;torch_float()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Accordingly, then, all &lt;code&gt;prepare_mushroom_data()&lt;/code&gt; does is break apart the data into those three parts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Indispensable aside:&lt;/em&gt; In this dataset, really &lt;em&gt;all&lt;/em&gt; features happen to be categorical – it’s just that for some, there are but two types. Technically, we could just have treated them the same as the non-binary features. But since normally in DL, we just leave binary features the way they are, we use this as an occasion to show how to handle a mix of various data types.&lt;/p&gt;
&lt;p&gt;Our custom &lt;code&gt;dataset&lt;/code&gt; defined, we create instances for training and validation; each gets its companion &lt;code&gt;dataloader&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_indices &amp;lt;- sample(1:nrow(mushroom_data), size = floor(0.8 * nrow(mushroom_data)))
valid_indices &amp;lt;- setdiff(1:nrow(mushroom_data), train_indices)

train_ds &amp;lt;- mushroom_dataset(train_indices)
train_dl &amp;lt;- train_ds %&amp;gt;% dataloader(batch_size = 256, shuffle = TRUE)

valid_ds &amp;lt;- mushroom_dataset(valid_indices)
valid_dl &amp;lt;- valid_ds %&amp;gt;% dataloader(batch_size = 256, shuffle = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="model"&gt;Model&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;torch&lt;/code&gt;, how much you &lt;em&gt;modularize&lt;/em&gt; your models is up to you. Often, high degrees of modularization enhance readability and help with troubleshooting.&lt;/p&gt;
&lt;p&gt;Here we factor out the embedding functionality. An &lt;code&gt;embedding_module&lt;/code&gt;, to be passed the categorical features only, will call &lt;code&gt;torch&lt;/code&gt;’s &lt;code&gt;nn_embedding()&lt;/code&gt; on each of them:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_module &amp;lt;- nn_module(
  
  initialize = function(cardinalities) {
    self$embeddings = nn_module_list(lapply(cardinalities, function(x) nn_embedding(num_embeddings = x, embedding_dim = ceiling(x/2))))
  },
  
  forward = function(x) {
    embedded &amp;lt;- vector(mode = &amp;quot;list&amp;quot;, length = length(self$embeddings))
    for (i in 1:length(self$embeddings)) {
      embedded[[i]] &amp;lt;- self$embeddings[[i]](x[ , i])
    }
    torch_cat(embedded, dim = 2)
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main model, when called, starts by embedding the categorical features, then appends the numerical input and continues processing:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;net &amp;lt;- nn_module(
  &amp;quot;mushroom_net&amp;quot;,

  initialize = function(cardinalities,
                        num_numerical,
                        fc1_dim,
                        fc2_dim) {
    self$embedder &amp;lt;- embedding_module(cardinalities)
    self$fc1 &amp;lt;- nn_linear(sum(map(cardinalities, function(x) ceiling(x/2)) %&amp;gt;% unlist()) + num_numerical, fc1_dim)
    self$fc2 &amp;lt;- nn_linear(fc1_dim, fc2_dim)
    self$output &amp;lt;- nn_linear(fc2_dim, 1)
  },

  forward = function(xcat, xnum) {
    embedded &amp;lt;- self$embedder(xcat)
    all &amp;lt;- torch_cat(list(embedded, xnum$to(dtype = torch_float())), dim = 2)
    all %&amp;gt;% self$fc1() %&amp;gt;%
      nnf_relu() %&amp;gt;%
      self$fc2() %&amp;gt;%
      self$output() %&amp;gt;%
      nnf_sigmoid()
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now instantiate this model, passing in, on the one hand, output sizes for the linear layers, and on the other, feature cardinalities. The latter will be used by the embedding modules to determine their output sizes, following a simple rule “embed into a space of size half the number of input values”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cardinalities &amp;lt;- map(
  mushroom_data[ , 2:ncol(mushroom_data)], compose(nlevels, as.factor)) %&amp;gt;%
  keep(function(x) x &amp;gt; 2) %&amp;gt;%
  unlist() %&amp;gt;%
  unname()

num_numerical &amp;lt;- ncol(mushroom_data) - length(cardinalities) - 1

fc1_dim &amp;lt;- 16
fc2_dim &amp;lt;- 16

model &amp;lt;- net(
  cardinalities,
  num_numerical,
  fc1_dim,
  fc2_dim
)

device &amp;lt;- if (cuda_is_available()) torch_device(&amp;quot;cuda:0&amp;quot;) else &amp;quot;cpu&amp;quot;

model &amp;lt;- model$to(device = device)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;The training loop now is “business as usual”:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters, lr = 0.1)

for (epoch in 1:20) {

  model$train()
  train_losses &amp;lt;- c()  

  for (b in enumerate(train_dl)) {
    optimizer$zero_grad()
    output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
    loss &amp;lt;- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))
    loss$backward()
    optimizer$step()
    train_losses &amp;lt;- c(train_losses, loss$item())
  }

  model$eval()
  valid_losses &amp;lt;- c()

  for (b in enumerate(valid_dl)) {
    output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
    loss &amp;lt;- nnf_binary_cross_entropy(output, b$y$to(dtype = torch_float(), device = device))
    valid_losses &amp;lt;- c(valid_losses, loss$item())
  }

  cat(sprintf(&amp;quot;Loss at epoch %d: training: %3f, validation: %3f\n&amp;quot;, epoch, mean(train_losses), mean(valid_losses)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: training: 0.274634, validation: 0.111689
Loss at epoch 2: training: 0.057177, validation: 0.036074
Loss at epoch 3: training: 0.025018, validation: 0.016698
Loss at epoch 4: training: 0.010819, validation: 0.010996
Loss at epoch 5: training: 0.005467, validation: 0.002849
Loss at epoch 6: training: 0.002026, validation: 0.000959
Loss at epoch 7: training: 0.000458, validation: 0.000282
Loss at epoch 8: training: 0.000231, validation: 0.000190
Loss at epoch 9: training: 0.000172, validation: 0.000144
Loss at epoch 10: training: 0.000120, validation: 0.000110
Loss at epoch 11: training: 0.000098, validation: 0.000090
Loss at epoch 12: training: 0.000079, validation: 0.000074
Loss at epoch 13: training: 0.000066, validation: 0.000064
Loss at epoch 14: training: 0.000058, validation: 0.000055
Loss at epoch 15: training: 0.000052, validation: 0.000048
Loss at epoch 16: training: 0.000043, validation: 0.000042
Loss at epoch 17: training: 0.000038, validation: 0.000038
Loss at epoch 18: training: 0.000034, validation: 0.000034
Loss at epoch 19: training: 0.000032, validation: 0.000031
Loss at epoch 20: training: 0.000028, validation: 0.000027&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While loss on the validation set is still decreasing, we’ll soon see that the network has learned enough to obtain an accuracy of 100%.&lt;/p&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To check classification accuracy, we re-use the validation set, seeing how we haven’t employed it for tuning anyway.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()

test_dl &amp;lt;- valid_ds %&amp;gt;% dataloader(batch_size = valid_ds$.length(), shuffle = FALSE)
iter &amp;lt;- test_dl$.iter()
b &amp;lt;- iter$.next()

output &amp;lt;- model(b$x[[1]]$to(device = device), b$x[[2]]$to(device = device))
preds &amp;lt;- output$to(device = &amp;quot;cpu&amp;quot;) %&amp;gt;% as.array()
preds &amp;lt;- ifelse(preds &amp;gt; 0.5, 1, 0)

comp_df &amp;lt;- data.frame(preds = preds, y = b[[2]] %&amp;gt;% as_array())
num_correct &amp;lt;- sum(comp_df$preds == comp_df$y)
num_total &amp;lt;- nrow(comp_df)
accuracy &amp;lt;- num_correct/num_total
accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Phew. No embarrassing failure for the DL approach on a task where straightforward rules are sufficient. Plus, we’ve really been parsimonious as to network size.&lt;/p&gt;
&lt;p&gt;Before concluding with an inspection of the learned embeddings, let’s have some fun obscuring things.&lt;/p&gt;
&lt;h2 id="making-the-task-harder"&gt;Making the task harder&lt;/h2&gt;
&lt;p&gt;The following rules (with accompanying accuracies) are reported in the dataset description.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Disjunctive rules for poisonous mushrooms, from most general
    to most specific:

    P_1) odor=NOT(almond.OR.anise.OR.none)
         120 poisonous cases missed, 98.52% accuracy

    P_2) spore-print-color=green
         48 cases missed, 99.41% accuracy
         
    P_3) odor=none.AND.stalk-surface-below-ring=scaly.AND.
              (stalk-color-above-ring=NOT.brown) 
         8 cases missed, 99.90% accuracy
         
    P_4) habitat=leaves.AND.cap-color=white
             100% accuracy     

    Rule P_4) may also be

    P_4&amp;#39;) population=clustered.AND.cap_color=white

    These rule involve 6 attributes (out of 22). &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, there’s no distinction being made between training and test sets; but we’ll stay with our 80:20 split anyway. We’ll successively remove all mentioned attributes, starting with the three that enabled 100% accuracy, and continuing our way up. Here are the results I obtained seeding the random number generator like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;torch_manual_seed(777)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width="90%" /&gt;
&lt;col width="9%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;without&lt;/th&gt;
&lt;th align="right"&gt;accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9994&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;&lt;code&gt;cap-color, population, habitat, stalk-surface-below-ring, stalk-color-above-ring, spore-print-color, odor&lt;/code&gt;&lt;/td&gt;
&lt;td align="right"&gt;0.9526&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Still 95% correct … While experiments like this are fun, it looks like they can also tell us something serious: Imagine the case of so-called “debiasing” by removing features like race, gender, or income. How many proxy variables may still be left that allow for inferring the masked attributes?&lt;/p&gt;
&lt;h2 id="a-look-at-the-hidden-representations"&gt;A look at the hidden representations&lt;/h2&gt;
&lt;p&gt;Looking at the weight matrix of an embedding module, what we see are the learned representations of a feature’s values. The first categorical column was &lt;code&gt;cap-shape&lt;/code&gt;; let’s extract its corresponding embeddings:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;embedding_weights &amp;lt;- vector(mode = &amp;quot;list&amp;quot;)
for (i in 1: length(model$embedder$embeddings)) {
  embedding_weights[[i]] &amp;lt;- model$embedder$embeddings[[i]]$parameters$weight$to(device = &amp;quot;cpu&amp;quot;)
}

cap_shape_repr &amp;lt;- embedding_weights[[1]]
cap_shape_repr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor
-0.0025 -0.1271  1.8077
-0.2367 -2.6165 -0.3363
-0.5264 -0.9455 -0.6702
 0.3057 -1.8139  0.3762
-0.8583 -0.7752  1.0954
 0.2740 -0.7513  0.4879
[ CPUFloatType{6,3} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of columns is three, since that’s what we chose when creating the embedding layer. The number of rows is six, matching the number of available categories. We may look up per-feature categories in the dataset description (&lt;em&gt;agaricus-lepiota.names&lt;/em&gt;):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;cap_shapes &amp;lt;- c(&amp;quot;bell&amp;quot;, &amp;quot;conical&amp;quot;, &amp;quot;convex&amp;quot;, &amp;quot;flat&amp;quot;, &amp;quot;knobbed&amp;quot;, &amp;quot;sunken&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For visualization, it’s convenient to do principal components analysis (but there are other options, like t-SNE). Here are the six cap shapes in two-dimensional space:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;pca &amp;lt;- prcomp(cap_shape_repr, center = TRUE, scale. = TRUE, rank = 2)$x[, c(&amp;quot;PC1&amp;quot;, &amp;quot;PC2&amp;quot;)]

pca %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(class = cap_shapes) %&amp;gt;%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point() +
  geom_label_repel(aes(label = class)) + 
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +
  theme(aspect.ratio = 1) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-11-03-torch-tabular/images/cap-shape.png" width="414" /&gt;&lt;/p&gt;
&lt;p&gt;Naturally, how interesting you find the results depends on how much you care about the hidden representation of a variable. Analyses like these may quickly turn into an activity where extreme caution is to be applied, as any biases in the data will immediately translate into biased representations. Moreover, reduction to two-dimensional space may or may not be adequate.&lt;/p&gt;
&lt;p&gt;This concludes our introduction to &lt;code&gt;torch&lt;/code&gt; for tabular data. While the conceptual focus was on categorical features, and how to make use of them in combination with numerical ones, we’ve taken care to also provide background on something that will come up time and again: defining a &lt;code&gt;dataset&lt;/code&gt; tailored to the task at hand.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>a8004a0d82a0fec874811441db208cbd</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular</guid>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-11-03-torch-tabular/images/preview.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classifying images with torch</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification</link>
      <description>


&lt;p&gt;In recent posts, we’ve been exploring essential &lt;code&gt;torch&lt;/code&gt; functionality: &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch/"&gt;tensors&lt;/a&gt;, the sine qua non of every deep learning framework; &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;autograd&lt;/a&gt;, &lt;code&gt;torch&lt;/code&gt;’s implementation of reverse-mode automatic differentiation; &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-07-torch-modules"&gt;modules&lt;/a&gt;, composable building blocks of neural networks; and &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-09-torch-optim/"&gt;optimizers&lt;/a&gt;, the – well – optimization algorithms that &lt;code&gt;torch&lt;/code&gt; provides.&lt;/p&gt;
&lt;p&gt;But we haven’t really had our “hello world” moment yet, at least not if by “hello world” you mean the inevitable &lt;em&gt;deep learning experience of classifying pets&lt;/em&gt;. Cat or dog? Beagle or boxer? Chinook or Chihuahua? We’ll distinguish ourselves by asking a (slightly) different question: What kind of bird?&lt;/p&gt;
&lt;p&gt;Topics we’ll address on our way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The core roles of &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;datasets&lt;/em&gt; and &lt;em&gt;data loaders&lt;/em&gt;, respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to apply &lt;code&gt;transform&lt;/code&gt;s, both for image preprocessing and data augmentation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to use Resnet &lt;span class="citation"&gt;(He et al. 2015)&lt;/span&gt;, a pre-trained model that comes with &lt;code&gt;torchvision&lt;/code&gt;, for transfer learning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to use learning rate schedulers, and in particular, the one-cycle learning rate algorithm [@abs-1708-07120].&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to find a good initial learning rate.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For convenience, the code is available on &lt;a href="https://colab.research.google.com/drive/1OJzzqiQVbh3ZdLB2L2t_DhBGInlh9o-k?usp=sharing"&gt;Google Colaboratory&lt;/a&gt; – no copy-pasting required.&lt;/p&gt;
&lt;h2 id="data-loading-and-preprocessing"&gt;Data loading and preprocessing&lt;/h2&gt;
&lt;p&gt;The example dataset used here is available on &lt;a href="https://www.kaggle.com/gpiosenka/100-bird-species/data" class="uri"&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Conveniently, it may be obtained using &lt;a href="https://github.com/mlverse/torchdatasets"&gt;&lt;code&gt;torchdatasets&lt;/code&gt;&lt;/a&gt;, which uses &lt;a href="https://github.com/rstudio/pins"&gt;&lt;code&gt;pins&lt;/code&gt;&lt;/a&gt; for authentication, retrieval and storage. To enable &lt;code&gt;pins&lt;/code&gt; to manage your Kaggle downloads, please follow the instructions &lt;a href="https://pins.rstudio.com/articles/boards-kaggle.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This dataset is very “clean,” unlike the images we may be used to from, e.g., &lt;a href="http://image-net.org/"&gt;ImageNet&lt;/a&gt;. To help with generalization, we introduce noise during training – in other words, we perform &lt;em&gt;data augmentation&lt;/em&gt;. In &lt;code&gt;torchvision&lt;/code&gt;, data augmentation is part of an &lt;em&gt;image processing pipeline&lt;/em&gt; that first converts an image to a tensor, and then applies any transformations such as resizing, cropping, normalization, or various forms of distorsion.&lt;/p&gt;
&lt;p&gt;Below are the transformations performed on the training set. Note how most of them are for data augmentation, while normalization is done to comply with what’s expected by ResNet.&lt;/p&gt;
&lt;h4 id="image-preprocessing-pipeline"&gt;Image preprocessing pipeline&lt;/h4&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
library(torchvision)
library(torchdatasets)

library(dplyr)
library(pins)
library(ggplot2)

device &amp;lt;- if (cuda_is_available()) torch_device(&amp;quot;cuda:0&amp;quot;) else &amp;quot;cpu&amp;quot;

train_transforms &amp;lt;- function(img) {
  img %&amp;gt;%
    # first convert image to tensor
    transform_to_tensor() %&amp;gt;%
    # then move to the GPU (if available)
    (function(x) x$to(device = device)) %&amp;gt;%
    # data augmentation
    transform_random_resized_crop(size = c(224, 224)) %&amp;gt;%
    # data augmentation
    transform_color_jitter() %&amp;gt;%
    # data augmentation
    transform_random_horizontal_flip() %&amp;gt;%
    # normalize according to what is expected by resnet
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the validation set, we don’t want to introduce noise, but still need to resize, crop, and normalize the images. The test set should be treated identically.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;valid_transforms &amp;lt;- function(img) {
  img %&amp;gt;%
    transform_to_tensor() %&amp;gt;%
    (function(x) x$to(device = device)) %&amp;gt;%
    transform_resize(256) %&amp;gt;%
    transform_center_crop(224) %&amp;gt;%
    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225))
}

test_transforms &amp;lt;- valid_transforms&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, let’s get the data, nicely divided into training, validation and test sets. Additionally, we tell the corresponding R objects what transformations they’re expected to apply:&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, download = TRUE, transform = train_transforms)

valid_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, split = &amp;quot;valid&amp;quot;, transform = valid_transforms)

test_ds &amp;lt;- bird_species_dataset(&amp;quot;data&amp;quot;, split = &amp;quot;test&amp;quot;, transform = test_transforms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two things to note. First, transformations are part of the &lt;em&gt;dataset&lt;/em&gt; concept, as opposed to the &lt;em&gt;data loader&lt;/em&gt; we’ll encounter shortly. Second, let’s take a look at how the images have been stored on disk. The overall directory structure (starting from &lt;code&gt;data&lt;/code&gt;, which we specified as the root directory to be used) is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/bird_species/train
data/bird_species/valid
data/bird_species/test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;valid&lt;/code&gt;, and &lt;code&gt;test&lt;/code&gt; directories, different classes of images reside in their own folders. For example, here is the directory layout for the first three classes in the test set:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/bird_species/test/ALBATROSS/
 - data/bird_species/test/ALBATROSS/1.jpg
 - data/bird_species/test/ALBATROSS/2.jpg
 - data/bird_species/test/ALBATROSS/3.jpg
 - data/bird_species/test/ALBATROSS/4.jpg
 - data/bird_species/test/ALBATROSS/5.jpg
 
data/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/1.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/2.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/3.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/4.jpg
 - data/bird_species/test/&amp;#39;ALEXANDRINE PARAKEET&amp;#39;/5.jpg
 
 data/test/&amp;#39;AMERICAN BITTERN&amp;#39;/
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/1.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/2.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/3.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/4.jpg
 - data/bird_species/test/&amp;#39;AMERICAN BITTERN&amp;#39;/5.jpg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is exactly the kind of layout expected by &lt;code&gt;torch&lt;/code&gt;s &lt;code&gt;image_folder_dataset()&lt;/code&gt; – and really &lt;code&gt;bird_species_dataset()&lt;/code&gt; instantiates a subtype of this class. Had we downloaded the data manually, respecting the required directory structure, we could have created the datasets like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# e.g.
train_ds &amp;lt;- image_folder_dataset(
  file.path(data_dir, &amp;quot;train&amp;quot;),
  transform = train_transforms)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we got the data, let’s see how many items there are in each set.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_ds$.length()
valid_ds$.length()
test_ds$.length()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;31316
1125
1125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That training set is really big! It’s thus recommended to run this on GPU, or just play around with the provided Colab notebook.&lt;/p&gt;
&lt;p&gt;With so many samples, we’re curious how many classes there are.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;class_names &amp;lt;- test_ds$classes
length(class_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we &lt;em&gt;do&lt;/em&gt; have a substantial training set, but the task is formidable as well: We’re going to tell apart no less than 225 different bird species.&lt;/p&gt;
&lt;h4 id="data-loaders"&gt;Data loaders&lt;/h4&gt;
&lt;p&gt;While &lt;em&gt;datasets&lt;/em&gt; know what to do with each single item, &lt;em&gt;data loaders&lt;/em&gt; know how to treat them collectively. How many samples make up a batch? Do we want to feed them in the same order always, or instead, have a different order chosen for every epoch?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch_size &amp;lt;- 64

train_dl &amp;lt;- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)
valid_dl &amp;lt;- dataloader(valid_ds, batch_size = batch_size)
test_dl &amp;lt;- dataloader(test_ds, batch_size = batch_size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data loaders, too, may be queried for their length. Now length means: How many batches?&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_dl$.length() 
valid_dl$.length() 
test_dl$.length()  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;490
18
18&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="some-birds"&gt;Some birds&lt;/h4&gt;
&lt;p&gt;Next, let’s view a few images from the test set. We can retrieve the first batch – images and corresponding classes – by creating an iterator from the &lt;code&gt;dataloader&lt;/code&gt; and calling &lt;code&gt;next()&lt;/code&gt; on it:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# for display purposes, here we are actually using a batch_size of 24
batch &amp;lt;- train_dl$.iter()$.next()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;batch&lt;/code&gt; is a list, the first item being the image tensors:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch[[1]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  24   3 224 224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the second, the classes:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;batch[[2]]$size()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Classes are coded as integers, to be used as indices in a vector of class names. We’ll use those for labeling the images.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;classes &amp;lt;- batch[[2]]
classes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1
 1
 1
 1
 1
 2
 2
 2
 2
 2
 3
 3
 3
 3
 3
 4
 4
 4
 4
 4
 5
 5
 5
 5
[ GPULongType{24} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The image tensors have shape &lt;code&gt;batch_size x num_channels x height x width&lt;/code&gt;. For plotting using &lt;code&gt;as.raster()&lt;/code&gt;, we need to reshape the images such that channels come last. We also undo the normalization applied by the &lt;code&gt;dataloader&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here are the first twenty-four images:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)

images &amp;lt;- as_array(batch[[1]]) %&amp;gt;% aperm(perm = c(1, 3, 4, 2))
mean &amp;lt;- c(0.485, 0.456, 0.406)
std &amp;lt;- c(0.229, 0.224, 0.225)
images &amp;lt;- std * images + mean
images &amp;lt;- images * 255
images[images &amp;gt; 255] &amp;lt;- 255
images[images &amp;lt; 0] &amp;lt;- 0

par(mfcol = c(4,6), mar = rep(1, 4))

images %&amp;gt;%
  purrr::array_tree(1) %&amp;gt;%
  purrr::set_names(class_names[as_array(classes)]) %&amp;gt;%
  purrr::map(as.raster, max = 255) %&amp;gt;%
  purrr::iwalk(~{plot(.x); title(.y)})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/image_classif_birds.png" width="250" /&gt;&lt;/p&gt;
&lt;h2 id="model"&gt;Model&lt;/h2&gt;
&lt;p&gt;The backbone of our model is a pre-trained instance of ResNet.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model_resnet18(pretrained = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we want to distinguish among our 225 bird species, while ResNet was trained on 1000 different classes. What can we do? We simply replace the output layer.&lt;/p&gt;
&lt;p&gt;The new output layer is also the only one whose weights we are going to train – leaving all other ResNet parameters the way they are. Technically, we &lt;em&gt;could&lt;/em&gt; perform backpropagation through the complete model, striving to fine-tune ResNet’s weights as well. However, this would slow down training significantly. In fact, the choice is not all-or-none: It is up to us how many of the original parameters to keep fixed, and how many to “set free” for fine tuning. For the task at hand, we’ll be content to just train the newly added output layer: With the abundance of animals, including birds, in ImageNet, we expect the trained ResNet to know a lot about them!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$parameters %&amp;gt;% purrr::walk(function(param) param$requires_grad_(FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To replace the output layer, the model is modified in-place:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_features &amp;lt;- model$fc$in_features

model$fc &amp;lt;- nn_linear(in_features = num_features, out_features = length(class_names))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now put the modified model on the GPU (if available):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model$to(device = device)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="training"&gt;Training&lt;/h2&gt;
&lt;p&gt;For optimization, we use cross entropy loss and stochastic gradient descent.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;criterion &amp;lt;- nn_cross_entropy_loss()

optimizer &amp;lt;- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="finding-an-optimally-efficient-learning-rate"&gt;Finding an optimally efficient learning rate&lt;/h4&gt;
&lt;p&gt;We set the learning rate to &lt;code&gt;0.1&lt;/code&gt;, but that is just a formality. As has become widely known due to the excellent lectures by &lt;a href="http://fast.ai"&gt;fast.ai&lt;/a&gt;, it makes sense to spend some time upfront to determine an efficient learning rate. While out-of-the-box, &lt;code&gt;torch&lt;/code&gt; does not provide a tool like fast.ai’s learning rate finder, the logic is straightforward to implement. Here’s how to find a good learning rate, as translated to R from &lt;a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"&gt;Sylvain Gugger’s post&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# ported from: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html

losses &amp;lt;- c()
log_lrs &amp;lt;- c()

find_lr &amp;lt;- function(init_value = 1e-8, final_value = 10, beta = 0.98) {

  num &amp;lt;- train_dl$.length()
  mult = (final_value/init_value)^(1/num)
  lr &amp;lt;- init_value
  optimizer$param_groups[[1]]$lr &amp;lt;- lr
  avg_loss &amp;lt;- 0
  best_loss &amp;lt;- 0
  batch_num &amp;lt;- 0

  for (b in enumerate(train_dl)) {

    batch_num &amp;lt;- batch_num + 1
    optimizer$zero_grad()
    output &amp;lt;- model(b[[1]]$to(device = device))
    loss &amp;lt;- criterion(output, b[[2]]$to(device = device))

    #Compute the smoothed loss
    avg_loss &amp;lt;- beta * avg_loss + (1-beta) * loss$item()
    smoothed_loss &amp;lt;- avg_loss / (1 - beta^batch_num)
    #Stop if the loss is exploding
    if (batch_num &amp;gt; 1 &amp;amp;&amp;amp; smoothed_loss &amp;gt; 4 * best_loss) break
    #Record the best loss
    if (smoothed_loss &amp;lt; best_loss || batch_num == 1) best_loss &amp;lt;- smoothed_loss

    #Store the values
    losses &amp;lt;&amp;lt;- c(losses, smoothed_loss)
    log_lrs &amp;lt;&amp;lt;- c(log_lrs, (log(lr, 10)))

    loss$backward()
    optimizer$step()

    #Update the lr for the next step
    lr &amp;lt;- lr * mult
    optimizer$param_groups[[1]]$lr &amp;lt;- lr
  }
}

find_lr()

df &amp;lt;- data.frame(log_lrs = log_lrs, losses = losses)
ggplot(df, aes(log_lrs, losses)) + geom_point(size = 1) + theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/lr_finder.png" width="372" /&gt;&lt;/p&gt;
&lt;p&gt;The best learning rate is not the exact one where loss is at a minimum. Instead, it should be picked somewhat earlier on the curve, while loss is still decreasing. &lt;code&gt;0.05&lt;/code&gt; looks like a sensible choice.&lt;/p&gt;
&lt;p&gt;This value is nothing but an anchor, however. &lt;em&gt;Learning rate schedulers&lt;/em&gt; allow learning rates to evolve according to some proven algorithm. Among others, &lt;code&gt;torch&lt;/code&gt; implements one-cycle learning [@abs-1708-07120], cyclical learning rates &lt;span class="citation"&gt;(Smith 2015)&lt;/span&gt;, and cosine annealing with warm restarts &lt;span class="citation"&gt;(Loshchilov and Hutter 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here, we use &lt;code&gt;lr_one_cycle()&lt;/code&gt;, passing in our newly found, optimally efficient, hopefully, value &lt;code&gt;0.05&lt;/code&gt; as a maximum learning rate. &lt;code&gt;lr_one_cycle()&lt;/code&gt; will start with a low rate, then gradually ramp up until it reaches the allowed maximum. After that, the learning rate will slowly, continuously decrease, until it falls slightly below its initial value.&lt;/p&gt;
&lt;p&gt;All this happens not per epoch, but exactly once, which is why the name has &lt;code&gt;one_cycle&lt;/code&gt; in it. Here’s how the evolution of learning rates looks in our example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blogs.rstudio.com/tensorflow//posts/2020-10-19-torch-image-classification/images/one_cycle_lr.png" width="315" /&gt;&lt;/p&gt;
&lt;p&gt;Before we start training, let’s quickly re-initialize the model, so as to start from a clean slate:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- model_resnet18(pretrained = TRUE)
model$parameters %&amp;gt;% purrr::walk(function(param) param$requires_grad_(FALSE))

num_features &amp;lt;- model$fc$in_features

model$fc &amp;lt;- nn_linear(in_features = num_features, out_features = length(class_names))

model &amp;lt;- model$to(device = device)

criterion &amp;lt;- nn_cross_entropy_loss()

optimizer &amp;lt;- optim_sgd(model$parameters, lr = 0.05, momentum = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And instantiate the scheduler:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;num_epochs = 10

scheduler &amp;lt;- optimizer %&amp;gt;% 
  lr_one_cycle(max_lr = 0.05, epochs = num_epochs, steps_per_epoch = train_dl$.length())&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="training-loop"&gt;Training loop&lt;/h4&gt;
&lt;p&gt;Now we train for ten epochs. For every training batch, we call &lt;code&gt;scheduler$step()&lt;/code&gt; to adjust the learning rate. Notably, this has to be done &lt;em&gt;after&lt;/em&gt; &lt;code&gt;optimizer$step()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_batch &amp;lt;- function(b) {

  optimizer$zero_grad()
  output &amp;lt;- model(b[[1]])
  loss &amp;lt;- criterion(output, b[[2]]$to(device = device))
  loss$backward()
  optimizer$step()
  scheduler$step()
  loss$item()

}

valid_batch &amp;lt;- function(b) {

  output &amp;lt;- model(b[[1]])
  loss &amp;lt;- criterion(output, b[[2]]$to(device = device))
  loss$item()
}

for (epoch in 1:num_epochs) {

  model$train()
  train_losses &amp;lt;- c()

  for (b in enumerate(train_dl)) {
    loss &amp;lt;- train_batch(b)
    train_losses &amp;lt;- c(train_losses, loss)
  }

  model$eval()
  valid_losses &amp;lt;- c()

  for (b in enumerate(valid_dl)) {
    loss &amp;lt;- valid_batch(b)
    valid_losses &amp;lt;- c(valid_losses, loss)
  }

  cat(sprintf(&amp;quot;\nLoss at epoch %d: training: %3f, validation: %3f\n&amp;quot;, epoch, mean(train_losses), mean(valid_losses)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loss at epoch 1: training: 2.662901, validation: 0.790769

Loss at epoch 2: training: 1.543315, validation: 1.014409

Loss at epoch 3: training: 1.376392, validation: 0.565186

Loss at epoch 4: training: 1.127091, validation: 0.575583

Loss at epoch 5: training: 0.916446, validation: 0.281600

Loss at epoch 6: training: 0.775241, validation: 0.215212

Loss at epoch 7: training: 0.639521, validation: 0.151283

Loss at epoch 8: training: 0.538825, validation: 0.106301

Loss at epoch 9: training: 0.407440, validation: 0.083270

Loss at epoch 10: training: 0.354659, validation: 0.080389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the model made good progress, but we don’t yet know anything about classification accuracy in absolute terms. We’ll check that out on the test set.&lt;/p&gt;
&lt;h2 id="test-set-accuracy"&gt;Test set accuracy&lt;/h2&gt;
&lt;p&gt;Finally, we calculate accuracy on the test set:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$eval()

test_batch &amp;lt;- function(b) {

  output &amp;lt;- model(b[[1]])
  labels &amp;lt;- b[[2]]$to(device = device)
  loss &amp;lt;- criterion(output, labels)
  
  test_losses &amp;lt;&amp;lt;- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values
  # and position 2 containing the respective indices
  predicted &amp;lt;- torch_max(output$data(), dim = 2)[[2]]
  total &amp;lt;&amp;lt;- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct &amp;lt;&amp;lt;- correct + (predicted == labels)$sum()$item()

}

test_losses &amp;lt;- c()
total &amp;lt;- 0
correct &amp;lt;- 0

for (b in enumerate(test_dl)) {
  test_batch(b)
}

mean(test_losses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.03719&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_accuracy &amp;lt;-  correct/total
test_accuracy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.98756&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An impressive result, given how many different species there are!&lt;/p&gt;
&lt;h2 id="wrapup"&gt;Wrapup&lt;/h2&gt;
&lt;p&gt;Hopefully, this has been a useful introduction to classifying images with &lt;code&gt;torch&lt;/code&gt;, as well as to its non-domain-specific architectural elements, like datasets, data loaders, and learning-rate schedulers. Future posts will explore other domains, as well as move on beyond “hello world” in image recognition. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-HeZRS15" class="csl-entry"&gt;
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. &lt;span&gt;“Deep Residual Learning for Image Recognition.”&lt;/span&gt; &lt;em&gt;CoRR&lt;/em&gt; abs/1512.03385. &lt;a href="http://arxiv.org/abs/1512.03385"&gt;http://arxiv.org/abs/1512.03385&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-LoshchilovH16a" class="csl-entry"&gt;
Loshchilov, Ilya, and Frank Hutter. 2016. &lt;span&gt;“&lt;span&gt;SGDR:&lt;/span&gt; Stochastic Gradient Descent with Restarts.”&lt;/span&gt; &lt;em&gt;CoRR&lt;/em&gt; abs/1608.03983. &lt;a href="http://arxiv.org/abs/1608.03983"&gt;http://arxiv.org/abs/1608.03983&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-Smith15a" class="csl-entry"&gt;
Smith, Leslie N. 2015. &lt;span&gt;“No More Pesky Learning Rate Guessing Games.”&lt;/span&gt; &lt;em&gt;CoRR&lt;/em&gt; abs/1506.01186. &lt;a href="http://arxiv.org/abs/1506.01186"&gt;http://arxiv.org/abs/1506.01186&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Physically, the dataset consists of a single &lt;code&gt;zip&lt;/code&gt; file; so it is really the first instruction that downloads all the data. The remaining two function calls perform semantic mappings only.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>99e215574cdffd1c16282428321ce8e7</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification</guid>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-19-torch-image-classification/images/image_classif_birds.png" medium="image" type="image/png" width="500" height="333"/>
    </item>
    <item>
      <title>sparklyr.flint 0.2: ASOF Joins, OLS Regression, and additional summarizers</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</link>
      <description>


&lt;p&gt;Since &lt;a href="https://cran.r-project.org/web/packages/sparklyr.flint/index.html"&gt;&lt;code&gt;sparklyr.flint&lt;/code&gt;&lt;/a&gt;, a &lt;a href="https://sparklyr.ai"&gt;&lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt; extension for leveraging &lt;a href="https://github.com/twosigma/flint"&gt;Flint&lt;/a&gt; time series functionalities through &lt;code&gt;sparklyr&lt;/code&gt;, was &lt;a href="https://blogs.rstudio.com/ai/posts/2020-09-07-sparklyr-flint"&gt;introduced&lt;/a&gt; in September, we have made a number of enhancements to it, and have successfully submitted &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 to CRAN.&lt;/p&gt;
&lt;p&gt;In this blog post, we highlight the following new features and improvements from &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#asof-joins"&gt;ASOF Joins&lt;/a&gt; of Timeseries RDDs&lt;/li&gt;
&lt;li&gt;&lt;a href="#ols-regression"&gt;OLS Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#additional-summarizers"&gt;Additional Summarizers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="asof-joins"&gt;ASOF Joins&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the term, ASOF joins are temporal join operations based on inexact matching of timestamps. Within the context of &lt;a href="https://spark.apache.org"&gt;Apache Spark&lt;/a&gt;, a join operation, loosely speaking, matches records from two data frames (let’s call them &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;) based on some criteria. A temporal join implies matching records in &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; based on timestamps, and with inexact matching of timestamps permitted, it is typically useful to join &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; along one of the following temporal directions:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Looking behind: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t&lt;/code&gt;, then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the most recent timestamp less than or equal to &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Looking ahead: if a record from &lt;code&gt;left&lt;/code&gt; has timestamp &lt;code&gt;t,&lt;/code&gt; then it gets matched with ones from &lt;code&gt;right&lt;/code&gt; having the smallest timestamp greater than or equal to (or alternatively, strictly greater than) &lt;code&gt;t&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, oftentimes it is not useful to consider two timestamps as “matching” if they are too far apart. Therefore, an additional constraint on the maximum amount of time to look behind or look ahead is usually also part of an ASOF join operation.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, all ASOF join functionalities of Flint are accessible via the &lt;code&gt;asof_join()&lt;/code&gt; method. For example, given 2 timeseries RDDs &lt;code&gt;left&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(sparklyr)
library(sparklyr.flint)

sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)
left &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10), u = seq(10))) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)
right &amp;lt;- copy_to(sc, tibble::tibble(t = seq(10) + 1, v = seq(10) + 1L)) %&amp;gt;%
  from_sdf(is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;, time_column = &amp;quot;t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following prints the result of matching each record from &lt;code&gt;left&lt;/code&gt; with the most recent record(s) from &lt;code&gt;right&lt;/code&gt; that are at most 1 second behind.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;gt;=&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1    NA
##  2 1970-01-01 00:00:02     2     2
##  3 1970-01-01 00:00:03     3     3
##  4 1970-01-01 00:00:04     4     4
##  5 1970-01-01 00:00:05     5     5
##  6 1970-01-01 00:00:06     6     6
##  7 1970-01-01 00:00:07     7     7
##  8 1970-01-01 00:00:08     8     8
##  9 1970-01-01 00:00:09     9     9
## 10 1970-01-01 00:00:10    10    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas if we change the temporal direction to “&amp;lt;”, then each record from &lt;code&gt;left&lt;/code&gt; will be matched with any record(s) from &lt;code&gt;right&lt;/code&gt; that is strictly in the future and is at most 1 second ahead of the current record from &lt;code&gt;left&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(asof_join(left, right, tol = &amp;quot;1s&amp;quot;, direction = &amp;quot;&amp;lt;&amp;quot;) %&amp;gt;% to_sdf())

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##    time                    u     v
##    &amp;lt;dttm&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 1970-01-01 00:00:01     1     2
##  2 1970-01-01 00:00:02     2     3
##  3 1970-01-01 00:00:03     3     4
##  4 1970-01-01 00:00:04     4     5
##  5 1970-01-01 00:00:05     5     6
##  6 1970-01-01 00:00:06     6     7
##  7 1970-01-01 00:00:07     7     8
##  8 1970-01-01 00:00:08     8     9
##  9 1970-01-01 00:00:09     9    10
## 10 1970-01-01 00:00:10    10    11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice regardless of which temporal direction is selected, an outer-left join is always performed (i.e., all timestamp values and &lt;code&gt;u&lt;/code&gt; values of &lt;code&gt;left&lt;/code&gt; from above will always be present in the output, and the &lt;code&gt;v&lt;/code&gt; column in the output will contain &lt;code&gt;NA&lt;/code&gt; whenever there is no record from &lt;code&gt;right&lt;/code&gt; that meets the matching criteria).&lt;/p&gt;
&lt;h2 id="ols-regression"&gt;OLS Regression&lt;/h2&gt;
&lt;p&gt;You might be wondering whether the version of this functionality in Flint is more or less identical to &lt;code&gt;lm()&lt;/code&gt; in R. Turns out it has much more to offer than &lt;code&gt;lm()&lt;/code&gt; does. An OLS regression in Flint will compute useful metrics such as &lt;a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"&gt;Akaike information criterion&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion"&gt;Bayesian information criterion&lt;/a&gt;, both of which are useful for model selection purposes, and the calculations of both are parallelized by Flint to fully utilize computational power available in a Spark cluster. In addition, Flint supports ignoring regressors that are constant or nearly constant, which becomes useful when an intercept term is included. To see why this is the case, we need to briefly examine the goal of the OLS regression, which is to find some column vector of coefficients &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt; that minimizes &lt;span class="math inline"&gt;\(\|\mathbf{y} - \mathbf{X} \mathbf{\beta}\|^2\)&lt;/span&gt;, where &lt;span class="math inline"&gt;\(\mathbf{y}\)&lt;/span&gt; is the column vector of response variables, and &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is a matrix consisting of columns of regressors plus an entire column of &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s representing the intercept terms. The solution to this problem is &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt;, assuming the Gram matrix &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; is non-singular. However, if &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; contains a column of all &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;s of intercept terms, and another column formed by a regressor that is constant (or nearly so), then columns of &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; will be linearly dependent (or nearly so) and &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; will be singular (or nearly so), which presents an issue computation-wise. However, if a regressor is constant, then it essentially plays the same role as the intercept terms do. So simply excluding such a constant regressor in &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; solves the problem. Also, speaking of inverting the Gram matrix, readers remembering the concept of “condition number” from numerical analysis must be thinking to themselves how computing &lt;span class="math inline"&gt;\(\mathbf{\beta} = (\mathbf{X}^\intercal\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{y}\)&lt;/span&gt; could be numerically unstable if &lt;span class="math inline"&gt;\(\mathbf{X}^\intercal\mathbf{X}\)&lt;/span&gt; has a large condition number. This is why Flint also outputs the condition number of the Gram matrix in the OLS regression result, so that one can sanity-check the underlying quadratic minimization problem being solved is well-conditioned.&lt;/p&gt;
&lt;p&gt;So, to summarize, the OLS regression functionality implemented in Flint not only outputs the solution to the problem, but also calculates useful metrics that help data scientists assess the sanity and predictive quality of the resulting model.&lt;/p&gt;
&lt;p&gt;To see OLS regression in action with &lt;code&gt;sparklyr.flint&lt;/code&gt;, one can run the following example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mtcars_sdf &amp;lt;- copy_to(sc, mtcars, overwrite = TRUE) %&amp;gt;%
  dplyr::mutate(time = 0L)
mtcars_ts &amp;lt;- from_sdf(mtcars_sdf, is_sorted = TRUE, time_unit = &amp;quot;SECONDS&amp;quot;)
model &amp;lt;- ols_regression(mtcars_ts, mpg ~ hp + wt) %&amp;gt;% to_sdf()

print(model %&amp;gt;% dplyr::select(akaikeIC, bayesIC, cond))

## # Source: spark&amp;lt;?&amp;gt; [?? x 3]
##   akaikeIC bayesIC    cond
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     155.    159. 345403.

# ^ output says condition number of the Gram matrix was within reason&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and obtain &lt;span class="math inline"&gt;\(\mathbf{\beta}\)&lt;/span&gt;, the vector of optimal coefficients, with the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(model %&amp;gt;% dplyr::pull(beta))

## [[1]]
## [1] -0.03177295 -3.87783074&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="additional-summarizers"&gt;Additional Summarizers&lt;/h2&gt;
&lt;p&gt;The EWMA (Exponential Weighted Moving Average), EMA half-life, and the standardized moment summarizers (namely, skewness and kurtosis) along with a few others which were missing in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 are now fully supported in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2.&lt;/p&gt;
&lt;h2 id="better-integration-with-sparklyr"&gt;Better Integration With &lt;code&gt;sparklyr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;While &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.1 included a &lt;code&gt;collect()&lt;/code&gt; method for exporting data from a Flint time-series RDD to an R data frame, it did not have a similar method for extracting the underlying Spark data frame from a Flint time-series RDD. This was clearly an oversight. In &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2, one can call &lt;code&gt;to_sdf()&lt;/code&gt; on a timeseries RDD to get back a Spark data frame that is usable in &lt;code&gt;sparklyr&lt;/code&gt; (e.g., as shown by &lt;code&gt;model %&amp;gt;% to_sdf() %&amp;gt;% dplyr::select(...)&lt;/code&gt; examples from above). One can also get to the underlying Spark data frame JVM object reference by calling &lt;code&gt;spark_dataframe()&lt;/code&gt; on a Flint time-series RDD (this is usually unnecessary in vast majority of &lt;code&gt;sparklyr&lt;/code&gt; use cases though).&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have presented a number of new features and improvements introduced in &lt;code&gt;sparklyr.flint&lt;/code&gt; 0.2 and deep-dived into some of them in this blog post. We hope you are as excited about them as we are.&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
&lt;h2 id="acknowledgement"&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;The author would like to thank Mara (&lt;a href="https://github.com/batpigandme"&gt;@batpigandme&lt;/a&gt;), Sigrid (&lt;a href="https://github.com/skeydan"&gt;@skeydan&lt;/a&gt;), and Javier (&lt;a href="https://github.com/javierluraschi"&gt;@javierluraschi&lt;/a&gt;) for their fantastic editorial inputs on this blog post!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>21fc4b2e51e9d4a153f2dae536e7279b</distill:md5>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released</guid>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-12-sparklyr-flint-0.2.0-released/images/sparklyr-flint-0.2.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Optimizers in torch</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim</link>
      <description>


&lt;p&gt;This is the fourth and last installment in a series introducing &lt;code&gt;torch&lt;/code&gt; basics. Initially, we &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch/"&gt;focused on &lt;em&gt;tensors&lt;/em&gt;&lt;/a&gt;. To illustrate their power, we coded a complete (if toy-size) neural network from scratch. We didn’t make use of any of &lt;code&gt;torch&lt;/code&gt;’s higher-level capabilities – not even &lt;em&gt;autograd&lt;/em&gt;, its automatic-differentiation feature.&lt;/p&gt;
&lt;p&gt;This changed in the &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;follow-up post&lt;/a&gt;. No more thinking about derivatives and the chain rule; a single call to &lt;code&gt;backward()&lt;/code&gt; did it all.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-07-torch-modules"&gt;In the third post&lt;/a&gt;, the code again saw a major simplification. Instead of tediously assembling a DAG&lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; by hand, we let &lt;em&gt;modules&lt;/em&gt; take care of the logic.&lt;/p&gt;
&lt;p&gt;Based on that last state, there are just two more things to do. For one, we still compute the loss by hand. And secondly, even though we get the gradients all nicely computed from &lt;em&gt;autograd&lt;/em&gt;, we still loop over the model’s parameters, updating them all ourselves. You won’t be surprised to hear that none of this is necessary.&lt;/p&gt;
&lt;h2 id="losses-and-loss-functions"&gt;Losses and loss functions&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;torch&lt;/code&gt; comes with all the usual loss functions, such as mean squared error, cross entropy, Kullback-Leibler divergence, and the like. In general, there are two usage modes.&lt;/p&gt;
&lt;p&gt;Take the example of calculating mean squared error. One way is to call &lt;code&gt;nnf_mse_loss()&lt;/code&gt; directly on the prediction and ground truth tensors. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x &amp;lt;- torch_randn(c(3, 2, 3))
y &amp;lt;- torch_zeros(c(3, 2, 3))

nnf_mse_loss(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
0.682362
[ CPUFloatType{} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other loss functions designed to be called directly start with &lt;code&gt;nnf_&lt;/code&gt; as well: &lt;code&gt;nnf_binary_cross_entropy()&lt;/code&gt;, &lt;code&gt;nnf_nll_loss()&lt;/code&gt;, &lt;code&gt;nnf_kl_div()&lt;/code&gt; … and so on.&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The second way is to define the algorithm in advance and call it at some later time. Here, respective constructors all start with &lt;code&gt;nn_&lt;/code&gt; and end in &lt;code&gt;_loss&lt;/code&gt;. For example: &lt;code&gt;nn_bce_loss()&lt;/code&gt;, &lt;code&gt;nn_nll_loss(),&lt;/code&gt; &lt;code&gt;nn_kl_div_loss()&lt;/code&gt; …&lt;a href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;loss &amp;lt;- nn_mse_loss()

loss(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
0.682362
[ CPUFloatType{} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method may be preferable when one and the same algorithm should be applied to more than one pair of tensors.&lt;/p&gt;
&lt;h2 id="optimizers"&gt;Optimizers&lt;/h2&gt;
&lt;p&gt;So far, we’ve been updating model parameters following a simple strategy: The gradients told us which direction on the loss curve was downward; the learning rate told us how big of a step to take. What we did was a straightforward implementation of &lt;em&gt;gradient descent&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;However, optimization algorithms used in deep learning get a lot more sophisticated than that. Below, we’ll see how to replace our manual updates using &lt;code&gt;optim_adam()&lt;/code&gt;, &lt;code&gt;torch&lt;/code&gt;’s implementation of the Adam algorithm &lt;span class="citation"&gt;(Kingma and Ba 2017)&lt;/span&gt;. First though, let’s take a quick look at how &lt;code&gt;torch&lt;/code&gt; optimizers work.&lt;/p&gt;
&lt;p&gt;Here is a very simple network, consisting of just one linear layer, to be called on a single data point.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- torch_randn(1, 3)

model &amp;lt;- nn_linear(3, 1)
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we create an optimizer, we tell it what parameters it is supposed to work on.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer &amp;lt;- optim_adam(model$parameters, lr = 0.01)
optimizer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;optim_adam&amp;gt;
  Inherits from: &amp;lt;torch_Optimizer&amp;gt;
  Public:
    add_param_group: function (param_group) 
    clone: function (deep = FALSE) 
    defaults: list
    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, 
    param_groups: list
    state: list
    step: function (closure = NULL) 
    zero_grad: function () &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At any time, we can inspect those parameters:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$param_groups[[1]]$params&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we perform the forward and backward passes. The backward pass calculates the gradients, but does &lt;em&gt;not&lt;/em&gt; update the parameters, as we can see both from the model &lt;em&gt;and&lt;/em&gt; the optimizer objects:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out &amp;lt;- model(data)
out$backward()

optimizer$param_groups[[1]]$params
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling &lt;code&gt;step()&lt;/code&gt; on the optimizer actually &lt;em&gt;performs&lt;/em&gt; the updates. Again, let’s check that both model and optimizer now hold the updated values:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;optimizer$step()

optimizer$param_groups[[1]]$params
model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;NULL
$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]

$weight
torch_tensor 
-0.0285  0.1312 -0.5536
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.2050
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we perform optimization in a loop, we need to make sure to call &lt;code&gt;optimizer$zero_grad()&lt;/code&gt; on every step, as otherwise gradients would be accumulated. You can see this in our final version of the network.&lt;/p&gt;
&lt;h2 id="simple-network-final-version"&gt;Simple network: final version&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)



### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32

model &amp;lt;- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

### network parameters ---------------------------------------------------------

# for adam, need to choose a much higher learning rate in this problem
learning_rate &amp;lt;- 0.08

optimizer &amp;lt;- optim_adam(model$parameters, lr = learning_rate)

### training loop --------------------------------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  
  y_pred &amp;lt;- model(x)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- nnf_mse_loss(y_pred, y, reduction = &amp;quot;sum&amp;quot;)
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation -------- 
  
  # Still need to zero out the gradients before the backward pass, only this time,
  # on the optimizer object
  optimizer$zero_grad()
  
  # gradients are still computed on the loss tensor (no change here)
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # use the optimizer to update model parameters
  optimizer$step()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it! We’ve seen all the major actors on stage: tensors, &lt;em&gt;autograd&lt;/em&gt;, modules, loss functions, and optimizers. In future posts, we’ll explore how to use &lt;em&gt;torch&lt;/em&gt; for standard deep learning tasks involving images, text, tabular data, and more. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-kingma2017adam" class="csl-entry"&gt;
Kingma, Diederik P., and Jimmy Ba. 2017. &lt;span&gt;“Adam: A Method for Stochastic Optimization.”&lt;/span&gt; &lt;a href="http://arxiv.org/abs/1412.6980"&gt;http://arxiv.org/abs/1412.6980&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;directed acyclic graph&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;The prefix &lt;code&gt;nnf_&lt;/code&gt; was chosen because in PyTorch, the corresponding functions live in &lt;a href="https://pytorch.org/docs/stable/nn.functional.html"&gt;torch.nn.functional&lt;/a&gt;.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;This time, the corresponding PyTorch module is &lt;a href="https://pytorch.org/docs/stable/nn.html"&gt;torch.nn&lt;/a&gt;.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>36d30c81f9e7940880a94bc0db25151b</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim</guid>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-09-torch-optim/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Using torch modules</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules</link>
      <description>


&lt;p&gt;&lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch"&gt;Initially&lt;/a&gt;, we started learning about &lt;code&gt;torch&lt;/code&gt; basics by coding a simple neural network from scratch, making use of just a single of &lt;code&gt;torch&lt;/code&gt;’s features: &lt;em&gt;tensors&lt;/em&gt;. &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-05-torch-network-with-autograd"&gt;Then&lt;/a&gt;, we immensely simplified the task, replacing manual backpropagation with &lt;em&gt;autograd&lt;/em&gt;. Today, we &lt;em&gt;modularize&lt;/em&gt; the network - in both the habitual and a very literal sense: Low-level matrix operations are swapped out for &lt;code&gt;torch&lt;/code&gt; &lt;code&gt;module&lt;/code&gt;s.&lt;/p&gt;
&lt;h2 id="modules"&gt;Modules&lt;/h2&gt;
&lt;p&gt;From other frameworks (Keras, say), you may be used to distinguishing between &lt;em&gt;models&lt;/em&gt; and &lt;em&gt;layers&lt;/em&gt;. In &lt;code&gt;torch&lt;/code&gt;, both are instances of &lt;code&gt;nn_Module()&lt;/code&gt;, and thus, have some methods in common. For those thinking in terms of “models” and “layers”, I’m artificially splitting up this section into two parts. In reality though, there is no dichotomy: New modules may be composed of existing ones up to arbitrary levels of recursion.&lt;/p&gt;
&lt;h3 id="base-modules-layers"&gt;Base modules (“layers”)&lt;/h3&gt;
&lt;p&gt;Instead of writing out an affine operation by hand – &lt;code&gt;x$mm(w1) + b1&lt;/code&gt;, say –, as we’ve been doing so far, we can create a linear module. The following snippet instantiates a linear layer that expects three-feature inputs and returns a single output per observation:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)
l &amp;lt;- nn_linear(3, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The module has two parameters, “weight” and “bias”. Both now come pre-initialized:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$weight
torch_tensor 
-0.0385  0.1412 -0.5436
[ CPUFloatType{1,3} ]

$bias
torch_tensor 
-0.1950
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Modules are callable; calling a module executes its &lt;code&gt;forward()&lt;/code&gt; method, which, for a linear layer, matrix-multiplies input and weights, and adds the bias.&lt;/p&gt;
&lt;p&gt;Let’s try this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data  &amp;lt;- torch_randn(10, 3)
out &amp;lt;- l(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, &lt;code&gt;out&lt;/code&gt; now holds some data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$data()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2711
-1.8151
-0.0073
 0.1876
-0.0930
 0.7498
-0.2332
-0.0428
 0.3849
-0.2618
[ CPUFloatType{10,1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition though, this tensor knows what will need to be done, should ever it be asked to calculate gradients:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AddmmBackward&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the difference between tensors returned by modules and self-created ones. When creating tensors ourselves, we need to pass &lt;code&gt;requires_grad = TRUE&lt;/code&gt; to trigger gradient calculation. With modules, &lt;code&gt;torch&lt;/code&gt; correctly assumes that we’ll want to perform backpropagation at some point.&lt;/p&gt;
&lt;p&gt;By now though, we haven’t called &lt;code&gt;backward()&lt;/code&gt; yet. Thus, no gradients have yet been computed:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$weight$grad
l$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
[ Tensor (undefined) ]
torch_tensor 
[ Tensor (undefined) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s change this:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in (function (self, gradient, keep_graph, create_graph)  : 
  grad can be implicitly created only for scalar outputs (_make_grads at ../torch/csrc/autograd/autograd.cpp:47)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why the error? &lt;em&gt;Autograd&lt;/em&gt; expects the output tensor to be a scalar, while in our example, we have a tensor of size &lt;code&gt;(10, 1)&lt;/code&gt;. This error won’t often occur in practice, where we work with &lt;em&gt;batches&lt;/em&gt; of inputs (sometimes, just a single batch). But still, it’s interesting to see how to resolve this.&lt;/p&gt;
&lt;p&gt;To make the example work, we introduce a – virtual – final aggregation step – taking the mean, say. Let’s call it &lt;code&gt;avg&lt;/code&gt;. If such a mean were taken, its gradient with respect to &lt;code&gt;l$weight&lt;/code&gt; would be obtained via the chain rule:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{equation*} 
 \frac{\partial \ avg}{\partial w} = \frac{\partial \ avg}{\partial \ out}  \ \frac{\partial \ out}{\partial w}
\end{equation*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Of the quantities on the right side, we’re interested in the second. We need to provide the first one, the way it would look &lt;em&gt;if really we were taking the mean&lt;/em&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;d_avg_d_out &amp;lt;- torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t()
out$backward(gradient = d_avg_d_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;l$weight$grad&lt;/code&gt; and &lt;code&gt;l$bias$grad&lt;/code&gt; &lt;em&gt;do&lt;/em&gt; contain gradients:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;l$weight$grad
l$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 1.3410  6.4343 -30.7135
[ CPUFloatType{1,3} ]
torch_tensor 
 100
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to &lt;code&gt;nn_linear()&lt;/code&gt; , &lt;code&gt;torch&lt;/code&gt; provides pretty much all the common layers you might hope for. But few tasks are solved by a single layer. How do you combine them? Or, in the usual lingo: How do you build &lt;em&gt;models&lt;/em&gt;?&lt;/p&gt;
&lt;h3 id="container-modules-models"&gt;Container modules (“models”)&lt;/h3&gt;
&lt;p&gt;Now, &lt;em&gt;models&lt;/em&gt; are just modules that contain other modules. For example, if all inputs are supposed to flow through the same nodes and along the same edges, then &lt;code&gt;nn_sequential()&lt;/code&gt; can be used to build a simple graph.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- nn_sequential(
    nn_linear(3, 16),
    nn_relu(),
    nn_linear(16, 1)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the same technique as above to get an overview of all model parameters (two weight matrices and two bias vectors):&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$parameters&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$`0.weight`
torch_tensor 
-0.1968 -0.1127 -0.0504
 0.0083  0.3125  0.0013
 0.4784 -0.2757  0.2535
-0.0898 -0.4706 -0.0733
-0.0654  0.5016  0.0242
 0.4855 -0.3980 -0.3434
-0.3609  0.1859 -0.4039
 0.2851  0.2809 -0.3114
-0.0542 -0.0754 -0.2252
-0.3175  0.2107 -0.2954
-0.3733  0.3931  0.3466
 0.5616 -0.3793 -0.4872
 0.0062  0.4168 -0.5580
 0.3174 -0.4867  0.0904
-0.0981 -0.0084  0.3580
 0.3187 -0.2954 -0.5181
[ CPUFloatType{16,3} ]

$`0.bias`
torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]

$`2.weight`
torch_tensor 
Columns 1 to 10-0.0908 -0.1786  0.0812 -0.0414 -0.0251 -0.1961  0.2326  0.0943 -0.0246  0.0748

Columns 11 to 16 0.2111 -0.1801 -0.0102 -0.0244  0.1223 -0.1958
[ CPUFloatType{1,16} ]

$`2.bias`
torch_tensor 
 0.2470
[ CPUFloatType{1} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To inspect an individual parameter, make use of its position in the sequential model. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model[[1]]$bias&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
-0.3714
 0.5603
-0.3791
 0.4372
-0.1793
-0.3329
 0.5588
 0.1370
 0.4467
 0.2937
 0.1436
 0.1986
 0.4967
 0.1554
-0.3219
-0.0266
[ CPUFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just like &lt;code&gt;nn_linear()&lt;/code&gt; above, this module can be called directly on data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out &amp;lt;- model(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On a composite module like this one, calling &lt;code&gt;backward()&lt;/code&gt; will backpropagate through all the layers:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward(gradient = torch_tensor(10)$`repeat`(10)$unsqueeze(1)$t())

# e.g.
model[[1]]$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CPUFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And placing the composite module on the GPU will move all tensors there:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;model$cuda()
model[[1]]$bias$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
  0.0000
-17.8578
  1.6246
 -3.7258
 -0.2515
 -5.8825
 23.2624
  8.4903
 -2.4604
  6.7286
 14.7760
-14.4064
 -1.0206
 -1.7058
  0.0000
 -9.7897
[ CUDAFloatType{16} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see how using &lt;code&gt;nn_sequential()&lt;/code&gt; can simplify our example network.&lt;/p&gt;
&lt;h2 id="simple-network-using-modules"&gt;Simple network using modules&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)


### define the network ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32

model &amp;lt;- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

### network parameters ---------------------------------------------------------

learning_rate &amp;lt;- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  
  y_pred &amp;lt;- model(x)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- (y_pred - y)$pow(2)$sum()
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation -------- 
  
  # Zero the gradients before running the backward pass.
  model$zero_grad()
  
  # compute gradient of the loss w.r.t. all learnable parameters of the model
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON&amp;#39;T want to record
  # for automatic gradient computation
  # Update each parameter by its `grad`
  
  with_no_grad({
    model$parameters %&amp;gt;% purrr::walk(function(param) param$sub_(learning_rate * param$grad))
  })
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The forward pass looks a lot better now; however, we still loop through the model’s parameters and update each one by hand. Furthermore, you may be already be suspecting that &lt;code&gt;torch&lt;/code&gt; provides abstractions for common loss functions. In the next and last installment of this series, we’ll address both points, making use of &lt;code&gt;torch&lt;/code&gt; losses and optimizers. See you then!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>89db528ec58a4b4da856d3b6eea438b7</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules</guid>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-07-torch-modules/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Introducing torch autograd</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd</link>
      <description>


&lt;p&gt;Last week, we saw how to code &lt;a href="https://blogs.rstudio.com/ai/posts/2020-10-01-torch-network-from-scratch"&gt;a simple network from scratch&lt;/a&gt;, using nothing but &lt;code&gt;torch&lt;/code&gt; &lt;em&gt;tensors&lt;/em&gt;. Predictions, loss, gradients, weight updates – all these things we’ve been computing ourselves. Today, we make a significant change: Namely, we spare ourselves the cumbersome calculation of gradients, and have &lt;code&gt;torch&lt;/code&gt; do it for us.&lt;/p&gt;
&lt;p&gt;Prior to that though, let’s get some background.&lt;/p&gt;
&lt;h2 id="automatic-differentiation-with-autograd"&gt;Automatic differentiation with &lt;em&gt;autograd&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;torch&lt;/code&gt; uses a module called &lt;em&gt;autograd&lt;/em&gt; to&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;record operations performed on tensors, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;store what will have to be done to obtain the corresponding gradients, once we’re entering the backward pass.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These prospective actions are stored internally as functions, and when it’s time to compute the gradients, these functions are applied in order: Application starts from the output node, and calculated gradients are successively &lt;em&gt;propagated&lt;/em&gt; &lt;em&gt;back&lt;/em&gt; through the network. This is a form of &lt;em&gt;reverse mode automatic differentiation&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id="autograd-basics"&gt;&lt;em&gt;Autograd&lt;/em&gt; basics&lt;/h4&gt;
&lt;p&gt;As users, we can see a bit of the implementation. As a prerequisite for this “recording” to happen, tensors have to be created with &lt;code&gt;requires_grad = TRUE&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

x &amp;lt;- torch_ones(2, 2, requires_grad = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be clear, &lt;code&gt;x&lt;/code&gt; now is a tensor &lt;em&gt;with respect to which&lt;/em&gt; gradients have to be calculated – normally, a tensor representing a weight or a bias, not the input data &lt;a href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. If we subsequently perform some operation on that tensor, assigning the result to &lt;code&gt;y&lt;/code&gt;,&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y &amp;lt;- x$mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we find that &lt;code&gt;y&lt;/code&gt; now has a non-empty &lt;code&gt;grad_fn&lt;/code&gt; that tells &lt;code&gt;torch&lt;/code&gt; how to compute the gradient of &lt;code&gt;y&lt;/code&gt; with respect to &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MeanBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Actual &lt;em&gt;computation&lt;/em&gt; of gradients is triggered by calling &lt;code&gt;backward()&lt;/code&gt; on the output tensor.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After &lt;code&gt;backward()&lt;/code&gt; has been called, &lt;code&gt;x&lt;/code&gt; has a non-null field termed &lt;code&gt;grad&lt;/code&gt; that stores the gradient of &lt;code&gt;y&lt;/code&gt; with respect to &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With longer chains of computations, we can take a glance at how &lt;code&gt;torch&lt;/code&gt; builds up a graph of backward operations. Here is a slightly more complex example – feel free to skip if you’re not the type who just &lt;em&gt;has&lt;/em&gt; to peek into things for them to make sense.&lt;/p&gt;
&lt;h4 id="digging-deeper"&gt;Digging deeper&lt;/h4&gt;
&lt;p&gt;We build up a simple graph of tensors, with inputs &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; being connected to output &lt;code&gt;out&lt;/code&gt; by intermediaries &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;x1 &amp;lt;- torch_ones(2, 2, requires_grad = TRUE)
x2 &amp;lt;- torch_tensor(1.1, requires_grad = TRUE)

y &amp;lt;- x1 * (x2 + 2)

z &amp;lt;- y$pow(2) * 3

out &amp;lt;- z$mean()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save memory, intermediate gradients are normally not being stored. Calling &lt;code&gt;retain_grad()&lt;/code&gt; on a tensor allows one to deviate from this default. Let’s do this here, for the sake of demonstration:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y$retain_grad()

z$retain_grad()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can go backwards through the graph and inspect &lt;code&gt;torch&lt;/code&gt;’s action plan for backprop, starting from &lt;code&gt;out$grad_fn&lt;/code&gt;, like so:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for mean, the last operation executed
out$grad_fn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MeanBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the multiplication by 3 in z = y.pow(2) * 3
out$grad_fn$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
MulBackward1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for pow in z = y.pow(2) * 3
out$grad_fn$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
PowBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the multiplication in y = x * (x + 2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
MulBackward0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# how to compute the gradient for the two branches of y = x * (x + 2),
# where the left branch is a leaf node (AccumulateGrad for x1)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
torch::autograd::AccumulateGrad
[[2]]
AddBackward1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# here we arrive at the other leaf node (AccumulateGrad for x2)
out$grad_fn$next_functions[[1]]$next_functions[[1]]$next_functions[[1]]$next_functions[[2]]$next_functions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
torch::autograd::AccumulateGrad&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now call &lt;code&gt;out$backward()&lt;/code&gt;, all tensors in the graph will have their respective gradients calculated.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;out$backward()

z$grad
y$grad
x2$grad
x1$grad&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;torch_tensor 
 0.2500  0.2500
 0.2500  0.2500
[ CPUFloatType{2,2} ]
torch_tensor 
 4.6500  4.6500
 4.6500  4.6500
[ CPUFloatType{2,2} ]
torch_tensor 
 18.6000
[ CPUFloatType{1} ]
torch_tensor 
 14.4150  14.4150
 14.4150  14.4150
[ CPUFloatType{2,2} ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this nerdy excursion, let’s see how &lt;em&gt;autograd&lt;/em&gt; makes our network simpler.&lt;/p&gt;
&lt;h2 id="the-simple-network-now-using-autograd"&gt;The simple network, now using &lt;em&gt;autograd&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;em&gt;autograd&lt;/em&gt;, we say good-bye to the tedious, error-prone process of coding backpropagation ourselves. A single method call does it all: &lt;code&gt;loss$backward()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;torch&lt;/code&gt; keeping track of operations as required, we don’t even have to explicitly name the intermediate tensors any more. We can code forward pass, loss calculation, and backward pass in just three lines:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;y_pred &amp;lt;- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)
  
loss &amp;lt;- (y_pred - y)$pow(2)$sum()

loss$backward()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the complete code. We’re at an intermediate stage: We still manually compute the forward pass and the loss, and we still manually update the weights. Due to the latter, there is something I need to explain. But I’ll let you check out the new version first:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(torch)

### generate training data -----------------------------------------------------

# input dimensionality (number of input features)
d_in &amp;lt;- 3
# output dimensionality (number of predicted features)
d_out &amp;lt;- 1
# number of observations in training set
n &amp;lt;- 100


# create random data
x &amp;lt;- torch_randn(n, d_in)
y &amp;lt;- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)


### initialize weights ---------------------------------------------------------

# dimensionality of hidden layer
d_hidden &amp;lt;- 32
# weights connecting input to hidden layer
w1 &amp;lt;- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 &amp;lt;- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 &amp;lt;- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 &amp;lt;- torch_zeros(1, d_out, requires_grad = TRUE)

### network parameters ---------------------------------------------------------

learning_rate &amp;lt;- 1e-4

### training loop --------------------------------------------------------------

for (t in 1:200) {
  ### -------- Forward pass --------
  
  y_pred &amp;lt;- x$mm(w1)$add(b1)$clamp(min = 0)$mm(w2)$add(b2)
  
  ### -------- compute loss -------- 
  loss &amp;lt;- (y_pred - y)$pow(2)$sum()
  if (t %% 10 == 0)
    cat(&amp;quot;Epoch: &amp;quot;, t, &amp;quot;   Loss: &amp;quot;, loss$item(), &amp;quot;\n&amp;quot;)
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we DON&amp;#39;T 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 &amp;lt;- w1$sub_(learning_rate * w1$grad)
     w2 &amp;lt;- w2$sub_(learning_rate * w2$grad)
     b1 &amp;lt;- b1$sub_(learning_rate * b1$grad)
     b2 &amp;lt;- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they&amp;#39;d accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As explained above, after &lt;code&gt;some_tensor$backward()&lt;/code&gt;, all tensors preceding it in the graph&lt;a href="#fn2" class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; will have their &lt;code&gt;grad&lt;/code&gt; fields populated. We make use of these fields to update the weights. But now that &lt;em&gt;autograd&lt;/em&gt; is “on”, whenever we execute an operation we &lt;em&gt;don’t&lt;/em&gt; want recorded for backprop, we need to explicitly exempt it: This is why we wrap the weight updates in a call to &lt;code&gt;with_no_grad()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While this is something you may file under “nice to know” – after all, once we arrive at the last post in the series, this manual updating of weights will be gone – the idiom of &lt;em&gt;zeroing gradients&lt;/em&gt; is here to stay: Values stored in &lt;code&gt;grad&lt;/code&gt; fields accumulate; whenever we’re done using them, we need to zero them out before reuse.&lt;/p&gt;
&lt;h2 id="outlook"&gt;Outlook&lt;/h2&gt;
&lt;p&gt;So where do we stand? We started out coding a network completely from scratch, making use of nothing but &lt;code&gt;torch&lt;/code&gt; tensors. Today, we got significant help from &lt;em&gt;autograd&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But we’re still manually updating the weights, – and aren’t deep learning frameworks known to provide abstractions (“layers”, or: “modules”) on top of tensor computations …?&lt;/p&gt;
&lt;p&gt;We address both issues in the follow-up installments. Thanks for reading!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Unless we &lt;em&gt;want&lt;/em&gt; to change the data, as when generating adversarial examples.&lt;a href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;All that have &lt;code&gt;requires_grad&lt;/code&gt; set to &lt;code&gt;TRUE&lt;/code&gt;, to be precise.&lt;a href="#fnref2" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>68b4457f4594ac1a742e88e6d9c790b4</distill:md5>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd</guid>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-05-torch-network-with-autograd/images/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Getting familiar with torch tensors</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch</link>
      <description>In this first installment of a four-part miniseries, we present the main things you will want to know about torch tensors. As an illustrative example, we'll code a simple neural network from scratch.</description>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch</guid>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-10-01-torch-network-from-scratch/images/pic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.4: Weighted Sampling, Tidyr Verbs, Robust Scaler, RAPIDS, and more</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</link>
      <description>Sparklyr 1.4 is now available! This release comes with delightful new features such as weighted sampling and tidyr verbs support for Spark dataframes, robust scaler for standardizing data based on median and interquartile range, spark_connect interface for RAPIDS GPU acceleration plugin, as well as a number of dplyr-related improvements.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released</guid>
      <pubDate>Wed, 30 Sep 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-09-30-sparklyr-1.4.0-released/images/sparklyr-1.4.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Please allow me to introduce myself: Torch for R</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</link>
      <description>Today, we are excited to introduce torch, an R package that allows you to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor-computation and automatic-differentiation capabilities essential to building neural networks.</description>
      <category>Packages/Releases</category>
      <category>Torch</category>
      <category>R</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r</guid>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-09-29-introducing-torch-for-r/images/pt.png" medium="image" type="image/png" width="919" height="264"/>
    </item>
    <item>
      <title>Introducing sparklyr.flint: A time-series extension for sparklyr</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</link>
      <description>We are pleased to announce that sparklyr.flint, a sparklyr extension for analyzing time series at scale with Flint, is now available on CRAN. Flint is an open-source library for working with time-series in Apache Spark which supports aggregates and joins on time-series datasets.</description>
      <category>R</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint</guid>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-09-07-sparklyr-flint/images/thumb.png" medium="image" type="image/png" width="126" height="77"/>
    </item>
    <item>
      <title>An introduction to weather forecasting with deep learning</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</link>
      <description>A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the "black-box end" of the continuum.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction</guid>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-09-01-weather-prediction/images/thumb.png" medium="image" type="image/png" width="600" height="332"/>
    </item>
    <item>
      <title>Training ImageNet with R</title>
      <dc:creator>Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</link>
      <description>This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Distributed Computing</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r</guid>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deepfake detection challenge from R</title>
      <dc:creator>Turgut Abdullayev</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-08-18-deepfake</link>
      <description>A couple of months ago, Amazon, Facebook, Microsoft, and other contributors initiated a challenge consisting of telling apart real and AI-generated ("fake") videos. We show how to approach this challenge from R.</description>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-08-18-deepfake</guid>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-08-18-deepfake/files/frame_2.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>FNN-VAE for noisy time series forecasting</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</link>
      <description>In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries</guid>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>State-of-the-art NLP models from R</title>
      <dc:creator>Turgut Abdullayev</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-30-state-of-the-art-nlp-models-from-r</link>
      <description>Nowadays, Microsoft, Google, Facebook, and OpenAI are sharing lots of state-of-the-art models in the field of Natural Language Processing. However, fewer materials exist how to use these models from R. In this post, we will show how R users can access and benefit from these models as well.</description>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-30-state-of-the-art-nlp-models-from-r</guid>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-07-30-state-of-the-art-nlp-models-from-r/files/dino.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Parallelized sampling using exponential variates</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-29-parallelized-sampling</link>
      <description>How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.</description>
      <category>Concepts</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-29-parallelized-sampling</guid>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-07-29-parallelized-sampling/images/dice.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Time series prediction with FNN-LSTM</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</link>
      <description>In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, "vanilla LSTM", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm</guid>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-07-20-fnn-lstm/images/old_faithful.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</link>
      <description>Sparklyr 1.3 is now available, featuring exciting new functionalities such as integration of Spark higher-order functions and data import/export in Avro and in user-defined serialization formats.</description>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released</guid>
      <pubDate>Thu, 16 Jul 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Deep attractors: Where deep learning meets chaos</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</link>
      <description>In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.</description>
      <category>R</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors</guid>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-06-24-deep-attractors/images/x_z.gif" medium="image" type="image/gif"/>
    </item>
    <item>
      <title>Easy PixelCNN with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</link>
      <description>PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.</description>
      <category>R</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn</guid>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-05-29-pixelcnn/images/thumb.png" medium="image" type="image/png" width="400" height="203"/>
    </item>
    <item>
      <title>Hacking deep learning: model inversion attack by example</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</link>
      <description>Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under "model inversion" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks</guid>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-05-15-model-inversion-attacks/images/results.png" medium="image" type="image/png" width="600" height="394"/>
    </item>
    <item>
      <title>Towards privacy: Encrypted deep learning with Syft and Keras</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</link>
      <description>Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.</description>
      <category>R</category>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft</guid>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect</title>
      <dc:creator>Yitao Li</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</link>
      <description>A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Distributed Computing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released</guid>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png" medium="image" type="image/png" width="1241" height="307"/>
    </item>
    <item>
      <title>pins 0.4: Versioning</title>
      <dc:creator>Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</link>
      <description>A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.</description>
      <category>R</category>
      <category>Packages/Releases</category>
      <category>Data Management</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04</guid>
      <pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-04-13-pins-04/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>A first look at federated learning with TensorFlow</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</link>
      <description>The term "federated learning" was coined to describe a form of distributed model training where the data remains on client devices, i.e., is never shipped to the coordinating server. In this post, we introduce central concepts and run first experiments with TensorFlow Federated, using R.</description>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro</guid>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-04-08-tf-federated-intro/images/federated_learning.png" medium="image" type="image/png" width="1122" height="570"/>
    </item>
    <item>
      <title>Introducing: The RStudio AI Blog</title>
      <dc:creator>The Multiverse Team</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-04-01-rstudio-ai-blog</link>
      <description>This blog just got a new title: RStudio AI Blog. We explain why.</description>
      <category>Meta</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-04-01-rstudio-ai-blog</guid>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-04-01-rstudio-ai-blog/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Infinite surprise - the iridescent personality of Kullback-Leibler divergence</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-02-19-kl-divergence</link>
      <description>Kullback-Leibler divergence is not just used to train variational autoencoders or Bayesian networks (and not just a hard-to-pronounce thing). It is a fundamental concept in information theory, put to use in a vast range of applications. Most interestingly, it's not always about constraint, regularization or compression. Quite on the contrary, sometimes it is about novelty, discovery and surprise.</description>
      <category>Probabilistic ML/DL</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-02-19-kl-divergence</guid>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-02-19-kl-divergence/images/ultimatemachine.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>NumPy-style broadcasting for R TensorFlow users</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</link>
      <description>Broadcasting, as done by Python's scientific computing library NumPy, involves dynamically extending shapes so that arrays of different sizes may be passed to operations that expect conformity - such as adding or multiplying elementwise. In NumPy, the way broadcasting works is specified exactly; the same rules apply to TensorFlow operations. For anyone who finds herself, occasionally, consulting Python code, this post strives to explain.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting</guid>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-01-24-numpy-broadcasting/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>First experiments with TensorFlow mixed-precision training</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</link>
      <description>TensorFlow 2.1, released last week, allows for mixed-precision training, making use of the Tensor Cores available in the most recent NVidia GPUs. In this post, we report first experimental results and provide some background on what this is all about.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training</guid>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2020-01-13-mixed-precision-training/images/tc.png" medium="image" type="image/png" width="589" height="399"/>
    </item>
    <item>
      <title>Differential Privacy with TensorFlow</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</link>
      <description>Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.</description>
      <category>Privacy &amp; Security</category>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy</guid>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-12-20-differential-privacy/images/cat.png" medium="image" type="image/png" width="400" height="251"/>
    </item>
    <item>
      <title>tfhub: R interface to TensorFlow Hub</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</link>
      <description>TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0</guid>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-12-18-tfhub-0.7.0/images/tfhub.png" medium="image" type="image/png" width="1365" height="909"/>
    </item>
    <item>
      <title>Gaussian Process Regression with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</link>
      <description>Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather "normal" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process</guid>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png" medium="image" type="image/png" width="818" height="352"/>
    </item>
    <item>
      <title>Getting started with Keras from R - the 2020 edition</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</link>
      <description>Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020</guid>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-11-27-gettingstarted-2020/images/website.png" medium="image" type="image/png" width="1591" height="725"/>
    </item>
    <item>
      <title>Variational convnets with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</link>
      <description>In a Bayesian neural network, layer weights are distributions, not tensors. Using tfprobability, the R wrapper to TensorFlow Probability, we can build regular Keras models that have probabilistic layers, and thus get uncertainty estimates "for free". In this post, we show how to define, train and obtain predictions from a probabilistic convolutional neural network.</description>
      <category>Probabilistic ML/DL</category>
      <category>Time Series</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet</guid>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-11-13-variational-convnet/images/bbb.png" medium="image" type="image/png" width="796" height="378"/>
    </item>
    <item>
      <title>tfprobability 0.8 on CRAN: Now how can you use it?</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</link>
      <description>Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.</description>
      <category>Probabilistic ML/DL</category>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran</guid>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-11-07-tfp-cran/images/tfprobability.png" medium="image" type="image/png" width="518" height="600"/>
    </item>
    <item>
      <title>Innocent unicorns considered harmful? How to experiment with GPT-2 from R</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <dc:creator>Javier Luraschi</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</link>
      <description>Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.</description>
      <category>Natural Language Processing</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2</guid>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-10-23-gpt-2/images/thumb.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>TensorFlow 2.0 is here - what changes for R users?</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</link>
      <description>TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges</guid>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-10-08-tf2-whatchanges/images/thumb.png" medium="image" type="image/png" width="400" height="400"/>
    </item>
    <item>
      <title>On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc</link>
      <description>TensorFlow Probability, and its R wrapper tfprobability, provide Markov Chain Monte Carlo (MCMC) methods that were used in a number of recent posts on this blog. These posts were directed to users already comfortable with the method, and terminology, per se, which readers mainly interested in deep learning won't necessarily be. Here we try to make up leeway, introducing Hamitonian Monte Carlo (HMC) as well as a few often-heard "buzzwords" accompanying it, always striving to keep in mind what it is all "for".</description>
      <category>Bayesian Modeling</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc</guid>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-10-03-intro-to-hmc/images/mb.png" medium="image" type="image/png" width="548" height="345"/>
    </item>
    <item>
      <title>BERT from R</title>
      <dc:creator>Turgut Abdullayev</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</link>
      <description>A deep learning model - BERT from Google AI Research - has yielded state-of-the-art results in a wide variety of Natural Language Processing (NLP) tasks. In this tutorial, we will show how to load and train the BERT model from R, using Keras.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r</guid>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-09-30-bert-r/images/bert.png" medium="image" type="image/png" width="437" height="367"/>
    </item>
    <item>
      <title>So, how come we can use TensorFlow from R?</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</link>
      <description>Have you ever wondered why you can call TensorFlow - mostly known as a Python framework - from R? If not - that's how it should be, as the R packages keras and tensorflow aim to make this process as transparent as possible to the user. But for them to be those helpful genies, someone else first has to tame the Python.</description>
      <category>TensorFlow/Keras</category>
      <category>Meta</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r</guid>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-08-29-using-tf-from-r/images/thumb.png" medium="image" type="image/png" width="739" height="516"/>
    </item>
    <item>
      <title>Image segmentation with U-Net</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</link>
      <description>In image segmentation, every pixel of an image is assigned a class. Depending on the application, classes could be different cell types; or the task could be binary, as in "cancer cell yes or no?". Area of application notwithstanding, the established neural network architecture of choice is U-Net. In this post, we show how to preprocess data and train a U-Net model on the Kaggle Carvana image segmentation data.</description>
      <category>Image Recognition &amp; Image Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet</guid>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-08-23-unet/images/unet.png" medium="image" type="image/png" width="1400" height="932"/>
    </item>
    <item>
      <title>Modeling censored data with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</link>
      <description>In this post we use tfprobability, the R interface to TensorFlow Probability, to model censored data. Again, the exposition is inspired by the treatment of this topic in Richard McElreath's Statistical Rethinking. Instead of cute cats though, we model immaterial entities from the cold world of technology: This post explores durations of CRAN package checks, a dataset that comes with Max Kuhn's parsnip.</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data</guid>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-07-31-censored-data/images/thumb_cropped.png" medium="image" type="image/png" width="955" height="396"/>
    </item>
    <item>
      <title>TensorFlow feature columns: Transforming your data recipes-style</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</link>
      <description>TensorFlow feature columns provide useful functionality for preprocessing categorical data and chaining transformations, like bucketization or feature crossing. From R, we use them in popular "recipes" style, creating and subsequently refining a feature specification. In this post, we show how using feature specs frees cognitive resources and lets you focus on what you really want to accomplish. What's more, because of its elegance, feature-spec code reads nice and is fun to write as well.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns</guid>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-07-09-feature-columns/images/feature_cols_hier.png" medium="image" type="image/png" width="1172" height="678"/>
    </item>
    <item>
      <title>Dynamic linear models with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability</link>
      <description>Previous posts featuring tfprobability - the R interface to TensorFlow Probability - have focused on enhancements to deep neural networks (e.g., introducing Bayesian uncertainty estimates) and fitting hierarchical models with Hamiltonian Monte Carlo. This time, we show how to fit time series using dynamic linear models (DLMs), yielding posterior predictive forecasts as well as the smoothed and filtered estimates from the Kálmán filter.</description>
      <category>Probabilistic ML/DL</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability</guid>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/images/thumb.png" medium="image" type="image/png" width="2012" height="1065"/>
    </item>
    <item>
      <title>Adding uncertainty estimates to Keras models with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</link>
      <description>As of today, there is no mainstream road to obtaining uncertainty estimates from neural networks. All that can be said is that, normally, approaches tend to be Bayesian in spirit, involving some way of putting a prior over model weights. This holds true as well for the method presented in this post: We show how to use tfprobability, the R interface to TensorFlow Probability, to add uncertainty estimates to a Keras model in an elegant and conceptually plausible way.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability</guid>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png" medium="image" type="image/png" width="2020" height="1020"/>
    </item>
    <item>
      <title>Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</link>
      <description>This post builds on our recent introduction to multi-level modeling with tfprobability, the R wrapper to TensorFlow Probability. We show how to pool not just mean values ("intercepts"), but also relationships ("slopes"), thus enabling models to learn from data in an even broader way. Again, we use an example from Richard McElreath's "Statistical Rethinking"; the terminology as well as the way we present this topic are largely owed to this book.</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes</guid>
      <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-05-24-varying-slopes/images/thumb.png" medium="image" type="image/png" width="509" height="249"/>
    </item>
    <item>
      <title>Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</link>
      <description>This post is a first introduction to MCMC modeling with tfprobability, the R interface to TensorFlow Probability (TFP). Our example is a multi-level model describing tadpole mortality, which may be known to the reader from Richard McElreath's wonderful "Statistical Rethinking".</description>
      <category>Bayesian Modeling</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow</guid>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png" medium="image" type="image/png" width="1612" height="659"/>
    </item>
    <item>
      <title>Experimenting with autoregressive flows in TensorFlow Probability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</link>
      <description>Continuing from the recent introduction to bijectors in TensorFlow Probability (TFP), this post brings autoregressivity to the table. Using TFP through the new R package tfprobability, we look at the implementation of masked autoregressive flows (MAF) and put them to use on two different datasets.</description>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows</guid>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-04-24-autoregressive-flows/images/made.png" medium="image" type="image/png" width="686" height="398"/>
    </item>
    <item>
      <title>Auto-Keras: Tuning-free deep learning from R</title>
      <dc:creator>Juan Cruz Rodriguez</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</link>
      <description>Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras</guid>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/images/thumbnail.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Getting into the flow: Bijectors in TensorFlow Probability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</link>
      <description>Normalizing flows are one of the lesser known, yet fascinating and successful architectures in unsupervised deep learning. In this post we provide a basic introduction to flows using tfprobability, an R wrapper to TensorFlow Probability. Upcoming posts will build on this, using more complex flows on more complex data.</description>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows</guid>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-04-05-bijectors-flows/images/flows.png" medium="image" type="image/png" width="904" height="325"/>
    </item>
    <item>
      <title>Math, code, concepts: A third road to deep learning</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-03-15-concepts-way-to-dl</link>
      <description>Not everybody who wants to get into deep learning has a strong background in math or programming. This post elaborates on a concepts-driven, abstraction-based way to learn what it's all about.</description>
      <category>Meta</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-03-15-concepts-way-to-dl</guid>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-03-15-concepts-way-to-dl/images/prev.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Audio classification with Keras: Looking closer at the non-deep learning parts</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</link>
      <description>Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background</guid>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-02-07-audio-background/images/seven2.png" medium="image" type="image/png" width="1714" height="846"/>
    </item>
    <item>
      <title>Discrete Representation Learning with VQ-VAE and TensorFlow Probability</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</link>
      <description>Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's "Neural Discrete Representation Learning" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.</description>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae</guid>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/images/thumb1.png" medium="image" type="image/png" width="510" height="287"/>
    </item>
    <item>
      <title>Getting started with TensorFlow Probability from R</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</link>
      <description>TensorFlow Probability offers a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. It works seamlessly with core TensorFlow and (TensorFlow) Keras. In this post, we provide a short introduction to the distributions layer and then, use it for sampling and calculating probabilities in a Variational Autoencoder.</description>
      <category>TensorFlow/Keras</category>
      <category>Probabilistic ML/DL</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability</guid>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png" medium="image" type="image/png" width="884" height="584"/>
    </item>
    <item>
      <title>Concepts in object detection</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</link>
      <description>As shown in a previous post, naming and locating a single object in an image is a task that may be approached in a straightforward way. This is not the same with general object detection, though - naming and locating several objects at once, with no prior information about how many objects are supposed to be detected.
In this post, we explain the steps involved in coding a basic single-shot object detector: Not unlike SSD (Single-shot Multibox Detector), but simplified and designed not for best performance, but comprehensibility.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts</guid>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-12-18-object-detection-concepts/images/results.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Entity embeddings for fun and profit</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</link>
      <description>Embedding layers are not just useful when working with language data. As "entity embeddings", they've recently become famous for applications on tabular, small-scale data. In this post, we exemplify two possible use cases, also drawing attention to what not to expect.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit</guid>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png" medium="image" type="image/png" width="820" height="410"/>
    </item>
    <item>
      <title>You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</link>
      <description>In deep learning, there is no obvious way of obtaining uncertainty estimates. In 2016, Gal and Ghahramani proposed a method that is both theoretically grounded and practical: use dropout at test time. In this post, we introduce a refined version of this method (Gal et al. 2017) that has the network itself learn how uncertain it is.</description>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Probabilistic ML/DL</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout</guid>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png" medium="image" type="image/png" width="2046" height="872"/>
    </item>
    <item>
      <title>Naming and locating objects in images</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</link>
      <description>Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects</guid>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/images/preds_train.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Representation learning with MMD-VAE</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</link>
      <description>Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae</guid>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/images/thumb.png" medium="image" type="image/png" width="468" height="178"/>
    </item>
    <item>
      <title>Winner takes all: A look at activations and cost functions</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</link>
      <description>Why do we use the activations we use, and how do they relate to the cost functions they tend to co-appear with? In this post we provide a conceptual introduction.</description>
      <category>TensorFlow/Keras</category>
      <category>Concepts</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro</guid>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-10-11-activations-intro/images/output.png" medium="image" type="image/png" width="800" height="384"/>
    </item>
    <item>
      <title>More flexible models with TensorFlow eager execution and Keras</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</link>
      <description>Advanced applications like generative adversarial networks, neural style transfer, and the attention mechanism ubiquitous in natural language processing used to be not-so-simple to implement with the Keras declarative coding paradigm. Now, with the advent of TensorFlow eager execution, things have changed. This post explores using eager execution with R.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup</guid>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-10-02-eager-wrapup/images/m.png" medium="image" type="image/png" width="384" height="126"/>
    </item>
    <item>
      <title>Collaborative filtering with embeddings</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</link>
      <description>Embeddings are not just for use in natural language processing. Here we apply embeddings to a common task in collaborative filtering - predicting user ratings - and on our way, strive for a better understanding of what an embedding layer really does.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender</guid>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-09-26-embeddings-recommender/images/m.png" medium="image" type="image/png" width="700" height="402"/>
    </item>
    <item>
      <title>Image-to-image translation with pix2pix</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</link>
      <description>Conditional GANs (cGANs) may be used to generate one type of object based on another - e.g., a map based on a photo, or a color video based on black-and-white. Here, we show how to implement the pix2pix approach with Keras and eager execution.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <category>Unsupervised Learning</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix</guid>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png" medium="image" type="image/png" width="842" height="536"/>
    </item>
    <item>
      <title>Attention-based Image Captioning with Keras</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</link>
      <description>Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning</guid>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning/images/showattendandtell.png" medium="image" type="image/png" width="627" height="269"/>
    </item>
    <item>
      <title>Neural style transfer with eager execution and Keras</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</link>
      <description>Continuing our series on combining Keras with TensorFlow eager execution, we show how to implement neural style transfer in a straightforward way. Based on this easy-to-adapt example, you can easily perform style transfer on your own images.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer</guid>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-09-10-eager-style-transfer/images/preview.png" medium="image" type="image/png" width="344" height="231"/>
    </item>
    <item>
      <title>Getting started with deep learning in R</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</link>
      <description>Many fields are benefiting from the use of deep learning, and with the R keras, tensorflow and related packages, you can now easily do state of the art deep learning in R. In this post, we want to give some orientation as to how to best get started.</description>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started</guid>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-09-07-getting-started/images/digits.png" medium="image" type="image/png" width="557" height="317"/>
    </item>
    <item>
      <title>Generating images with Keras and TensorFlow eager execution</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</link>
      <description>Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities (often but not always images). We show how to code them using Keras and TensorFlow eager execution.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan</guid>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/images/thumb.png" medium="image" type="image/png" width="240" height="144"/>
    </item>
    <item>
      <title>Attention-based Neural Machine Translation with Keras</title>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</link>
      <description>As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.</description>
      <category>Natural Language Processing</category>
      <category>TensorFlow/Keras</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer</guid>
      <pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/images/attention.png" medium="image" type="image/png" width="606" height="448"/>
    </item>
    <item>
      <title>Classifying physical activity from smartphone data</title>
      <dc:creator>Nick Strayer</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection</link>
      <description>Using Keras to train a convolutional neural network to classify physical activity. The dataset was built from the recordings of 30 subjects performing basic activities and postural transitions while carrying a waist-mounted smartphone with embedded inertial sensors.</description>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection</guid>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-07-17-activity-detection/index_files/figure-html5/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1152" height="768"/>
    </item>
    <item>
      <title>Predicting Sunspot Frequency with Keras</title>
      <dc:creator>Matt Dancho</dc:creator>
      <dc:creator>Sigrid Keydana</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</link>
      <description>In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.</description>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm</guid>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-06-25-sunspots-lstm/images/backtested_test.png" medium="image" type="image/png" width="800" height="416"/>
    </item>
    <item>
      <title>Simple Audio Classification with Keras</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</link>
      <description>In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.</description>
      <category>TensorFlow/Keras</category>
      <category>Audio Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-06-06-simple-audio-classification-keras</guid>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      <media:content url="https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png" medium="image" type="image/png"/>
    </item>
    <item>
      <title>GPU Workstations in the Cloud with Paperspace</title>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-04-02-rstudio-gpu-paperspace</link>
      <description>If you don't have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU.</description>
      <category>Cloud</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-04-02-rstudio-gpu-paperspace</guid>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-04-02-rstudio-gpu-paperspace/images/paperspace-mnist-cnn.png" medium="image" type="image/png" width="2030" height="1338"/>
    </item>
    <item>
      <title>lime v0.4: The Kitten Picture Edition</title>
      <dc:creator>Thomas Lin Pedersen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</link>
      <description>A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis</description>
      <category>Packages/Releases</category>
      <category>TensorFlow/Keras</category>
      <category>Explainability</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition</guid>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png" medium="image" type="image/png" width="1344" height="672"/>
    </item>
    <item>
      <title>Deep Learning for Cancer Immunotherapy</title>
      <dc:creator>Leon Eyrich Jessen</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</link>
      <description>The aim of this post is to illustrate how deep learning is being applied in cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient's own immune system to fight the cancer.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy</guid>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png" medium="image" type="image/png" width="3000" height="1800"/>
    </item>
    <item>
      <title>Predicting Fraud with Autoencoders and Keras</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</link>
      <description>In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML. The basis of our model will be the Kaggle Credit Card Fraud Detection dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Unsupervised Learning</category>
      <category>Cloud</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder</guid>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-keras-fraud-autoencoder/images/preview.png" medium="image" type="image/png" width="790" height="537"/>
    </item>
    <item>
      <title>Analyzing rtweet Data with kerasformula</title>
      <dc:creator>Pete Mohanty</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</link>
      <description>The kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices. We use kerasformula to predict how popular tweets will be based on how often the tweet was retweeted and favorited.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula</guid>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png" medium="image" type="image/png" width="672" height="480"/>
    </item>
    <item>
      <title>Deep Learning With Keras To Predict Customer Churn</title>
      <dc:creator>Matt Dancho</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</link>
      <description>Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.</description>
      <category>TensorFlow/Keras</category>
      <category>Tabular Data</category>
      <category>Explainability</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn</guid>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png" medium="image" type="image/png" width="2696" height="1696"/>
    </item>
    <item>
      <title>R Interface to Google CloudML</title>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</link>
      <description>We are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including on-demand access to training on GPUs and hyperparameter tuning to optimize key attributes of model architectures.</description>
      <category>Cloud</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml</guid>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png" medium="image" type="image/png" width="394" height="211"/>
    </item>
    <item>
      <title>Classifying Duplicate Questions from Quora with Keras</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</link>
      <description>In this post we will use Keras to classify duplicated questions from Quora. Our implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors)</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora</guid>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png" medium="image" type="image/png" width="1302" height="788"/>
    </item>
    <item>
      <title>Word Embeddings with Keras</title>
      <dc:creator>Daniel Falbel</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</link>
      <description>Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras</guid>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png" medium="image" type="image/png" width="700" height="450"/>
    </item>
    <item>
      <title>Time Series Forecasting with Recurrent Neural Networks</title>
      <dc:creator>François Chollet</dc:creator>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</link>
      <description>In this post, we'll review three advanced techniques for improving the performance and generalization power of recurrent neural networks.  We'll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building.</description>
      <category>TensorFlow/Keras</category>
      <category>Time Series</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks</guid>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png" medium="image" type="image/png" width="6000" height="4000"/>
    </item>
    <item>
      <title>Image Classification on Small Datasets with Keras</title>
      <dc:creator>François Chollet</dc:creator>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</link>
      <description>Having to train an image-classification model using very little data is a common situation, in this article we review three techniques for tackling this problem including feature extraction and fine tuning from a pretrained network.</description>
      <category>TensorFlow/Keras</category>
      <category>Image Recognition &amp; Image Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets</guid>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png" medium="image" type="image/png" width="678" height="453"/>
    </item>
    <item>
      <title>Deep Learning for Text Classification with Keras</title>
      <dc:creator>François Chollet</dc:creator>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</link>
      <description>Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this excerpt from the book Deep Learning with R, you'll learn to classify movie reviews as positive or negative, based on the text content of the reviews.</description>
      <category>TensorFlow/Keras</category>
      <category>Natural Language Processing</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras</guid>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras/images/training-history.png" medium="image" type="image/png" width="1400" height="865"/>
    </item>
    <item>
      <title>tfruns: Tools for TensorFlow Training Runs</title>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</link>
      <description>The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns</guid>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-10-04-tfruns/preview.png" medium="image" type="image/png" width="2006" height="1116"/>
    </item>
    <item>
      <title>Keras for R</title>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</link>
      <description>We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.</description>
      <category>TensorFlow/Keras</category>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r</guid>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-09-06-keras-for-r/preview.png" medium="image" type="image/png" width="669" height="414"/>
    </item>
    <item>
      <title>TensorFlow Estimators</title>
      <dc:creator>Yuan Tang</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</link>
      <description>The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r</guid>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png" medium="image" type="image/png" width="1198" height="796"/>
    </item>
    <item>
      <title>TensorFlow v1.3 Released</title>
      <dc:creator>J.J. Allaire</dc:creator>
      <link>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</link>
      <description>The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.</description>
      <category>Packages/Releases</category>
      <guid>https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released</guid>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <media:content url="https://blogs.rstudio.com/tensorflow/posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png" medium="image" type="image/png" width="3876" height="741"/>
    </item>
  </channel>
</rss>
