[
  {
    "path": "posts/2020-09-29-introducing-torch-for-r/",
    "title": "Please allow me to introduce myself: Torch for R",
    "description": "Today, we are excited to introduce torch, an R package allowing to use PyTorch-like functionality natively from R. No Python installation is required: torch is built directly on top of libtorch, a C++ library that provides the tensor computation and automatic differentiation capabilities essential to building neural networks.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-29",
    "categories": [
      "Packages/Releases",
      "Torch",
      "R"
    ],
    "contents": "\nLast January at rstudio::conf, in that distant past when conferences still used to take place at some physical location, my colleague Daniel gave a talk introducing new features and ongoing development in the tensorflow ecosystem. In the Q&A part, he was asked something unexpected: Were we going to build support for PyTorch? He hesitated; that was in fact the plan, and he had already played around with natively implementing torch tensors at a prior time, but he was not completely certain how well “it” would work.\n“It”, that is an implementation which does not bind to Python Torch, meaning, we don’t install the PyTorch wheel and import it via reticulate. Instead, we delegate to the underlying C++ library libtorch for tensor computations and automatic differentiation, while neural network features – layers, activations, optimizers – are implemented directly in R. Removing the intermediary has at least two benefits: For one, the leaner software stack means fewer possible problems in installation and fewer places to look when troubleshooting. Secondly, through its non-dependence on Python, torch does not require users to install and maintain a suitable Python environment. Depending on operating system and context, this can make an enormous difference: For example, in many organizations employees are not allowed to manipulate privileged software installations on their laptops.\nSo why did Daniel hesitate, and, if I recall correctly, give a not-too-conclusive answer? On the one hand, it was not clear whether compilation against libtorch would, on some operating systems, pose severe difficulties. (It did, but difficulties turned out to be surmountable.)1 On the other, the sheer amount of work involved in re-implementing – not all, but a big amount of – PyTorch in R seemed intimidating. Today, there is still lots of work to be done (we’ll pick up that thread at the end), but the main obstacles have been ovecome, and enough components are available that torch can be useful to the R community. Thus, without further ado, let’s train a neural network.\nYou’re not at your laptop now? Just follow along in the companion notebook on Colaboratory.\nInstallation\ntorch\nInstalling torch is as straightforward as typing\n\n\ninstall.packages(\"torch\")\n\nThis will detect whether you have CUDA installed, and either download the CPU or the GPU version of libtorch. Then, it will install the R package from CRAN. To make use of the very newest features, you can also install the development version from GitHub:\n\n\ndevtools::install_github(\"mlverse/torch\")\n\nTo quickly check the installation, and whether GPU support works fine (assuming that there is a CUDA-capable NVidia GPU), create a tensor on the CUDA device:\n\n\ntorch_tensor(1, device = \"cuda\")\n\n\ntorch_tensor \n 1\n[ CUDAFloatType{1} ]\nIf all our hello torch example did was running a network on, say, simulated data, we could stop here. As we’ll do image classification, however, we need to install another package: torchvision.\ntorchvision\n\n\n\nWhereas torch is where tensors, network modules, and generic data loading functionality live, datatype-specific capabilities are – or will be – provided by dedicated packages. In general, these capabilities comprise three types of things: datasets, tools for pre-processing and data loading, and pre-trained models.\nAs of this writing, PyTorch has dedicated libraries for three domain areas: vision, text, and audio. In R, we plan to proceed analogously – “plan”, because torchtext and torchaudio are still to be created. Right now, torchvision is all we need:\n\n\ninstall.packages(\"torchvision\")\n\nAnd we’re ready to load the data.\nData loading and pre-processing\n\n\nlibrary(torch)\nlibrary(torchvision)\n\nThe list of vision datasets bundled with PyTorch is long, and they’re continually being added to torchvision.\nThe one we need right now is available already, and it’s – MNIST? … not quite: It’s my favorite “MNIST dropin”, Kuzushiji-MNIST (Clanuwat et al. 2018). Like other datasets explicitly created to replace MNIST, it has ten classes – characters, in this case, depicted as grayscale images of resolution 28x28.\nHere are the first 32 characters:\n\n\n\nFigure 1: Kuzushiji MNIST.\n\n\n\nDataset\nThe following code will download the data, separately for training and test sets.\n\n\ntrain_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds <- kmnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\nNote the transform argument. transform_to_tensor takes an image and applies two transformations: First, it normalizes the pixels to the range between 0 and 1. Then, it adds another dimension in front. Why?\nContrary to what you might expect – if until now, you’ve been using keras – the additional dimension is not the batch dimension. Batching will be taken care of by the dataloader, to be introduced next. Instead, this is the channels dimension that in torch, is found before the width and height dimensions by default.\nOne thing I’ve found to be extremely useful about torch is how easy it is to inspect objects. Even though we’re dealing with a dataset, a custom object, and not an R array or even a torch tensor, we can easily peek at what’s inside. Indexing in torch is 1-based, conforming to the R user’s intuitions. Consequently,\n\n\ntrain_ds[1]\n\ngives us the first element in the dataset, an R list of two tensors corresponding to input and target, respectively. (We don’t reproduce the output here, but you can see for yourself in the notebook.)\nLet’s inspect the shape of the input tensor:\n\n\ntrain_ds[1][[1]]$size()\n\n\n[1]  1 28 28\nNow we have the data, we need someone to feed them to a deep learning model, nicely batched and all. In torch, this is the task of data loaders.\nData loader\nEach of training and test sets get their own data loader:\n\n\ntrain_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl <- dataloader(test_ds, batch_size = 32)\n\nAgain, torch makes it easy to verify we did the correct thing. To take a look at the content of the first batch, do\n\n\ntrain_iter <- train_dl$.iter()\ntrain_iter$.next()\n\nFunctionality like this may not seem indispensable when working with a well-known dataset, but will turn out to be very useful when a lot of domain-specific pre-processing is required.\nNow that we’ve seen how to load data, all prerequisites are fulfilled for visualizing them. Here is the code that was used to display the first batch of characters, above:\n\n\npar(mfrow = c(4,8), mar = rep(0, 4))\nimages <- train_dl$.iter()$.next()[[1]][1:32, 1, , ] \nimages %>%\n  purrr::array_tree(1) %>%\n  purrr::map(as.raster) %>%\n  purrr::iwalk(~{plot(.x)})\n\nWe’re ready to define our network – a simple convnet.\nNetwork\nIf you’ve been using keras custom models (or have some experience with PyTorch), the following way of defining a network may not look too surprising.\nYou use nn_module() to define an R6 class that will hold the network’s components. Its layers are created in initialize(); forward() describes what happens during the network’s forward pass. One thing on terminology: In torch, layers are called modules, as are networks. This makes sense: The design is truly modular in that any module can be used as a component in a larger one.\n\n\nnet <- nn_module(\n  \n  \"KMNIST-CNN\",\n  \n  initialize = function() {\n    # in_channels, out_channels, kernel_size, stride = 1, padding = 0\n    self$conv1 <- nn_conv2d(1, 32, 3)\n    self$conv2 <- nn_conv2d(32, 64, 3)\n    self$dropout1 <- nn_dropout2d(0.25)\n    self$dropout2 <- nn_dropout2d(0.5)\n    self$fc1 <- nn_linear(9216, 128)\n    self$fc2 <- nn_linear(128, 10)\n  },\n  \n  forward = function(x) {\n    x %>% \n      self$conv1() %>%\n      nnf_relu() %>%\n      self$conv2() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      self$dropout1() %>%\n      torch_flatten(start_dim = 2) %>%\n      self$fc1() %>%\n      nnf_relu() %>%\n      self$dropout2() %>%\n      self$fc2()\n  }\n)\n\nThe layers – apologies: modules – themselves may look familiar. Unsurprisingly, nn_conv2d() performs two-dimensional convolution; nn_linear() multiplies by a weight matrix and adds a vector of biases. But what are those numbers: nn_linear(128, 10), say?\nIn torch, instead of the number of units in a layer, you specify input and output dimensionalities of the “data” that run through it. Thus, nn_linear(128, 10) has 128 input connections and outputs 10 values – one for every class. In some cases, such as this one, specifying dimensions is easy – we know how many input edges there are (namely, the same as the number of output edges from the previous layer), and we know how many output values we need. But how about this module’s predecessor? How do we arrive at 9216 input connections?\nHere, a bit of calculation is necessary. We go through all actions that happen in forward() – if they change shapes, we keep track of the transformation; if they don’t, we ignore them.\nSo, we start with input tensors of shape batch_size x 1 x 28 x 28. Then,\nnn_conv2d(1, 32, 3) , or equivalently, nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = 3),applies a convolution with kernel size 3, stride 1 (the default) and no padding (the default). We can consult the documentation to look up the resulting output size, or just intuitively reason that with a kernel of size 3 and no padding, the image will shrink by one pixel in each direction, resulting in a spatial resolution of 26 x 26. Per channel, that is. Thus, the actual output shape is batch_size x 32 x 26 x 26 . Next,\nnnf_relu() applies ReLU activation, in no way touching the shape. Next is\nnn_conv2d(32, 64, 3), another convolution with zero padding and kernel size 3. Output size now is batch_size x 64 x 24 x 24 . Now, the second\nnnf_relu() again does nothing to the output shape, but\nnnf_max_pool2d(2) (equivalently: nnf_max_pool2d(kernel_size = 2)) does: It applies max pooling over regions of extension 2 x 2, thus downsizing the output to a format of batch_size x 32 x 12 x 12 . Now,\nnn_dropout2d(0.25) is a no-op, shape-wise, but if we want to apply a linear layer later, we need to merge all of the channels, height and width axes into a single dimension. This is done in\ntorch_flatten(start_dim = 2). Output shape is now batch_size * 9216 , since 64 * 12 * 12 = 9216 . Thus here we have the 9216 input connections fed into the\nnn_linear(9216, 128) discussed above. Again,\nnnf_relu() and nn_dropout2d(0.5) leave dimensions as they are, and finally,\nnn_linear(128, 10) gives us the desired output scores, one for each of the ten classes.\nNow you’ll be thinking, – what if my network is more complicated? Calculations could become pretty cumbersome. Luckily, with torch’s flexibility, there is another way. Since every layer is callable in isolation, we can just … create some sample data and see what happens!\nHere is a sample “image” – or more precisely, a one-item batch containing it:\n\n\nx <- torch_randn(c(1, 1, 28, 28))\n\nWhat if we call the first conv2d module on it?\n\n\nconv1 <- nn_conv2d(1, 32, 3)\nconv1(x)$size()\n\n\n[1]  1 32 26 26\nOr both conv2d modules?\n\n\nconv2 <- nn_conv2d(32, 64, 3)\n(conv1(x) %>% conv2())$size()\n\n\n[1]  1 64 24 24\nAnd so on. This is just one example illustrating how torchs flexibility makes developing neural nets easier.\nBack to the main thread. We instantiate the model, and we ask torch to allocate its weights (parameters) on the GPU:\n\n\nmodel <- net()\nmodel$to(device = \"cuda\")\n\nWe’ll do the same for the input and output data – that is, we’ll move them to the GPU. This is done in the training loop, which we’ll inspect next.\nTraining\nIn torch, when creating an optimizer, we tell it what to operate on, namely, the model’s parameters:\n\n\noptimizer <- optim_adam(model$parameters)\n\nWhat about the loss function? For classification with more than two classes, we use cross entropy, in torch: nnf_cross_entropy(prediction, ground_truth):\n\n\n# this will be called for every batch, see training loop below\nloss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n\nUnlike categorical cross entropy in keras , which would expect prediction to contain probabilities, as obtained by applying a softmax activation, torch’s nnf_cross_entropy() works with the raw outputs (the logits). This is why the network’s last linear layer was not followed by any activation.\nThe training loop, in fact, is a double one: It loops over epochs and batches. For every batch, it calls the model on the input, calculates the loss, and has the optimizer update the weights:\n\n\nfor (epoch in 1:5) {\n\n  l <- c()\n\n  for (b in enumerate(train_dl)) {\n    # make sure each batch's gradient updates are calculated from a fresh start\n    optimizer$zero_grad()\n    # get model predictions\n    output <- model(b[[1]]$to(device = \"cuda\"))\n    # calculate loss\n    loss <- nnf_cross_entropy(output, b[[2]]$to(device = \"cuda\"))\n    # calculate gradient\n    loss$backward()\n    # apply weight updates\n    optimizer$step()\n    # track losses\n    l <- c(l, loss$item())\n  }\n\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(l)))\n}\n\n\nLoss at epoch 1: 1.795564\nLoss at epoch 2: 1.540063\nLoss at epoch 3: 1.495343\nLoss at epoch 4: 1.461649\nLoss at epoch 5: 1.446628\nAlthough there is a lot more that could be done – calculate metrics or evaluate performance on a validation set, for example – the above is a typical (if simple) template for a torch training loop.\nThe optimizer-related idioms in particular\n\n\noptimizer$zero_grad()\n# ...\nloss$backward()\n# ...\noptimizer$step()\n\nyou’ll keep encountering over and over.\nFinally, let’s evaluate model performance on the test set.\nEvaluation\nPutting a model in eval mode tells torch not to calculate gradients and perform backprop during the operations that follow:\n\n\nmodel$eval()\n\nWe iterate over the test set, keeping track of losses and accuracies obtained on the batches.\n\n\ntest_losses <- c()\ntotal <- 0\ncorrect <- 0\n\nfor (b in enumerate(test_dl)) {\n  output <- model(b[[1]]$to(device = \"cuda\"))\n  labels <- b[[2]]$to(device = \"cuda\")\n  loss <- nnf_cross_entropy(output, labels)\n  test_losses <- c(test_losses, loss$item())\n  # torch_max returns a list, with position 1 containing the values \n  # and position 2 containing the respective indices\n  predicted <- torch_max(output$data(), dim = 2)[[2]]\n  total <- total + labels$size(1)\n  # add number of correct classifications in this batch to the aggregate\n  correct <- correct + (predicted == labels)$sum()$item()\n}\n\nmean(test_losses)\n\n\n[1] 1.53784480643349\nHere is mean accuracy, computed as proportion of correct classifications:\n\n\ntest_accuracy <-  correct/total\ntest_accuracy\n\n\n[1] 0.9449\nThat’s it for our first torch example. Where to from here?\nLearn\nTo learn more, check out our vignettes on the torch website. To begin, you may want to check out these in particular:\n“Getting started” series: Build a simple neural network from scratch, starting from low-level tensor manipulation and gradually adding in higher-level features like automatic differentiation and network modules.\nMore on tensors: Tensor creation and indexing\nBackpropagation in torch: autograd\nIf you have questions, or run into problems, please feel free to ask on GitHub or on the RStudio community forum.\nWe need you\nWe very much wish that the R community will find the new functionality useful. But that’s not all. Instead, we hope that you, many of you, will take part in the journey.\nThere is not just a whole framework to be built, including many specialized modules, activation functions, optimizers and schedulers, with more of each being added continuously, on the Python side.\nThere is not just that whole “bag of data types” to be taken care of (images, text, audio…), each of which demand their own pre-processing and data-loading functionality. As everyone knows from experience, ease of data preparation is a, perhaps the essential factor in how usable a framework is.\nThen, there is the ever-expanding ecosystem of libraries built on top of PyTorch: PySyft and CrypTen for privacy-preserving machine learning, PyTorch Geometric for deep learning on manifolds, and Pyro for probabilistic programming, to name just a few.\nAll this is much more than can be done by one or two people: We need your help! Contributions are greatly welcomed at absolutely any scale:\nAdd or improve documentation, add introductory examples\nImplement missing layers (modules), activations, helper functions…\nImplement model architectures\nPort some of the PyTorch ecosystem\nOne component that should be of special interest to the R community is Torch distributions, the basis for probabilistic computation. This package is built upon by e.g. the aforementioned Pyro; at the same time, the distributions that live there are used in probabilistic neural networks or normalizing flows.\nTo reiterate, participation from the R community is greatly encouraged (more than that – fervently hoped for!). Have fun with torch, and thanks for reading!\n\n\nClanuwat, Tarin, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. 2018. “Deep Learning for Classical Japanese Literature.” December 3, 2018. http://arxiv.org/abs/cs.CV/1812.01718.\n\n\nIn a nutshell, Javier had the idea of wrapping libtorch into lantern, a C interface to libtorch, thus avoiding cross-compiler issues between MinGW and Visual Studio.↩︎\n",
    "preview": "posts/2020-09-29-introducing-torch-for-r/images/pt.png",
    "last_modified": "2020-09-17T20:58:55+02:00",
    "input_file": "introducing_torch_for_R.utf8.md",
    "preview_width": 919,
    "preview_height": 264
  },
  {
    "path": "posts/2020-09-01-weather-prediction/",
    "title": "An introduction to weather forecasting with deep learning",
    "description": "A few weeks ago, we showed how to forecast chaotic dynamical systems with deep learning, augmented by a custom constraint derived from domain-specific insight. Global weather is a chaotic system, but of much higher complexity than many tasks commonly addressed with machine and/or deep learning. In this post, we provide a practical introduction featuring a simple deep learning baseline for atmospheric forecasting. While far away from being competitive, it serves to illustrate how more sophisticated and compute-intensive models may approach that formidable task by means of methods situated on the \"black-box end\" of the continuum.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-09-01",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series"
    ],
    "preview": "posts/2020-09-01-weather-prediction/images/thumb.png",
    "last_modified": "2020-09-01T15:19:45+02:00",
    "input_file": "weather_prediction_deep_learning.utf8.md",
    "preview_width": 1667,
    "preview_height": 923
  },
  {
    "path": "posts/2020-08-24-training-imagenet-with-r/",
    "title": "Training ImageNet with R",
    "description": "This post explores how to train large datasets with TensorFlow and R. Specifically, we present how to download and repartition ImageNet, followed by training ImageNet across multiple GPUs in distributed environments using TensorFlow and Apache Spark.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Distributed Computing",
      "Data Management"
    ],
    "preview": "posts/2020-08-24-training-imagenet-with-r/images/fishing-net.jpg",
    "last_modified": "2020-08-21T19:44:12-07:00",
    "input_file": "training-imagenet-with-r.utf8.md"
  },
  {
    "path": "posts/2020-08-18-deepfake/",
    "title": "Deepfake detection challenge from R",
    "description": "A couple of months ago, Amazon, Facebook, Microsoft, and other contributors initiated a challenge consisting of telling apart real and AI-generated (\"fake\") videos. We show how to approach this challenge from R.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-08-18",
    "categories": [
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2020-08-18-deepfake/files/frame_2.jpg",
    "last_modified": "2020-08-24T09:26:16-07:00",
    "input_file": "deepfake.utf8.md"
  },
  {
    "path": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/",
    "title": "FNN-VAE for noisy time series forecasting",
    "description": "In the last part of this mini-series on forecasting with false nearest neighbors (FNN) loss, we replace the LSTM autoencoder from the previous post by a convolutional VAE, resulting in equivalent prediction performance but significantly lower training time. In addition, we find that FNN regularization is of great help when an underlying deterministic process is obscured by substantial noise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-31",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "preview": "posts/2020-07-31-fnn-vae-for-noisy-timeseries/images/kb.jpg",
    "last_modified": "2020-07-31T16:14:46+02:00",
    "input_file": "fnn_vae_for_noisy_timeseries.utf8.md"
  },
  {
    "path": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/",
    "title": "State-of-the-art NLP models from R",
    "description": "Nowadays, Microsoft, Google, Facebook, and OpenAI are sharing lots of state-of-the-art models in the field of Natural Language Processing. However, fewer materials exist how to use these models from R. In this post, we will show how R users can access and benefit from these models as well.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2020-07-30",
    "categories": [
      "Natural Language Processing"
    ],
    "preview": "posts/2020-07-30-state-of-the-art-nlp-models-from-r/files/dino.jpg",
    "last_modified": "2020-07-30T09:47:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-29-parallelized-sampling/",
    "title": "Parallelized sampling using exponential variates",
    "description": "How can the seemingly iterative process of weighted sampling without replacement be transformed into something highly parallelizable? Turns out a well-known technique based on exponential variates accomplishes exactly that.",
    "author": [
      {
        "name": "Yitao Li",
        "url": {}
      }
    ],
    "date": "2020-07-29",
    "categories": [
      "Concepts",
      "Distributed Computing"
    ],
    "preview": "posts/2020-07-29-parallelized-sampling/images/dice.jpg",
    "last_modified": "2020-08-14T11:00:16-07:00",
    "input_file": "parallelized-sampling.utf8.md"
  },
  {
    "path": "posts/2020-07-20-fnn-lstm/",
    "title": "Time series prediction with FNN-LSTM",
    "description": "In a recent post, we showed how an LSTM autoencoder, regularized by false nearest neighbors (FNN) loss, can be used to reconstruct the attractor of a nonlinear, chaotic dynamical system. Here, we explore how that same technique assists in prediction. Matched up with a comparable, capacity-wise, \"vanilla LSTM\", FNN-LSTM improves performance on a set of very different, real-world datasets, especially for the initial steps in a multi-step forecast.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-07-20",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "preview": "posts/2020-07-20-fnn-lstm/images/old_faithful.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-16-sparklyr-1.3.0-released/",
    "title": "sparklyr 1.3: Higher-order Functions, Avro and Custom Serializers",
    "description": "Sparklyr 1.3 is now available, featuring exciting new functionalities such as integration of Spark higher-order functions and data import/export in Avro and in user-defined serialization formats.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-07-16",
    "categories": [
      "Packages/Releases",
      "Distributed Computing"
    ],
    "preview": "posts/2020-07-16-sparklyr-1.3.0-released/images/sparklyr-1.3.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-06-24-deep-attractors/",
    "title": "Deep attractors: Where deep learning meets chaos",
    "description": "In nonlinear dynamics, when the state space is thought to be multidimensional but all we have for data is just a univariate time series, one may attempt to reconstruct the true space via delay coordinate embeddings. However, it is not clear a priori how to choose dimensionality and time lag of the reconstruction space. In this post, we show how to use an autoencoder architecture to circumvent the problem: Given just a scalar series of observations, the autoencoder directly learns to represent attractors of chaotic systems in adequate dimensionality.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-06-24",
    "categories": [
      "R",
      "TensorFlow/Keras",
      "Time Series",
      "Unsupervised Learning"
    ],
    "preview": "posts/2020-06-24-deep-attractors/images/x_z.gif",
    "last_modified": "2020-07-30T19:02:42+02:00",
    "input_file": "deep_attractors.utf8.md"
  },
  {
    "path": "posts/2020-05-29-pixelcnn/",
    "title": "Easy PixelCNN with tfprobability",
    "description": "PixelCNN is a deep learning architecture - or bundle of architectures - designed to generate highly realistic-looking images. To use it, no reverse-engineering of arXiv papers or search for reference implementations is required: TensorFlow Probability and its R wrapper, tfprobability, now include a PixelCNN distribution that can be used to train a straightforwardly-defined neural network in a parameterizable way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-29",
    "categories": [
      "R",
      "Image Recognition & Image Processing",
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "preview": "posts/2020-05-29-pixelcnn/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2250,
    "preview_height": 1140
  },
  {
    "path": "posts/2020-05-15-model-inversion-attacks/",
    "title": "Hacking deep learning: model inversion attack by example",
    "description": "Compared to other applications, deep learning models might not seem too likely as victims of privacy attacks. However, methods exist to determine whether an entity was used in the training set (an adversarial attack called member inference), and techniques subsumed under \"model inversion\" allow to reconstruct raw data input given just model output (and sometimes, context information). This post shows an end-to-end example of model inversion, and explores mitigation strategies using TensorFlow Privacy.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-05-15",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2020-05-15-model-inversion-attacks/images/results.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2378,
    "preview_height": 1563
  },
  {
    "path": "posts/2020-04-29-encrypted_keras_with_syft/",
    "title": "Towards privacy: Encrypted deep learning with Syft and Keras",
    "description": "Deep learning need not be irreconcilable with privacy protection. Federated learning enables on-device, distributed model training; encryption keeps model and gradient updates private; differential privacy prevents the training data from leaking. As of today, private and secure deep learning is an emerging technology. In this post, we introduce Syft, an open-source framework that integrates with PyTorch as well as TensorFlow. In an example use case, we obtain private predictions from a Keras model.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-29",
    "categories": [
      "R",
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2020-04-29-encrypted_keras_with_syft/images/thumb.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-21-sparklyr-1.2.0-released/",
    "title": "sparklyr 1.2: Foreach, Spark 3.0 and Databricks Connect",
    "description": "A new sparklyr release is now available. This sparklyr 1.2 release features new functionalities such as support for Databricks Connect, a Spark backend for the 'foreach' package, inter-op improvements for working with Spark 3.0 preview, as well as a number of bug fixes and improvements addressing user-visible pain points.",
    "author": [
      {
        "name": "Yitao Li",
        "url": "https://github.com/yl790"
      }
    ],
    "date": "2020-04-21",
    "categories": [
      "R",
      "Packages/Releases",
      "Distributed Computing"
    ],
    "preview": "posts/2020-04-21-sparklyr-1.2.0-released/images/sparklyr.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1241,
    "preview_height": 307
  },
  {
    "path": "posts/2020-04-13-pins-04/",
    "title": "pins 0.4: Versioning",
    "description": "A new release of pins is available on CRAN today. This release adds support to time travel across dataset versions, which improves collaboration and protects your code from breaking when remote resources change unexpectedly.",
    "author": [
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2020-04-13",
    "categories": [
      "R",
      "Packages/Releases",
      "Data Management"
    ],
    "preview": "posts/2020-04-13-pins-04/images/thumb.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-08-tf-federated-intro/",
    "title": "A first look at federated learning with TensorFlow",
    "description": "The term \"federated learning\" was coined to describe a form of distributed model training where the data remains on client devices, i.e., is never shipped to the coordinating server. In this post, we introduce central concepts and run first experiments with TensorFlow Federated, using R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-04-08",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2020-04-08-tf-federated-intro/images/federated_learning.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1122,
    "preview_height": 570
  },
  {
    "path": "posts/2020-04-01-rstudio-ai-blog/",
    "title": "Introducing: The RStudio AI Blog",
    "description": "This blog just got a new title: RStudio AI Blog. We explain why.",
    "author": [
      {
        "name": "The Multiverse Team",
        "url": {}
      }
    ],
    "date": "2020-03-30",
    "categories": [
      "Meta"
    ],
    "preview": "posts/2020-04-01-rstudio-ai-blog/images/thumb.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-19-kl-divergence/",
    "title": "Infinite surprise - the iridescent personality of Kullback-Leibler divergence",
    "description": "Kullback-Leibler divergence is not just used to train variational autoencoders or Bayesian networks (and not just a hard-to-pronounce thing). It is a fundamental concept in information theory, put to use in a vast range of applications. Most interestingly, it's not always about constraint, regularization or compression. Quite on the contrary, sometimes it is about novelty, discovery and surprise.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-02-19",
    "categories": [
      "Probabilistic ML/DL",
      "Concepts"
    ],
    "preview": "posts/2020-02-19-kl-divergence/images/ultimatemachine.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-24-numpy-broadcasting/",
    "title": "NumPy-style broadcasting for R TensorFlow users",
    "description": "Broadcasting, as done by Python's scientific computing library NumPy, involves dynamically extending shapes so that arrays of different sizes may be passed to operations that expect conformity - such as adding or multiplying elementwise. In NumPy, the way broadcasting works is specified exactly; the same rules apply to TensorFlow operations. For anyone who finds herself, occasionally, consulting Python code, this post strives to explain.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "preview": "posts/2020-01-24-numpy-broadcasting/images/thumb.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-13-mixed-precision-training/",
    "title": "First experiments with TensorFlow mixed-precision training",
    "description": "TensorFlow 2.1, released last week, allows for mixed-precision training, making use of the Tensor Cores available in the most recent NVidia GPUs. In this post, we report first experimental results and provide some background on what this is all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2020-01-13",
    "categories": [
      "TensorFlow/Keras"
    ],
    "preview": "posts/2020-01-13-mixed-precision-training/images/tc.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 589,
    "preview_height": 399
  },
  {
    "path": "posts/2019-12-20-differential-privacy/",
    "title": "Differential Privacy with TensorFlow",
    "description": "Differential Privacy guarantees that results of a database query are basically independent of the presence in the data of a single individual. Applied to machine learning, we expect that no single training example influences the parameters of the trained model in a substantial way. This post introduces TensorFlow Privacy, a library built on top of TensorFlow, that can be used to train differentially private deep learning models from R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-20",
    "categories": [
      "Privacy & Security",
      "TensorFlow/Keras",
      "Time Series"
    ],
    "preview": "posts/2019-12-20-differential-privacy/images/cat.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1206
  },
  {
    "path": "posts/2019-12-18-tfhub-0.7.0/",
    "title": "tfhub: R interface to TensorFlow Hub",
    "description": "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      }
    ],
    "date": "2019-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "preview": "posts/2019-12-18-tfhub-0.7.0/images/tfhub.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1365,
    "preview_height": 909
  },
  {
    "path": "posts/2019-12-10-variational-gaussian-process/",
    "title": "Gaussian Process Regression with tfprobability",
    "description": "Continuing our tour of applications of TensorFlow Probability (TFP), after Bayesian Neural Networks, Hamiltonian Monte Carlo and State Space Models, here we show an example of Gaussian Process Regression. In fact, what we see is a rather \"normal\" Keras network, defined and trained in pretty much the usual way, with TFP's Variational Gaussian Process layer pulling off all the magic.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-12-10",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-12-10-variational-gaussian-process/images/kernel_cookbook.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 818,
    "preview_height": 352
  },
  {
    "path": "posts/2019-11-27-gettingstarted-2020/",
    "title": "Getting started with Keras from R - the 2020 edition",
    "description": "Looking for materials to get started with deep learning from R? This post presents useful tutorials, guides, and background documentation on the new TensorFlow for R website.  Advanced users will find pointers to applications of new release 2.0 (or upcoming 2.1!) features alluded to in the recent TensorFlow 2.0 post.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-27",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-11-27-gettingstarted-2020/images/website.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1591,
    "preview_height": 725
  },
  {
    "path": "posts/2019-11-13-variational-convnet/",
    "title": "Variational convnets with tfprobability",
    "description": "In a Bayesian neural network, layer weights are distributions, not tensors. Using tfprobability, the R wrapper to TensorFlow Probability, we can build regular Keras models that have probabilistic layers, and thus get uncertainty estimates \"for free\". In this post, we show how to define, train and obtain predictions from a probabilistic convolutional neural network.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-13",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-11-13-variational-convnet/images/bbb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 796,
    "preview_height": 378
  },
  {
    "path": "posts/2019-11-07-tfp-cran/",
    "title": "tfprobability 0.8 on CRAN: Now how can you use it?",
    "description": "Part of the r-tensorflow ecosystem, tfprobability is an R wrapper to TensorFlow Probability, the Python probabilistic programming framework developed by Google. We take the occasion of tfprobability's acceptance on CRAN to give a high-level introduction, highlighting interesting use cases and applications.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-11-07",
    "categories": [
      "Probabilistic ML/DL",
      "Packages/Releases",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-11-07-tfp-cran/images/tfprobability.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 518,
    "preview_height": 600
  },
  {
    "path": "posts/2019-10-23-gpt-2/",
    "title": "Innocent unicorns considered harmful? How to experiment with GPT-2 from R",
    "description": "Is society ready to deal with challenges brought about by artificially-generated  information - fake images, fake videos, fake text? While this post won't answer that question, it should help form an opinion on the threat exerted by fake text as of this writing, autumn 2019.  We introduce gpt2, an R package that wraps OpenAI's public implementation of GPT-2, the language model that early this year surprised the NLP community with the unprecedented quality of its creations.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      },
      {
        "name": "Javier Luraschi",
        "url": {}
      }
    ],
    "date": "2019-10-23",
    "categories": [
      "Natural Language Processing",
      "Packages/Releases"
    ],
    "preview": "posts/2019-10-23-gpt-2/images/thumb.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-08-tf2-whatchanges/",
    "title": "TensorFlow 2.0 is here - what changes for R users?",
    "description": "TensorFlow 2.0 was finally released last week. As R users we have two kinds of questions. First, will my keras code still run? And second, what is it that changes? In this post, we answer both and, then, give a tour of exciting new developments in the r-tensorflow ecosystem.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-08",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "preview": "posts/2019-10-08-tf2-whatchanges/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 400
  },
  {
    "path": "posts/2019-10-03-intro-to-hmc/",
    "title": "On leapfrogs, crashing satellites, and going nuts: A very first conceptual introduction to Hamiltonian Monte Carlo",
    "description": "TensorFlow Probability, and its R wrapper tfprobability, provide Markov Chain Monte Carlo (MCMC) methods that were used in a number of recent posts on this blog. These posts were directed to users already comfortable with the method, and terminology, per se, which readers mainly interested in deep learning won't necessarily be. Here we try to make up leeway, introducing Hamitonian Monte Carlo (HMC) as well as a few often-heard \"buzzwords\" accompanying it, always striving to keep in mind what it is all \"for\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-10-03",
    "categories": [
      "Bayesian Modeling",
      "Concepts"
    ],
    "preview": "posts/2019-10-03-intro-to-hmc/images/mb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 548,
    "preview_height": 345
  },
  {
    "path": "posts/2019-09-30-bert-r/",
    "title": "BERT from R",
    "description": "A deep learning model - BERT from Google AI Research - has yielded state-of-the-art results in a wide variety of Natural Language Processing (NLP) tasks. In this tutorial, we will show how to load and train the BERT model from R, using Keras.",
    "author": [
      {
        "name": "Turgut Abdullayev",
        "url": "https://github.com/henry090"
      }
    ],
    "date": "2019-09-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-09-30-bert-r/images/bert.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 437,
    "preview_height": 367
  },
  {
    "path": "posts/2019-08-29-using-tf-from-r/",
    "title": "So, how come we can use TensorFlow from R?",
    "description": "Have you ever wondered why you can call TensorFlow - mostly known as a Python framework - from R? If not - that's how it should be, as the R packages keras and tensorflow aim to make this process as transparent as possible to the user. But for them to be those helpful genies, someone else first has to tame the Python.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-29",
    "categories": [
      "TensorFlow/Keras",
      "Meta",
      "Concepts"
    ],
    "preview": "posts/2019-08-29-using-tf-from-r/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 739,
    "preview_height": 516
  },
  {
    "path": "posts/2019-08-23-unet/",
    "title": "Image segmentation with U-Net",
    "description": "In image segmentation, every pixel of an image is assigned a class. Depending on the application, classes could be different cell types; or the task could be binary, as in \"cancer cell yes or no?\". Area of application notwithstanding, the established neural network architecture of choice is U-Net. In this post, we show how to preprocess data and train a U-Net model on the Kaggle Carvana image segmentation data.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-08-23",
    "categories": [
      "Image Recognition & Image Processing",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-08-23-unet/images/unet.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 932
  },
  {
    "path": "posts/2019-07-31-censored-data/",
    "title": "Modeling censored data with tfprobability",
    "description": "In this post we use tfprobability, the R interface to TensorFlow Probability, to model censored data. Again, the exposition is inspired by the treatment of this topic in Richard McElreath's Statistical Rethinking. Instead of cute cats though, we model immaterial entities from the cold world of technology: This post explores durations of CRAN package checks, a dataset that comes with Max Kuhn's parsnip.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-31",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-07-31-censored-data/images/thumb_cropped.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 955,
    "preview_height": 396
  },
  {
    "path": "posts/2019-07-09-feature-columns/",
    "title": "TensorFlow feature columns: Transforming your data recipes-style",
    "description": "TensorFlow feature columns provide useful functionality for preprocessing categorical data and chaining transformations, like bucketization or feature crossing. From R, we use them in popular \"recipes\" style, creating and subsequently refining a feature specification. In this post, we show how using feature specs frees cognitive resources and lets you focus on what you really want to accomplish. What's more, because of its elegance, feature-spec code reads nice and is fun to write as well.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": {}
      },
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-07-09",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "preview": "posts/2019-07-09-feature-columns/images/feature_cols_hier.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1172,
    "preview_height": 678
  },
  {
    "path": "posts/2019-06-25-dynamic_linear_models_tfprobability/",
    "title": "Dynamic linear models with tfprobability",
    "description": "Previous posts featuring tfprobability - the R interface to TensorFlow Probability - have focused on enhancements to deep neural networks (e.g., introducing Bayesian uncertainty estimates) and fitting hierarchical models with Hamiltonian Monte Carlo. This time, we show how to fit time series using dynamic linear models (DLMs), yielding posterior predictive forecasts as well as the smoothed and filtered estimates from the Kálmán filter.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-24",
    "categories": [
      "Probabilistic ML/DL",
      "Time Series"
    ],
    "preview": "posts/2019-06-25-dynamic_linear_models_tfprobability/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2012,
    "preview_height": 1065
  },
  {
    "path": "posts/2019-06-05-uncertainty-estimates-tfprobability/",
    "title": "Adding uncertainty estimates to Keras models with tfprobability",
    "description": "As of today, there is no mainstream road to obtaining uncertainty estimates from neural networks. All that can be said is that, normally, approaches tend to be Bayesian in spirit, involving some way of putting a prior over model weights. This holds true as well for the method presented in this post: We show how to use tfprobability, the R interface to TensorFlow Probability, to add uncertainty estimates to a Keras model in an elegant and conceptually plausible way.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-06-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts"
    ],
    "preview": "posts/2019-06-05-uncertainty-estimates-tfprobability/images/uci_both.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2020,
    "preview_height": 1020
  },
  {
    "path": "posts/2019-05-24-varying-slopes/",
    "title": "Hierarchical partial pooling, continued: Varying slopes models with TensorFlow Probability",
    "description": "This post builds on our recent introduction to multi-level modeling with tfprobability, the R wrapper to TensorFlow Probability. We show how to pool not just mean values (\"intercepts\"), but also relationships (\"slopes\"), thus enabling models to learn from data in an even broader way. Again, we use an example from Richard McElreath's \"Statistical Rethinking\"; the terminology as well as the way we present this topic are largely owed to this book.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-24",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-05-24-varying-slopes/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 509,
    "preview_height": 249
  },
  {
    "path": "posts/2019-05-06-tadpoles-on-tensorflow/",
    "title": "Tadpoles on TensorFlow: Hierarchical partial pooling with tfprobability",
    "description": "This post is a first introduction to MCMC modeling with tfprobability, the R interface to TensorFlow Probability (TFP). Our example is a multi-level model describing tadpole mortality, which may be known to the reader from Richard McElreath's wonderful \"Statistical Rethinking\".",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-05-06",
    "categories": [
      "Bayesian Modeling",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-05-06-tadpoles-on-tensorflow/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1612,
    "preview_height": 659
  },
  {
    "path": "posts/2019-04-24-autoregressive-flows/",
    "title": "Experimenting with autoregressive flows in TensorFlow Probability",
    "description": "Continuing from the recent introduction to bijectors in TensorFlow Probability (TFP), this post brings autoregressivity to the table. Using TFP through the new R package tfprobability, we look at the implementation of masked autoregressive flows (MAF) and put them to use on two different datasets.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-24",
    "categories": [
      "Probabilistic ML/DL",
      "Unsupervised Learning",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2019-04-24-autoregressive-flows/images/made.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 686,
    "preview_height": 398
  },
  {
    "path": "posts/2019-04-16-autokeras/",
    "title": "Auto-Keras: Tuning-free deep learning from R",
    "description": "Sometimes in deep learning, architecture design and hyperparameter tuning pose substantial challenges. Using Auto-Keras, none of these is needed: We start a search procedure and extract the best-performing model. This post presents Auto-Keras in action on the well-known MNIST dataset.",
    "author": [
      {
        "name": "Juan Cruz Rodriguez",
        "url": "https://jcrodriguez.rbind.io"
      }
    ],
    "date": "2019-04-16",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "preview": "posts/2019-04-16-autokeras/images/thumbnail.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-05-bijectors-flows/",
    "title": "Getting into the flow: Bijectors in TensorFlow Probability",
    "description": "Normalizing flows are one of the lesser known, yet fascinating and successful architectures in unsupervised deep learning. In this post we provide a basic introduction to flows using tfprobability, an R wrapper to TensorFlow Probability. Upcoming posts will build on this, using more complex flows on more complex data.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-04-05",
    "categories": [
      "Probabilistic ML/DL",
      "TensorFlow/Keras",
      "Concepts",
      "Unsupervised Learning"
    ],
    "preview": "posts/2019-04-05-bijectors-flows/images/flows.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 904,
    "preview_height": 325
  },
  {
    "path": "posts/2019-03-15-concepts-way-to-dl/",
    "title": "Math, code, concepts: A third road to deep learning",
    "description": "Not everybody who wants to get into deep learning has a strong background in math or programming. This post elaborates on a concepts-driven, abstraction-based way to learn what it's all about.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-03-15",
    "categories": [
      "Meta",
      "Concepts"
    ],
    "preview": "posts/2019-03-15-concepts-way-to-dl/images/prev.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-07-audio-background/",
    "title": "Audio classification with Keras: Looking closer at the non-deep learning parts",
    "description": "Sometimes, deep learning is seen - and welcomed - as a way to avoid laborious preprocessing of data. However, there are cases where preprocessing of sorts does not only help improve prediction, but constitutes a fascinating topic in itself. One such case is audio classification. In this post, we build on a previous post on this blog, this time focusing on explaining some of the non-deep learning background. We then link the concepts explained to updated for near-future releases TensorFlow code.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-02-07",
    "categories": [
      "TensorFlow/Keras",
      "Concepts",
      "Audio Processing"
    ],
    "preview": "posts/2019-02-07-audio-background/images/seven2.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1714,
    "preview_height": 846
  },
  {
    "path": "posts/2019-01-24-vq-vae/",
    "title": "Discrete Representation Learning with VQ-VAE and TensorFlow Probability",
    "description": "Mostly when thinking of Variational Autoencoders (VAEs), we picture the prior as an isotropic Gaussian. But this is by no means a necessity. The Vector Quantised Variational Autoencoder (VQ-VAE) described in van den Oord et al's \"Neural Discrete Representation Learning\" features a discrete latent space that allows to learn impressively concise latent representations. In this post, we combine elements of Keras, TensorFlow, and TensorFlow Probability to see if we can generate convincing letters resembling those in Kuzushiji-MNIST.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "preview": "posts/2019-01-24-vq-vae/images/thumb1.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 510,
    "preview_height": 287
  },
  {
    "path": "posts/2019-01-08-getting-started-with-tf-probability/",
    "title": "Getting started with TensorFlow Probability from R",
    "description": "TensorFlow Probability offers a vast range of functionality ranging from distributions over probabilistic network layers to probabilistic inference. It works seamlessly with core TensorFlow and (TensorFlow) Keras. In this post, we provide a short introduction to the distributions layer and then, use it for sampling and calculating probabilities in a Variational Autoencoder.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2019-01-08",
    "categories": [
      "TensorFlow/Keras",
      "Probabilistic ML/DL",
      "Unsupervised Learning"
    ],
    "preview": "posts/2019-01-08-getting-started-with-tf-probability/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 884,
    "preview_height": 584
  },
  {
    "path": "posts/2018-12-18-object-detection-concepts/",
    "title": "Concepts in object detection",
    "description": "As shown in a previous post, naming and locating a single object in an image is a task that may be approached in a straightforward way. This is not the same with general object detection, though - naming and locating several objects at once, with no prior information about how many objects are supposed to be detected.\nIn this post, we explain the steps involved in coding a basic single-shot object detector: Not unlike SSD (Single-shot Multibox Detector), but simplified and designed not for best performance, but comprehensibility.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-12-18",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-12-18-object-detection-concepts/images/results.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-26-embeddings-fun-and-profit/",
    "title": "Entity embeddings for fun and profit",
    "description": "Embedding layers are not just useful when working with language data. As \"entity embeddings\", they've recently become famous for applications on tabular, small-scale data. In this post, we exemplify two possible use cases, also drawing attention to what not to expect.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "preview": "posts/2018-11-26-embeddings-fun-and-profit/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 820,
    "preview_height": 410
  },
  {
    "path": "posts/2018-11-12-uncertainty_estimates_dropout/",
    "title": "You sure? A Bayesian approach to obtaining uncertainty estimates from neural networks",
    "description": "In deep learning, there is no obvious way of obtaining uncertainty estimates. In 2016, Gal and Ghahramani proposed a method that is both theoretically grounded and practical: use dropout at test time. In this post, we introduce a refined version of this method (Gal et al. 2017) that has the network itself learn how uncertain it is.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-12",
    "categories": [
      "Image Recognition & Image Processing",
      "Probabilistic ML/DL",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2018-11-12-uncertainty_estimates_dropout/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2046,
    "preview_height": 872
  },
  {
    "path": "posts/2018-11-05-naming-locating-objects/",
    "title": "Naming and locating objects in images",
    "description": "Object detection (the act of classifying and localizing multiple objects in a scene) is one of the more difficult, but very relevant in practice deep learning tasks. We'll build up to it in several posts. Here we start with the simpler tasks of naming and locating a single object.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-11-05",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-11-05-naming-locating-objects/images/preds_train.jpg",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-22-mmd-vae/",
    "title": "Representation learning with MMD-VAE",
    "description": "Like GANs, variational autoencoders (VAEs) are often used to generate images. However, VAEs add an additional promise: namely, to model an underlying latent space. Here, we first look at a typical implementation that maximizes the evidence lower bound. Then, we compare it to one of the more recent competitors, MMD-VAE, from the Info-VAE (information maximizing VAE) family.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-22",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-10-22-mmd-vae/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 178
  },
  {
    "path": "posts/2018-10-11-activations-intro/",
    "title": "Winner takes all: A look at activations and cost functions",
    "description": "Why do we use the activations we use, and how do they relate to the cost functions they tend to co-appear with? In this post we provide a conceptual introduction.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-11",
    "categories": [
      "TensorFlow/Keras",
      "Concepts"
    ],
    "preview": "posts/2018-10-11-activations-intro/images/output.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 384
  },
  {
    "path": "posts/2018-10-02-eager-wrapup/",
    "title": "More flexible models with TensorFlow eager execution and Keras",
    "description": "Advanced applications like generative adversarial networks, neural style transfer, and the attention mechanism ubiquitous in natural language processing used to be not-so-simple to implement with the Keras declarative coding paradigm. Now, with the advent of TensorFlow eager execution, things have changed. This post explores using eager execution with R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-10-02",
    "categories": [
      "TensorFlow/Keras"
    ],
    "preview": "posts/2018-10-02-eager-wrapup/images/m.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 384,
    "preview_height": 126
  },
  {
    "path": "posts/2018-09-26-embeddings-recommender/",
    "title": "Collaborative filtering with embeddings",
    "description": "Embeddings are not just for use in natural language processing. Here we apply embeddings to a common task in collaborative filtering - predicting user ratings - and on our way, strive for a better understanding of what an embedding layer really does.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-26",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "preview": "posts/2018-09-26-embeddings-recommender/images/m.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 402
  },
  {
    "path": "posts/2018-09-20-eager-pix2pix/",
    "title": "Image-to-image translation with pix2pix",
    "description": "Conditional GANs (cGANs) may be used to generate one type of object based on another - e.g., a map based on a photo, or a color video based on black-and-white. Here, we show how to implement the pix2pix approach with Keras and eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-20",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing",
      "Unsupervised Learning"
    ],
    "preview": "posts/2018-09-20-eager-pix2pix/images/pix2pixlosses.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 842,
    "preview_height": 536
  },
  {
    "path": "posts/2018-09-17-eager-captioning/",
    "title": "Attention-based Image Captioning with Keras",
    "description": "Image captioning is a challenging task at intersection of vision and language. Here, we demonstrate using Keras and eager execution to incorporate an attention mechanism that allows the network to concentrate on image features relevant to the current state of text generation.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-17",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-09-17-eager-captioning/images/showattendandtell.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 627,
    "preview_height": 269
  },
  {
    "path": "posts/2018-09-10-eager-style-transfer/",
    "title": "Neural style transfer with eager execution and Keras",
    "description": "Continuing our series on combining Keras with TensorFlow eager execution, we show how to implement neural style transfer in a straightforward way. Based on this easy-to-adapt example, you can easily perform style transfer on your own images.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-10",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-09-10-eager-style-transfer/images/preview.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 344,
    "preview_height": 231
  },
  {
    "path": "posts/2018-09-07-getting-started/",
    "title": "Getting started with deep learning in R",
    "description": "Many fields are benefiting from the use of deep learning, and with the R keras, tensorflow and related packages, you can now easily do state of the art deep learning in R. In this post, we want to give some orientation as to how to best get started.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-09-07",
    "categories": [
      "TensorFlow/Keras"
    ],
    "preview": "posts/2018-09-07-getting-started/images/digits.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 557,
    "preview_height": 317
  },
  {
    "path": "posts/2018-08-26-eager-dcgan/",
    "title": "Generating images with Keras and TensorFlow eager execution",
    "description": "Generative adversarial networks (GANs) are a popular deep learning approach to generating new entities (often but not always images). We show how to code them using Keras and TensorFlow eager execution.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-08-26",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-08-26-eager-dcgan/images/thumb.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 144
  },
  {
    "path": "posts/2018-07-30-attention-layer/",
    "title": "Attention-based Neural Machine Translation with Keras",
    "description": "As sequence to sequence prediction tasks get more involved, attention mechanisms have proven helpful. A prominent example is neural machine translation. Following a recent Google Colaboratory notebook, we show how to implement attention in R.",
    "author": [
      {
        "name": "Sigrid Keydana",
        "url": {}
      }
    ],
    "date": "2018-07-30",
    "categories": [
      "Natural Language Processing",
      "TensorFlow/Keras"
    ],
    "preview": "posts/2018-07-30-attention-layer/images/attention.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 606,
    "preview_height": 448
  },
  {
    "path": "posts/2018-07-17-activity-detection/",
    "title": "Classifying physical activity from smartphone data",
    "description": "Using Keras to train a convolutional neural network to classify physical activity. The dataset was built from the recordings of 30 subjects performing basic activities and postural transitions while carrying a waist-mounted smartphone with embedded inertial sensors.",
    "author": [
      {
        "name": "Nick Strayer",
        "url": "http://nickstrayer.me"
      }
    ],
    "date": "2018-07-17",
    "categories": [],
    "preview": "posts/2018-07-17-activity-detection/index_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "posts/2018-06-25-sunspots-lstm/",
    "title": "Predicting Sunspot Frequency with Keras",
    "description": "In this post we will examine making time series predictions using the sunspots dataset that ships with base R. Sunspots are dark spots on the sun, associated with lower temperature. Our post will focus on both how to apply deep learning to time series forecasting, and how to properly apply cross validation in this domain.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      },
      {
        "name": "Sigrid Keydana",
        "url": "https://github.com/skeydan"
      }
    ],
    "date": "2018-06-25",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "preview": "posts/2018-06-25-sunspots-lstm/images/backtested_test.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 416
  },
  {
    "path": "posts/2018-06-06-simple-audio-classification-keras/",
    "title": "Simple Audio Classification with Keras",
    "description": "In this tutorial we will build a deep learning model to classify words. We will use the Speech Commands dataset which consists of 65,000 one-second audio files of people saying 30 different words.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-06-06",
    "categories": [
      "TensorFlow/Keras",
      "Audio Processing"
    ],
    "preview": "https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-02-rstudio-gpu-paperspace/",
    "title": "GPU Workstations in the Cloud with Paperspace",
    "description": "If you don't have local access to a modern NVIDIA GPU, your best bet is typically to run GPU intensive training jobs in the cloud. Paperspace is a cloud service that provides access to a fully preconfigured Ubuntu 16.04 desktop environment equipped with a GPU.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-04-02",
    "categories": [
      "Cloud"
    ],
    "preview": "posts/2018-04-02-rstudio-gpu-paperspace/images/paperspace-mnist-cnn.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2030,
    "preview_height": 1338
  },
  {
    "path": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/",
    "title": "lime v0.4: The Kitten Picture Edition",
    "description": "A new major release of lime has landed on CRAN. lime is an R port of the Python library of the same name by Marco Ribeiro that allows the user to pry open black box machine learning models and explain their outcomes on a per-observation basis",
    "author": [
      {
        "name": "Thomas Lin Pedersen",
        "url": "https://github.com/thomasp85"
      }
    ],
    "date": "2018-03-09",
    "categories": [
      "Packages/Releases",
      "TensorFlow/Keras",
      "Explainability",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2018-03-09-lime-v04-the-kitten-picture-edition/images/unnamed-chunk-8-1.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 672
  },
  {
    "path": "posts/2018-01-29-dl-for-cancer-immunotherapy/",
    "title": "Deep Learning for Cancer Immunotherapy",
    "description": "The aim of this post is to illustrate how deep learning is being applied in cancer immunotherapy (Immuno-oncology or Immunooncology) - a cancer treatment strategy, where the aim is to utilize the cancer patient's own immune system to fight the cancer.",
    "author": [
      {
        "name": "Leon Eyrich Jessen",
        "url": "https://twitter.com/jessenleon"
      }
    ],
    "date": "2018-01-29",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data"
    ],
    "preview": "posts/2018-01-29-dl-for-cancer-immunotherapy/images/01_ffn_02_results_3_by_3_confusion_matrix.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 3000,
    "preview_height": 1800
  },
  {
    "path": "posts/2018-01-24-keras-fraud-autoencoder/",
    "title": "Predicting Fraud with Autoencoders and Keras",
    "description": "In this post we will train an autoencoder to detect credit card fraud. We will also demonstrate how to train Keras models in the cloud using CloudML. The basis of our model will be the Kaggle Credit Card Fraud Detection dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-25",
    "categories": [
      "TensorFlow/Keras",
      "Unsupervised Learning",
      "Cloud"
    ],
    "preview": "posts/2018-01-24-keras-fraud-autoencoder/images/preview.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 790,
    "preview_height": 537
  },
  {
    "path": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/",
    "title": "Analyzing rtweet Data with kerasformula",
    "description": "The kerasformula package offers a high-level interface for the R interface to Keras. It’s main interface is the kms function, a regression-style interface to keras_model_sequential that uses formulas and sparse matrices. We use kerasformula to predict how popular tweets will be based on how often the tweet was retweeted and favorited.",
    "author": [
      {
        "name": "Pete Mohanty",
        "url": "https://sites.google.com/site/petemohanty/"
      }
    ],
    "date": "2018-01-24",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "preview": "posts/2018-01-24-analyzing-rtweet-data-with-kerasformula/images/densities-1.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 480
  },
  {
    "path": "posts/2018-01-11-keras-customer-churn/",
    "title": "Deep Learning With Keras To Predict Customer Churn",
    "description": "Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.",
    "author": [
      {
        "name": "Matt Dancho",
        "url": "https://github.com/mdancho84"
      }
    ],
    "date": "2018-01-11",
    "categories": [
      "TensorFlow/Keras",
      "Tabular Data",
      "Explainability"
    ],
    "preview": "posts/2018-01-11-keras-customer-churn/images/customer_churn_analysis_corrr.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2696,
    "preview_height": 1696
  },
  {
    "path": "posts/2018-01-10-r-interface-to-cloudml/",
    "title": "R Interface to Google CloudML",
    "description": "We are excited to announce the availability of the cloudml package, which provides an R interface to Google Cloud Machine Learning Engine. CloudML provides a number of services including on-demand access to training on GPUs and hyperparameter tuning to optimize key attributes of model architectures.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2018-01-10",
    "categories": [
      "Cloud",
      "Packages/Releases"
    ],
    "preview": "posts/2018-01-10-r-interface-to-cloudml/images/cloudml.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 394,
    "preview_height": 211
  },
  {
    "path": "posts/2018-01-09-keras-duplicate-questions-quora/",
    "title": "Classifying Duplicate Questions from Quora with Keras",
    "description": "In this post we will use Keras to classify duplicated questions from Quora. Our implementation is inspired by the Siamese Recurrent Architecture, with modifications to the similarity measure and the embedding layers (the original paper uses pre-trained word vectors)",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2018-01-09",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "preview": "posts/2018-01-09-keras-duplicate-questions-quora/keras-duplicate-questions-quora.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1302,
    "preview_height": 788
  },
  {
    "path": "posts/2017-12-22-word-embeddings-with-keras/",
    "title": "Word Embeddings with Keras",
    "description": "Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semantically similar words are mapped to nearby points. In this example we'll use Keras to generate word embeddings for the Amazon Fine Foods Reviews dataset.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2017-12-22",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "preview": "posts/2017-12-22-word-embeddings-with-keras/word-embeddings-with-keras.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/",
    "title": "Time Series Forecasting with Recurrent Neural Networks",
    "description": "In this post, we'll review three advanced techniques for improving the performance and generalization power of recurrent neural networks.  We'll demonstrate all three concepts on a temperature-forecasting problem, where you have access to a time series of data points coming from sensors installed on the roof of a building.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-20",
    "categories": [
      "TensorFlow/Keras",
      "Time Series"
    ],
    "preview": "posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/images/jena_temp-r.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4000
  },
  {
    "path": "posts/2017-12-14-image-classification-on-small-datasets/",
    "title": "Image Classification on Small Datasets with Keras",
    "description": "Having to train an image-classification model using very little data is a common situation, in this article we review three techniques for tackling this problem including feature extraction and fine tuning from a pretrained network.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-14",
    "categories": [
      "TensorFlow/Keras",
      "Image Recognition & Image Processing"
    ],
    "preview": "posts/2017-12-14-image-classification-on-small-datasets/images/swapping_fc_classifier.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 678,
    "preview_height": 453
  },
  {
    "path": "posts/2017-12-07-text-classification-with-keras/",
    "title": "Deep Learning for Text Classification with Keras",
    "description": "Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this excerpt from the book Deep Learning with R, you'll learn to classify movie reviews as positive or negative, based on the text content of the reviews.",
    "author": [
      {
        "name": "François Chollet",
        "url": "https://github.com/fchollet"
      },
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-12-07",
    "categories": [
      "TensorFlow/Keras",
      "Natural Language Processing"
    ],
    "preview": "posts/2017-12-07-text-classification-with-keras/images/training-history.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1400,
    "preview_height": 865
  },
  {
    "path": "posts/2017-10-04-tfruns/",
    "title": "tfruns: Tools for TensorFlow Training Runs",
    "description": "The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-10-04",
    "categories": [
      "Packages/Releases"
    ],
    "preview": "posts/2017-10-04-tfruns/preview.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 2006,
    "preview_height": 1116
  },
  {
    "path": "posts/2017-09-06-keras-for-r/",
    "title": "Keras for R",
    "description": "We are excited to announce that the keras package is now available on CRAN. The package provides an R interface to Keras, a high-level neural networks API developed with a focus on enabling fast experimentation.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-09-05",
    "categories": [
      "TensorFlow/Keras",
      "Packages/Releases"
    ],
    "preview": "posts/2017-09-06-keras-for-r/preview.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 669,
    "preview_height": 414
  },
  {
    "path": "posts/2017-08-31-tensorflow-estimators-for-r/",
    "title": "TensorFlow Estimators",
    "description": "The tfestimators package is an R interface to TensorFlow Estimators, a high-level API that provides implementations of many different model types including linear models and deep neural networks.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": "https://orcid.org/0000-0001-5243-233X"
      }
    ],
    "date": "2017-08-31",
    "categories": [
      "Packages/Releases"
    ],
    "preview": "posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-architecture.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 1198,
    "preview_height": 796
  },
  {
    "path": "posts/2017-08-17-tensorflow-v13-released/",
    "title": "TensorFlow v1.3 Released",
    "description": "The final release of TensorFlow v1.3 is now available. This release marks the initial availability of several canned estimators including DNNClassifier and  DNNRegressor.",
    "author": [
      {
        "name": "J.J. Allaire",
        "url": "https://github.com/jjallaire"
      }
    ],
    "date": "2017-08-17",
    "categories": [
      "Packages/Releases"
    ],
    "preview": "posts/2017-08-17-tensorflow-v13-released/tensorflow-logo.png",
    "last_modified": "2020-07-30T08:48:59+02:00",
    "input_file": {},
    "preview_width": 3876,
    "preview_height": 741
  }
]
